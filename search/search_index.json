{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DiffBot Documentation","text":"<p>This project guides you on how to build an autonomous two wheel differential drive robot.  The robot can operate on a Raspberry Pi 4 B or NVIDIA Jetson Nano Developer Kit  running ROS Noetic or ROS Melodic middleware on Ubuntu Mate 20.04 and Ubuntu 18.04, respectively. With a motor driver that actuates two brushed motors the robot can drive autonomously to a desired location while sensing its environment using sensors,  such as a laser scanner to avoid obstacles and a camera to detect objects. Odometry wheel encoders (also refered to as speed sensors)  combined with an inertial measurement unit (IMU) are used together with the laser scanner for localization in a previously stored map.  Unseen enviornments can be mapped with the laser scanner, making use of open source SLAM algorithms such as <code>gmapping</code>. </p> <p>The following video gives an overview of the robot's components:</p> <p>The project is split into multiple parts, to adress the following main aspects of the robot.</p> <ul> <li>Bill of Materials (BOM) and the theory behind the parts.</li> <li>Theory of (mobile) robots.</li> <li>Assembly of the robot platform and the components.</li> <li>Setup of ROS (Noetic or Melodic) on either Raspberry Pi 4 B or Jetson Nano,    which are both Single Board Computers (SBC) and are the brain of the robot.</li> <li>Modeling the Robot in Blender and URDF to simulate it in Gazebo.</li> <li>ROS packages and nodes: </li> <li>Hardware drivers to interact with the hardware components</li> <li>High level nodes for perception, navigation, localization and control.</li> </ul> <p>Use the menu to learn more about the ROS packages and other components of the robot.</p> <p>Note</p> <p>Using a Jetson Nano instead of a Raspberry Pi is also possible. See the Jetson Nano Setup section in this documentation for more details.  To run ROS Noetic Docker is needed.</p>"},{"location":"#source-code","title":"Source Code","text":"<p>The source code for this project can be found in the ros-mobile-robots/diffbot GitHub repository.</p>"},{"location":"#remo-robot","title":"Remo Robot","text":"<p>You can find Remo robot (Research Education Modular/Mobile Open robot), a 3D printable and modular robot description package available at ros-mobile-robots/remo_description.  The stl files are freely availabe from the repository and stored inside the git lfs (Git large file system) on GitHub.  The bandwith limit for open source projects on GitHub is 1.0 GB per month,  which is why you might not be able to clone/pull the files because the quota is already exhausted this month.  To support this work and in case you need the files immediately, you can access them through the following link:</p> <p>Access Remo STL files</p>"},{"location":"#best-practices-and-rep","title":"Best Practices and REP","text":"<p>The project tries to follow the ROS best practices as good as possible.  This includes examples and patterns on producing and contributing high quality code,  as well as on testing, and other quality oriented practices, like continuous integration.  You can read more about it on the ROS Quality wiki. This includes also following the advices given in the ROS Enhancement Proposals (REPs). Throughout the documentation links to corresponding REPs are given.</p> <p>The wiki section ROS developer's guide is a good starting point for getting used to the common practices for developing components to be shared with the community. It includes links to naming conventions (e.g. for packages) and ROS C++ and Python style guides.</p> <p>Other good resources to learn more about ROS best practices is the Autonomous Systems Lab of ETH Zurich.</p> <p>Note</p> <p>Your contributions to the code or documentation are most welcome but please try to follow the mentioned best pratices where possible.</p>"},{"location":"#testing-debugging-and-ci","title":"Testing, Debugging and CI","text":"<p>For a ROS catkin workspace explaining gTest and rostest see Ros-Test-Example and its documentation. To run tests with catkin-tools, see Building and running tests.</p> <p>To get a workspace that allows a debugger to stop at breakpoints, it is required to build the catkin workspace with Debug Symbols.  For this the command <code>catkin build --save-config --cmake-args -DCMAKE_BUILD_TYPE=Debug</code> is used, mentioned in the catkin-tools cheat sheet.</p> <p>This repository makes use of automated builds when new code is pushed or a pull reuqest is made to this repository. For this the Travis and GitHub actions configurations (yml files) from ROS Industrial CI are used.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is using material design theme, which is based on MkDocs. Future code documentation will make use of doxygen and rosdoc_lite.</p>"},{"location":"#references","title":"References","text":"<p>Helpful resources to bring your own robots into ROS are:</p> <ul> <li>Understand ROS Concepts</li> <li>Follow ROS Tutorials such as Using ROS on your custom Robot</li> <li>Books:<ul> <li>Mastering ROS for Robotics Programming: Best practices and troubleshooting solutions when working with ROS, 3rd Edition this book contains also a chapter about about Remo</li> <li>Introduction to Autonomous Robots (free book)</li> <li>Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph</li> <li>Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media</li> <li>Mastering ROS for Robotics Programming Second Edition from Packt</li> <li>Elements of Robotics Robots and Their Applications from Springer</li> <li>ROS Robot Programming Book for Free! Handbook from Robotis written by Turtlebot3 Developers</li> </ul> </li> <li>Courses:<ul> <li>Robocademy</li> <li>ROS Online Course for Beginner</li> <li>Udacity Robotics Software Engineer</li> <li>Self-Driving Cars with Duckietown by ETH Zurich</li> </ul> </li> </ul>"},{"location":"DG01D-E-motor-with-encoder/","title":"Motor and Encoder","text":""},{"location":"DG01D-E-motor-with-encoder/#motor-with-wheel-encoder","title":"Motor with Wheel Encoder","text":"<p>The DG01D-E is a single hobby motor with a hall speed encoder.  This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V.  The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used,  generally 3.3V or 5V is used.</p> <p>The hall sensor can sense the North and South poles of its magnetic plate.  When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level.  Meanwhile the North is the inverse and, when sensed, the hall output will result a low level.</p> <p>It is not clear which encoder the current DG01D-E motor has built in. To find its resolution (counts, cycles and pulses per revolution, see image below for the difference),  we can spin the wheel for one full rotation and record the number of ticks, also known as counts.  This way we can obtain the geared resolution, meaning the resolution measured at the wheel axle and not the motor shaft.  It is possible to calculate the resolution at the motor shaft from the resolution measured at the wheel axle when we know the gear reduction ratio.  Because the datasheet is not precise about the gearbox ratio (1:48) it is hard to know exactly the pulses, counts, (and cycles) per revolution at the motor shaft itself  (where the encoder is attached). The output resolution of the wheel (after the gear reduction) on the other hand can be measured quite easily with a  script encoders_test.ino code.  From these measurements it seems that this encoder has only 3 pulses per revolution (ppr) because with the code that reads both edges (rising and falling) of  both channels we can observe roughly 542 at the wheel output shaft (lets round it up to 576 counts per wheel revolution to obtain 3 ppr.  We could also assume that the gear ratio might not be exactly correct). So 576 counts / 48:1 gear ratio / 2 channels / 2 edges = 3 pulses per revolution at the motor shaft.  Having an encoder with higher ppr (and having the same gear reduction ratio of 48:1) would yield a more accurate odometry.  For example let's say we get an encoder with 7 ppr, then we get the following output resolution at the wheel:  7 ppr (at motor shaft) * 48:1 gear ratio * 2 channels * 2 edges = 1344 ppr (measured at wheel).  The following image shows the difference between counts, pulses (and cycles).</p> Difference between pulses, counts and cycles of a quadrature encoder. <p>Source: cuidevices.com</p> <p>It is also possible to get the PPR from the number of magnetic polse, which you can see with a magnetic field paper:</p> <p></p> <p>There are six poles which results in three pulses per revolution.</p> <p> </p>"},{"location":"DG01D-E-motor-with-encoder/#terminal-pin-layout","title":"Terminal Pin Layout","text":"<p>The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot.</p> <ul> <li>G (Blue): hall power negative</li> <li>H1 (Green): hall H1 output signal, square wave</li> <li>H2 (Yellow): hall H2 output signal, square wave</li> <li>V (Orange): hall power positive</li> <li>M+ (Red): motor positive pole</li> <li>M- (Brown): motor negative pole</li> </ul> <p>The following image shows the motor from its side with the corresponding pin descriptions:</p> DG01D-E Motor with encoder pin description."},{"location":"DG01D-E-motor-with-encoder/#wheel-encoder-setup-with-ros","title":"Wheel Encoder Setup with ROS","text":"<p>The following video gives an idea of what has to be done to get the motor and encoder working with ROS.</p>"},{"location":"DG01D-E-motor-with-encoder/#wheel-encoder-measurements","title":"Wheel Encoder Measurements","text":"<p>This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor.  The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC.  The <code>motor_example.py</code> applies 50-100% of the 10 VDC which leads to the following output voltages on the motor:</p> Voltage sweep measurements <ul> <li>0:00 Forward Speed 50: 6.5 VDC</li> <li>0:12 Back Speed 50: 6.5 VDC </li> <li>0:23 Forward Speed 60: 6.9 VDC</li> <li>0:34 Back Speed 60: 6.9 VDC</li> <li>0:46 Forward Speed 70:  7.2 VDC</li> <li>0:56 Back Speed 70:  7.2 VDC</li> <li>1:07 Forward 80: 7.3 VDC</li> <li>1:18 Back 80: 7.3 VDC</li> <li>1:29 Forward 90: 7.6 VDC</li> <li>1:41 Back 90: 7.6 VDC</li> <li>1:52 Forward 100: 7.9 VDC</li> <li>2:02 Back 100: 7.9 VDC</li> </ul> <p>At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation.</p> Summary of installation instructions <ol> <li>Add repository to the updater <pre><code>sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" &gt;/etc/apt/sources.list.d/picoscope.list'\n</code></pre></li> <li>Import public key <pre><code>wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add -\n</code></pre></li> <li> <p>Update package manager cache <pre><code>sudo apt-get update\n</code></pre></p> </li> <li> <p>Install PicoScope</p> </li> </ol> <pre><code>sudo apt-get install picoscope\n</code></pre>"},{"location":"components/","title":"Components of an Autonomous Differential Drive Mobile Robot","text":"","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#bom-for-remo-robot","title":"BOM for REMO Robot","text":"<p>The following figure shows a 3D-printed Remo robot together with its components that satisfy the requirements for the ROS Navigation Stack. These parts are introduced next:</p> Remo prototype. <p>Bill of Materials (BOM) for REMO robot:</p> Part Quantity Cost Store Notes Raspberry Pi 4 B (4 Gb) 1 $55.0 Sparkfun, Amazon.com, Amazon.de SanDisk 64 GB SD Card Class 10 1 $13.99 Amazon.com, Amazon.de SLAMTEC RPLiDAR A2M8 (12 m) 1 $319.00 Robotshop, Amazon.com, Amazon.de Other, less expensive, LiDARs will work as well, e.g., RPLiDAR A1 Adafruit DC Motor (+ Stepper) FeatherWing 1 $19.95 adafruit.com, Amazon.de Teensy 4.0 or 3.2 1 $19.95 Amazon.com, PJRC Teensy 4.0, PJRC Teensy 3.2 Hobby Motor with Encoder - Metal Gear (DG01D-E) 2 $5.95 Amazon.com, Sparkfun Powerbank (e.g 15'000 mAh, or 10'000 mAh) 1 $23.99 Amazon.de, Anker Amazon.com, Anker Amazon.de The Powerbank from Goobay (and Anker) is close to the maximum possible size LxWxH: 135.5 x 71 x 18 mm Battery pack (for four or eight batteries) 1 $5.59 Amazon.com, Amazon.de USB cable pack 1 $6.99 Amazon.com, Amazon.de Type A to Micro, right angle Remo Base 1 -- 3D printable, see <code>remo_description</code> Caster ball 1 $6.30 Amazon.com, Amazon.de 25.4 mm (1-inch) diameter; Alternatively any smooth, durable 3/4\" ball bearing for the caster Wheels 2 $3.50 Amazon.com, Amazon.de, Sparkfun, exp-tech.de Wheels are often part of a robotics kit or can be purchased separately Power supply 1 $7.50 Amazon.com, Adafruit Micro USB, 5V, 2.5A","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#usb-wi-fi-dongle-optional","title":"USB Wi-Fi Dongle (optional)","text":"<p>For improved connectivity use a Wi-Fi USB dongle.</p> Part Quantity Cost URL Notes WiFi Dongle - TP-Link Archer T2U Nano 1 $17.99 Amazon RTL8811AU chipset WiFi Dongle - TP-Link Archer T2U Plus 1 $19.99 Amazon RTL8811AU chipset","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#camera-modules-optional","title":"Camera Modules (optional)","text":"<p>The camera modules are currently optional. SLAM, localization and navigation is currently laser based. Get a camera in case you plan to do applications such as object detection, visual graph based SLAM methods, etc.</p> Part Quantity Cost Store Notes Raspi Camera Module V2, 8 MP, 1080p 1 $ Amazon.com, Amazon.de OAK-1 1 $149 OpenCV.ai OAK-D 1 $199 OpenCV.ai OAK-D Lite 1 -- OpenCV.ai Will be released","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#assembly-hardware","title":"Assembly Hardware","text":"<p>You need the following parts to build REMO. They come in packs, so order the quantity you need for the number of REMOs you are going to build.</p> Part Qty per REMO Qty per pack Cost per REMO URL Notes Adhesive pads 2 48 $0.14 optional Velcro strap 2 48 $0.14 To fix the battery pack M2 screw (self tapping) 20 100 $1.29 Amazon 8mm long, self tapping M2 screw 4 60 $0.47 Amazon 8mm long M3 screw 4 60 $0.47 Amazon 25mm long, to fix the motors to the base frame M3 nut 4 100 $0.24 Amazon To fix the motors to the base frame M2 Brass threaded inserts 4 100 $0.24 Amazon Jumper wires 4 40 $0.13 Amazon Female-female, ~20cm","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#optional-parts","title":"Optional parts","text":"Part Quantity Cost Store Notes Jetson Nano 1 $99.00 NVIDIA PiOLED display 1 $14.95 Adafruit, Amazon PiOLED header 1 $5.95 Adafruit, Amazon, Sparkfun 2x(3+) right angle male","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#components","title":"Components","text":"<p>The following shows a more detailed part list and assembly of the robot platform and the components.</p> Category Hardware Part Number Data Sheet &amp; Info Accessories Case for Raspberry Pi 4 B Slim acrylic case for Raspberry Pi 4, stackable, rainbow/transparent BerryBase Micro SD Card SanDisk 64GB Class 10 SanDisk, Ubuntu 18.04 Image Robot Car Kit 2WD robot05 Instructions manual Power bank Intenso Powerbank S10000 Intenso Actuator (Deprecated) Gearbox motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC Adafruit DG01E-E Motor with encoder DG01E-E Hobby motor with quadrature encoder Sparkfun Board Raspberry Pi 4 B Raspberry Pi 4 B - 4 GB OEM Website Cables Jumper - Female to Female Jumper - Male to Male Micro USB - USB Cable Camera extension cable I2C 4 pin cable Electronics Fan Fan 30x30x7mm 5V DC with Dupont connector BerryBase I2C motor driver Grove - I2C Motor Driver Seeed Studio I2C Hub Grove - I2C Hub Seeed Studio Human Machine Interface OLED Display Grove OLED Display 0.96\" Seeed Studio LED Ring NeoPixel Ring 12x5050 RGB LED Adafruit Sensors Camera module Raspberry Pi - camera module v2.1 Raspberry Pi Ultrasonic ranger Grove - Ultrasonic Ranger Seeed Studio IMU Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 Adafruit Odometry Joy-IT - LM393 Speed Sensor with H206 slot-type opto interrupter Joy-IT <p>Order list</p> Part Store Raspberry Pi 4 B (4 Gb) Amazon.com, Amazon.de SanDisk 64 GB SD Card Class 10 Amazon.com, Amazon.de Robot Smart Chassis Kit Amazon.com, Amazon.de SLAMTEC RPLidar A2M8 (12 m) Amazon.com, Amazon.de Grove Ultrasonic Ranger Amazon.com, Amazon.de Raspi Camera Module V2, 8 MP, 1080p Amazon.com, Amazon.de Grove Motor Driver seeedstudio.com, Amazon.de I2C Hub seeedstudio.com, Amazon.de <p>Additional (Optional) Equipment</p> Part Store PicoScope 3000 Series Oscilloscope 2CH Amazon.de VOLTCRAFT PPS-16005 Amazon.de","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#board-raspberry-pi-4-b","title":"Board - Raspberry Pi 4 B","text":"<p>The main processing unit of the robot is a Raspberry Pi 4 B  with 4 GB of RAM. </p> Raspberry Pi 4 B - 4 GB RAM variant.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#accessories-and-electronics","title":"Accessories and Electronics","text":"","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#case-and-cooling","title":"Case and Cooling","text":"<p>To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors.</p> Stackable Rainbow Case for Raspberry Pi 4 B. <p>With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor.</p> Heatsinks and cooling fan for Raspberry Pi 4 B.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#sd-card","title":"SD Card","text":"<p>The Raspberry Pi requires a medium to boot from.  For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. </p> SanDisk Micro SD Card Class 10. <p>Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high  performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended.  Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s (source)</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#robot-base","title":"Robot Base","text":"<p>For the robot base you have at least two options to choose from. Either the robot car kit consisting of plexi glas material or a more sturdier 3D printed variant named Remo.</p> DiffBotRemo <p>The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. </p> <p> Parts of the 2WD Robot Car Kit 05 from Joy-IT. </p> <p>The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD,  are included in the kit which can be attached to the motors that are provided too.  A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. </p> <p>The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements.  With that punched disk and additional speed sensors it is possible to implement odometry in ROS.  To power the motors a battery compartment is available together with a switch to turn the robot on or off.</p> <p>Remo is a 3D printable Research Education Mobile/Modular Open robot platform. You can find more information in the following video and  on the <code>remo_description</code> package page.</p> <p></p> <p></p> <p>Alternatively, you can build your own mobile robot!</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#power-supplies","title":"Power Supplies","text":"<p>As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V.</p> Power bank with 10.000 mAh from Intenso. <p>To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\).</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#i2c-hub","title":"I2C Hub","text":"<p>The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi</p> Grove I2C Hub.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#breadboard-and-gpio-extension-cable","title":"Breadboard and GPIO Extension Cable","text":"<p>Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B.</p> Breadboard with GPIO extension cable.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#sensors","title":"Sensors","text":"<p>Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#perception","title":"Perception","text":"<p>Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#ultrasonic-ranger","title":"Ultrasonic Ranger","text":"<p>To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. </p> Grove Ultrasonic Ranger for obstacle avoidance. <p>It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO. For example physical pin 11 of the Raspberry Pi connected to the <code>SIG</code> pin on the sensor can provide the PWM communication.</p> Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL <p>The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub.</p> <p>As an alternative we could use the HC SR04.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#camera","title":"Camera","text":"RPi Camera v2.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#localization","title":"Localization","text":"","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#inertial-measurement-unit","title":"Inertial Measurement Unit","text":"<p>An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used.</p> 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#odometry","title":"Odometry","text":"<p>For the used odometry sensor see the section below Motor and Wheel Encoder</p> Alternative Optical Sensor <p>To estimate the change in position over time (odometry) the robot will utilize an optical speed sensor.  Specifically the Joy-IT Speed Sensor which combines a LM393 (datasheet) comperator and a H206 slot-type opto interrupter.</p> LM393 Speed Sensor from Joy-IT. <p>Technical Specifications:</p> <ul> <li>Dimensions: 32 x 14 x 7mm </li> <li>Operating voltage: 3.3V to 5V (we will use 3.3V)</li> <li>Two outputs: Digital (D0) and Analog (A0)</li> </ul> <p>References: https://dronebotworkshop.com/robot-car-with-speed-sensors/</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#actuators","title":"Actuators","text":"<ul> <li>Grove - I2C Motor Driver V1.3</li> </ul>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#control","title":"Control","text":"<p>To drive the two motors of the car kit we use the  Grove - I2C Motor Driver V1.3 from Seeed Studio.</p> Grove - I2C Motor Driver.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#motor-and-wheel-encoder","title":"Motor and Wheel Encoder","text":"<p>The DG01E-E Hobby Motor has a quadrature encoder built in,  which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders.</p> DG01D-E Motor with wheel encoders. <p>For more details such as the encoder pulses per revolution see the documentation on Motor and Encoder.</p> Alternative Brushed Gear Motor","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#brushed-gearbox-motor","title":"Brushed Gearbox Motor","text":"DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC.","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#human-machine-interface-hmi","title":"Human Machine Interface (HMI)","text":"<p>The human machine interface is the layer between the user and the robot. </p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"components/#oled-display","title":"OLED Display","text":"<p>To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display  from Seeed Studio.</p> Grove - I2C 0.96 inch OLED Display. <p>The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout.</p> <p>Library</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","sensors","components"]},{"location":"diffbot_bringup/","title":"Hardware Bringup","text":""},{"location":"diffbot_bringup/#diffbot-bring-up-package","title":"DiffBot Bring Up Package","text":"<p>The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using <code>catkin-tools</code>:</p> <pre><code>fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup\nCreating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"...\nCreated file diffbot_bringup/package.xml\nCreated file diffbot_bringup/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup.\n</code></pre> <p>The package provides a <code>launch</code> folder which includes <code>minimal.launch</code> and <code>bringup.launch</code>.</p> <p>The <code>minimal.launch</code> is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the <code>diffbot_base</code> package.  It will also setup the ROS controller manager with  DiffBot's hardware interface.</p> <p>For the motor driver the node <code>motor_driver.py</code> from the <code>grove_motor_driver</code> package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. </p> <pre><code>&lt;launch&gt;\n    &lt;!-- Including the following launch file from diffbot_base package will --&gt;\n    &lt;!-- Load the robot description onto the parameter server --&gt;\n    &lt;!-- Run the controller manager with DiffBot's hardware interface --&gt;\n    &lt;!-- Load the controller config onto the parameter server --&gt;\n    &lt;include file=\"$(find diffbot_base)/launch/diffbot.launch\"&gt;\n        &lt;!-- arg name=\"model\" value=\"$(arg model)\" /--&gt;\n    &lt;/include&gt;\n\n\n    &lt;!-- Motors --&gt;\n    &lt;!-- --&gt;\n    &lt;node name=\"motor_driver\" pkg=\"grove_motor_driver\" type=\"motor_driver.py\" respawn=\"false\"\n        output=\"screen\" ns=\"diffbot\" /&gt;\n\n    &lt;!-- Encoders --&gt;\n    &lt;!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --&gt;\n    &lt;node name=\"rosserial_teensy\" pkg=\"rosserial_python\" type=\"serial_node.py\" respawn=\"false\"\n        output=\"screen\" ns=\"diffbot\" args=\"_port:=/dev/ttyACM0\n                                            _baud:=115200\"/&gt;\n&lt;/launch&gt;\n</code></pre> <p>As mentioned, the ROS controller used for DiffBot is the <code>diff_drive_controller</code>.  This controller publishes a transform message (see its published topics),  via the <code>/tf</code> topic, between the <code>odom</code> frame and the frame configured in the controller's configuration specified by the <code>base_frame_id</code>.  In the case of DiffBot this is the <code>base_footprint</code>, a conventional link, defined in REP-120, for mobile robots that specifies the robot's footprint.</p> <p>Because this is the only transform published by <code>diff_drive_controller</code> another node is needed to publish rest of the link transformations. It is the well known <code>robot_state_publisher</code>, which uses the joint states published by the ROS controller <code>joint_state_controller</code> (not to be confused with <code>joint_state_publisher</code> - it is not used here, see this answer for the difference) to create the transforms between the links.</p> <p>To do this the <code>bringup.launch</code> includes the <code>minimal.launch</code> and then runs the <code>robot_state_publisher</code>:</p> <pre><code>&lt;launch&gt;\n    &lt;include file=\"$(find diffbot_bringup)/launch/minimal.launch\"&gt;\n        &lt;!-- arg name=\"model\" value=\"$(arg model)\" /--&gt;\n    &lt;/include&gt;\n\n    &lt;!-- Starting robot state publish which will publish tf --&gt;\n    &lt;!-- This is needed to publish transforms between all links --&gt;\n    &lt;!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --&gt;\n    &lt;!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --&gt;\n    &lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\"\n        output=\"screen\" ns=\"diffbot\" /&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"diffbot_control/","title":"Control","text":""},{"location":"diffbot_control/#diffbot-control-package","title":"DiffBot Control Package","text":"<p>As described in the ROS Integration and  Gazebo Simulation sections,  DiffBot makes use of ROS Control repositories.  Specifically the <code>diff_drive_controller</code> package from the  <code>ros_controllers</code> meta package.  To leverage ROS Control for the simulation with Gazebo the robot description and the  controller configuration (usually a <code>MYROBOT_control.yaml</code> file) is required. For the real hardware its required to implement  a class derived from <code>hardware_interface::RobotHW</code>.</p> <p>The convention to control a robot (in simulation and in the real world) is to have a package named <code>MYROBOT_control</code>. In case of DiffBot its called <code>diffbot_control</code> and created with <code>catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]]</code>:</p> <pre><code>catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs \nCreating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"...\nCreated file diffbot_control/CMakeLists.txt\nCreated file diffbot_control/package.xml\nCreated folder diffbot_control/include/diffbot_control\nCreated folder diffbot_control/src\nSuccessfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control.\n</code></pre> <p>To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. </p> Dependency Source Ubuntu/Debian Package <code>diff_drive_controller</code> https://github.com/ros-controls/ros_controllers <code>ros-noetic-ros-controllers</code> <p>To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the <code>src</code> folder of a ros catkin workspace and execute the <code>catkin build</code> command. Also make sure to source the workspace after building new packages with <code>source devel/setup.bash</code>.</p> <pre><code>cd /home/fjp/git/diffbot/ros/  # Navigate to the workspace\ncatkin build              # Build all the packages in the workspace\nls build                  # Show the resulting build space\nls devel                  # Show the resulting devel space\n</code></pre> <p>Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.</p>"},{"location":"diffbot_control/#ros-control-in-gazebo","title":"ROS Control in Gazebo","text":"<p>Two great resources to get the <code>diff_drive_controller</code> working inside Gazebo is the Gazebo ROS Control Tutorial of <code>rrbot</code> and the R2D2 ROS URDF Tutorial, especially the last section, The Wheels on the Droid Go Round and Round.</p> <p>To spawn DiffBot inside Gazebo, RViz and control it with the <code>rqt_robot_steering</code> plugin,  launch the <code>diffbot.launch</code> inside the <code>diffbot_control</code> package:</p> <pre><code>roslaunch diffbot_control diffbot.launch\n</code></pre> <p>This launch file makes use of <code>diffbot_gazebo/launch/diffbot.launch</code>, <code>diffbot_control/launch/diffbot_control.launch</code> to run gazebo and the <code>diff_drive_controller</code>. It also opens RViz with the configuration stored in <code>diffbot_control/rviz/diffbot.rviz</code>.  The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved.</p>"},{"location":"diffbot_control/#ros-control-on-the-real-hardware","title":"ROS Control on the Real Hardware","text":"<p>As mentioned above the its required to implement a class derived from  <code>hardware_interface::RobotHW</code>. Let's call it <code>DiffBotHW</code> and create it inside the <code>diffbot_control/src</code> folder.</p>"},{"location":"diffbot_gazebo/","title":"Simulation","text":""},{"location":"diffbot_gazebo/#simulate-diffbot-in-gazebo","title":"Simulate DiffBot in Gazebo","text":"<p>As described in the Creating your own Gazebo ROS Package, it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named <code>/MYROBOT_gazebo</code>. For DiffBot the package is named <code>diffbot_gazebo</code>. Another example that follows best pratices is <code>rrbot</code> which can be found in the gazebo_ros_demos repository.</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo\nCreating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"...\nCreated file diffbot_gazebo/package.xml\nCreated file diffbot_gazebo/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo.\n</code></pre> <p>The <code>diffbot_gazebo</code> package contains a launch file to lauch a world in Gazebo and spawn the robot model,  which is defined in the previously created <code>diffbot_description</code> package.  For the launch files the convention is to have a folder named <code>launch</code> and for Gazebo world files a folder named <code>world</code> inside a package.</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world\n</code></pre> <p>Inside the <code>launch</code> folder is the <code>diffbot.launch</code>.</p> <pre><code>&lt;launch&gt;\n    &lt;!-- these are the arguments you can pass this launch file, for example paused:=true --&gt;\n    &lt;arg name=\"paused\" default=\"false\"/&gt;\n    &lt;arg name=\"use_sim_time\" default=\"true\"/&gt;\n    &lt;arg name=\"gui\" default=\"true\"/&gt;\n    &lt;arg name=\"headless\" default=\"false\"/&gt;\n    &lt;arg name=\"debug\" default=\"false\"/&gt;\n\n    &lt;!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --&gt;\n    &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt;\n        &lt;arg name=\"world_name\" value=\"$(find diffbot_gazebo)/worlds/diffbot.world\"/&gt;\n        &lt;arg name=\"debug\" value=\"$(arg debug)\" /&gt;\n        &lt;arg name=\"gui\" value=\"$(arg gui)\" /&gt;\n        &lt;arg name=\"paused\" value=\"$(arg paused)\"/&gt;\n        &lt;arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/&gt;\n        &lt;arg name=\"headless\" value=\"$(arg headless)\"/&gt;\n    &lt;/include&gt;\n&lt;/launch&gt;\n</code></pre> <p>In the <code>world</code> folder of the <code>diffbot_gazebo</code> package is the <code>diffbot.world</code> file:</p> <pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;sdf version=\"1.4\"&gt;\n  &lt;world name=\"default\"&gt;\n    &lt;include&gt;\n      &lt;uri&gt;model://ground_plane&lt;/uri&gt;\n    &lt;/include&gt;\n    &lt;include&gt;\n      &lt;uri&gt;model://sun&lt;/uri&gt;\n    &lt;/include&gt;\n    &lt;include&gt;\n      &lt;uri&gt;model://gas_station&lt;/uri&gt;\n      &lt;name&gt;gas_station&lt;/name&gt;\n      &lt;pose&gt;-2.0 7.0 0 0 0 0&lt;/pose&gt;\n    &lt;/include&gt;\n  &lt;/world&gt;\n&lt;/sdf&gt;\n</code></pre> <p>With these files build the catkin workspace and source it to make the new <code>diffbot_gazebo</code> package visible to <code>roslaunch</code>:</p> <pre><code>catkin build\nsource devel/setup.zsh\n</code></pre> <p>Then its possible to launch the <code>diffbot.launch</code> with:</p> <pre><code>roslaunch diffbot_gazebo diffbot.launch\n</code></pre> <p>This will lead to the following output:</p> <pre><code>roslaunch diffbot_gazebo diffbot.launch \n... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log\nChecking log directory for disk usage. This may take a while.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is &lt;1GB.\n\nstarted roslaunch server http://tensorbook:32837/\n\nSUMMARY\n========\n\nPARAMETERS\n * /gazebo/enable_ros_network: True\n * /rosdistro: noetic\n * /rosversion: 1.15.8\n * /use_sim_time: True\n\nNODES\n  /\n    gazebo (gazebo_ros/gzserver)\n    gazebo_gui (gazebo_ros/gzclient)\n\nROS_MASTER_URI=http://localhost:11311\n\nprocess[gazebo-1]: started with pid [393352]\nprocess[gazebo_gui-2]: started with pid [393357]\n[ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin.\n[ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting...\n[ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin.\n[ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting...\n[ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available.\n[ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready.\n</code></pre> <p>Also, the Gazebo simulator will open a new window with the objects defined in <code>diffbot.world</code> except for the Gas station because it is a model  that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way.</p> Empty world of DiffBot. <p>To get the Gas station or other available models it is possible to clone the gazebo models repository into your <code>/home/your_username/.gazebo</code> folder, e.g.:</p> <p><pre><code>/home/fjp/.gazeb$ git clone osrf/gazebo_models\n</code></pre> Then add this path inside Gazebo to insert these models into your world file.</p>"},{"location":"diffbot_gazebo/#using-ros-launch-to-spawn-urdf-robots","title":"Using ROS launch to Spawn URDF Robots","text":"<p>According to the Gazebo roslaunch tutorial the recommended way  to spawn a robot into Gazebo is to use a launch file. Therefore, edit the <code>diffbot.launch</code> inside the <code>diffbot_gazebo</code> package by adding the following inside the <code>&lt;launch&gt; &lt;/launch</code> tag:</p> <pre><code>   &lt;!-- Load the URDF into the ROS Parameter Server --&gt;\n   &lt;param name=\"robot_description\"\n       command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /&gt;\n\n   &lt;!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --&gt;\n   &lt;node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\"\n       args=\"-urdf -model diffbot -param robot_description\"/&gt;\n</code></pre> <p>See also the complete <code>diffbot.launch</code> file.</p> <p>This will open Gazebo simulator and show the DiffBot model:</p> <p> Empty world including DiffBot. </p>"},{"location":"diffbot_gazebo/#moving-the-robot","title":"Moving the Robot","text":"<p>Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin <code>gazebo_ros_control</code>, see also the Gazebo ROS Control Tutorial. Using the ROS Control and its Gazebo plugin is done in case of DiffBot.  An alternative would be to use the existing <code>differential_drive_controller</code> Gazebo plugin without having to rely on ROS Control.  The next section explains the <code>diffbot_control</code> package in more detail and how to setup the  <code>diff_drive_controller</code> from the <code>ros_controllers</code> package.</p>"},{"location":"diffbot_gazebo/#adding-sensors","title":"Adding Sensors","text":"<p>To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also <code>gazebo_ros_pkgs</code> which is a package or interface for using ROS with the Gazebo simulator.</p>"},{"location":"diffbot_gazebo/#camera","title":"Camera","text":"<p>This section follows Gazebo tutorial Adding a Camera.</p>"},{"location":"diffbot_gazebo/#laser-lidar","title":"Laser (Lidar)","text":"<p>This section follows Gazebo tutorial Adding a Laser GPU.</p>"},{"location":"diffbot_gazebo/#ultrasonic-ranger","title":"Ultrasonic Ranger","text":"<p>See the source of the <code>gazebo_ros_range</code> plugin.</p>"},{"location":"diffbot_gazebo/#inertial-measurement-unit-imu","title":"Inertial Measurement Unit (IMU)","text":"<p>This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor?</p>"},{"location":"diffbot_gazebo/#troubleshooting","title":"Troubleshooting","text":"<p>A quick way to verify if the conversion from xacro to urdf to sdf is working is the following (source: Tutorial URDF in Gazebo): First convert the xacro model to a urdf model with the <code>xacro</code> command:</p> <pre><code>xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf\n</code></pre> <p>This will output the urdf into a file named <code>diffbot.urdf</code> in the current working directory.</p> <p>Then use the <code>gz</code> command to create a sdf:</p> <pre><code># gazebo3 and above\ngz sdf -p MODEL.urdf\n</code></pre> DiffBot sdf. <pre><code>&lt;sdf version='1.7'&gt;\n  &lt;model name='diffbot'&gt;\n    &lt;link name='base_footprint'&gt;\n      &lt;inertial&gt;\n        &lt;pose&gt;-0.012273 0 0.040818 0 -0 0&lt;/pose&gt;\n        &lt;mass&gt;5.5&lt;/mass&gt;\n        &lt;inertia&gt;\n          &lt;ixx&gt;0.0387035&lt;/ixx&gt;\n          &lt;ixy&gt;0&lt;/ixy&gt;\n          &lt;ixz&gt;0.000552273&lt;/ixz&gt;\n          &lt;iyy&gt;0.0188626&lt;/iyy&gt;\n          &lt;iyz&gt;0&lt;/iyz&gt;\n          &lt;izz&gt;0.0561591&lt;/izz&gt;\n        &lt;/inertia&gt;\n      &lt;/inertial&gt;\n      &lt;collision name='base_footprint_collision'&gt;\n        &lt;pose&gt;0 0 0 0 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;box&gt;\n            &lt;size&gt;0.001 0.001 0.001&lt;/size&gt;\n          &lt;/box&gt;\n        &lt;/geometry&gt;\n        &lt;surface&gt;\n          &lt;contact&gt;\n            &lt;ode/&gt;\n          &lt;/contact&gt;\n          &lt;friction&gt;\n            &lt;ode/&gt;\n          &lt;/friction&gt;\n        &lt;/surface&gt;\n      &lt;/collision&gt;\n      &lt;collision name='base_footprint_fixed_joint_lump__base_link_collision_1'&gt;\n        &lt;pose&gt;0 0 0.04 0 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;box&gt;\n            &lt;size&gt;0.3 0.15 0.02&lt;/size&gt;\n          &lt;/box&gt;\n        &lt;/geometry&gt;\n        &lt;surface&gt;\n          &lt;contact&gt;\n            &lt;ode/&gt;\n          &lt;/contact&gt;\n          &lt;friction&gt;\n            &lt;ode/&gt;\n          &lt;/friction&gt;\n        &lt;/surface&gt;\n      &lt;/collision&gt;\n      &lt;collision name='base_footprint_fixed_joint_lump__caster_link_collision_2'&gt;\n        &lt;pose&gt;-0.135 0 0.029 0 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;sphere&gt;\n            &lt;radius&gt;0.025&lt;/radius&gt;\n          &lt;/sphere&gt;\n        &lt;/geometry&gt;\n        &lt;surface&gt;\n          &lt;contact&gt;\n            &lt;ode/&gt;\n          &lt;/contact&gt;\n          &lt;friction&gt;\n            &lt;ode/&gt;\n          &lt;/friction&gt;\n        &lt;/surface&gt;\n      &lt;/collision&gt;\n      &lt;visual name='base_footprint_fixed_joint_lump__base_link_visual'&gt;\n        &lt;pose&gt;0 0 0.04 0 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;box&gt;\n            &lt;size&gt;0.3 0.15 0.02&lt;/size&gt;\n          &lt;/box&gt;\n        &lt;/geometry&gt;\n        &lt;material&gt;\n          &lt;script&gt;\n            &lt;name&gt;Gazebo/White&lt;/name&gt;\n            &lt;uri&gt;file://media/materials/scripts/gazebo.material&lt;/uri&gt;\n          &lt;/script&gt;\n        &lt;/material&gt;\n      &lt;/visual&gt;\n      &lt;visual name='base_footprint_fixed_joint_lump__caster_link_visual_1'&gt;\n        &lt;pose&gt;-0.115 0 0.029 0 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;sphere&gt;\n            &lt;radius&gt;0.025&lt;/radius&gt;\n          &lt;/sphere&gt;\n        &lt;/geometry&gt;\n      &lt;/visual&gt;\n      &lt;velocity_decay/&gt;\n      &lt;velocity_decay/&gt;\n      &lt;gravity&gt;1&lt;/gravity&gt;\n      &lt;velocity_decay/&gt;\n    &lt;/link&gt;\n    &lt;joint name='front_left_wheel_joint' type='revolute'&gt;\n      &lt;pose relative_to='base_footprint'&gt;0.105 -0.085 0.04 0 -0 0&lt;/pose&gt;\n      &lt;parent&gt;base_footprint&lt;/parent&gt;\n      &lt;child&gt;front_left_wheel&lt;/child&gt;\n      &lt;axis&gt;\n        &lt;xyz&gt;0 1 0&lt;/xyz&gt;\n        &lt;limit&gt;\n          &lt;lower&gt;-1e+16&lt;/lower&gt;\n          &lt;upper&gt;1e+16&lt;/upper&gt;\n        &lt;/limit&gt;\n        &lt;dynamics&gt;\n          &lt;spring_reference&gt;0&lt;/spring_reference&gt;\n          &lt;spring_stiffness&gt;0&lt;/spring_stiffness&gt;\n        &lt;/dynamics&gt;\n      &lt;/axis&gt;\n    &lt;/joint&gt;\n    &lt;link name='front_left_wheel'&gt;\n      &lt;pose relative_to='front_left_wheel_joint'&gt;0 0 0 0 -0 0&lt;/pose&gt;\n      &lt;inertial&gt;\n        &lt;pose&gt;0 0 0 0 -0 0&lt;/pose&gt;\n        &lt;mass&gt;2.5&lt;/mass&gt;\n        &lt;inertia&gt;\n          &lt;ixx&gt;0.00108333&lt;/ixx&gt;\n          &lt;ixy&gt;0&lt;/ixy&gt;\n          &lt;ixz&gt;0&lt;/ixz&gt;\n          &lt;iyy&gt;0.00108333&lt;/iyy&gt;\n          &lt;iyz&gt;0&lt;/iyz&gt;\n          &lt;izz&gt;0.002&lt;/izz&gt;\n        &lt;/inertia&gt;\n      &lt;/inertial&gt;\n      &lt;collision name='front_left_wheel_collision'&gt;\n        &lt;pose&gt;0 0 0 1.5708 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;cylinder&gt;\n            &lt;length&gt;0.02&lt;/length&gt;\n            &lt;radius&gt;0.04&lt;/radius&gt;\n          &lt;/cylinder&gt;\n        &lt;/geometry&gt;\n        &lt;surface&gt;\n          &lt;contact&gt;\n            &lt;ode&gt;\n              &lt;kp&gt;1e+07&lt;/kp&gt;\n              &lt;kd&gt;1&lt;/kd&gt;\n            &lt;/ode&gt;\n          &lt;/contact&gt;\n          &lt;friction&gt;\n            &lt;ode&gt;\n              &lt;mu&gt;1&lt;/mu&gt;\n              &lt;mu2&gt;1&lt;/mu2&gt;\n              &lt;fdir1&gt;1 0 0&lt;/fdir1&gt;\n            &lt;/ode&gt;\n          &lt;/friction&gt;\n        &lt;/surface&gt;\n      &lt;/collision&gt;\n      &lt;visual name='front_left_wheel_visual'&gt;\n        &lt;pose&gt;0 0 0 1.5708 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;cylinder&gt;\n            &lt;length&gt;0.02&lt;/length&gt;\n            &lt;radius&gt;0.04&lt;/radius&gt;\n          &lt;/cylinder&gt;\n        &lt;/geometry&gt;\n        &lt;material&gt;\n          &lt;script&gt;\n            &lt;name&gt;Gazebo/Grey&lt;/name&gt;\n            &lt;uri&gt;file://media/materials/scripts/gazebo.material&lt;/uri&gt;\n          &lt;/script&gt;\n        &lt;/material&gt;\n      &lt;/visual&gt;\n      &lt;gravity&gt;1&lt;/gravity&gt;\n      &lt;velocity_decay/&gt;\n    &lt;/link&gt;\n    &lt;joint name='front_right_wheel_joint' type='revolute'&gt;\n      &lt;pose relative_to='base_footprint'&gt;0.105 0.085 0.04 0 -0 0&lt;/pose&gt;\n      &lt;parent&gt;base_footprint&lt;/parent&gt;\n      &lt;child&gt;front_right_wheel&lt;/child&gt;\n      &lt;axis&gt;\n        &lt;xyz&gt;0 1 0&lt;/xyz&gt;\n        &lt;limit&gt;\n          &lt;lower&gt;-1e+16&lt;/lower&gt;\n          &lt;upper&gt;1e+16&lt;/upper&gt;\n        &lt;/limit&gt;\n        &lt;dynamics&gt;\n          &lt;spring_reference&gt;0&lt;/spring_reference&gt;\n          &lt;spring_stiffness&gt;0&lt;/spring_stiffness&gt;\n        &lt;/dynamics&gt;\n      &lt;/axis&gt;\n    &lt;/joint&gt;\n    &lt;link name='front_right_wheel'&gt;\n      &lt;pose relative_to='front_right_wheel_joint'&gt;0 0 0 0 -0 0&lt;/pose&gt;\n      &lt;inertial&gt;\n        &lt;pose&gt;0 0 0 0 -0 0&lt;/pose&gt;\n        &lt;mass&gt;2.5&lt;/mass&gt;\n        &lt;inertia&gt;\n          &lt;ixx&gt;0.00108333&lt;/ixx&gt;\n          &lt;ixy&gt;0&lt;/ixy&gt;\n          &lt;ixz&gt;0&lt;/ixz&gt;\n          &lt;iyy&gt;0.00108333&lt;/iyy&gt;\n          &lt;iyz&gt;0&lt;/iyz&gt;\n          &lt;izz&gt;0.002&lt;/izz&gt;\n        &lt;/inertia&gt;\n      &lt;/inertial&gt;\n      &lt;collision name='front_right_wheel_collision'&gt;\n        &lt;pose&gt;0 0 0 1.5708 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;cylinder&gt;\n            &lt;length&gt;0.02&lt;/length&gt;\n            &lt;radius&gt;0.04&lt;/radius&gt;\n          &lt;/cylinder&gt;\n        &lt;/geometry&gt;\n        &lt;surface&gt;\n          &lt;contact&gt;\n            &lt;ode&gt;\n              &lt;kp&gt;1e+07&lt;/kp&gt;\n              &lt;kd&gt;1&lt;/kd&gt;\n            &lt;/ode&gt;\n          &lt;/contact&gt;\n          &lt;friction&gt;\n            &lt;ode&gt;\n              &lt;mu&gt;1&lt;/mu&gt;\n              &lt;mu2&gt;1&lt;/mu2&gt;\n              &lt;fdir1&gt;1 0 0&lt;/fdir1&gt;\n            &lt;/ode&gt;\n          &lt;/friction&gt;\n        &lt;/surface&gt;\n      &lt;/collision&gt;\n      &lt;visual name='front_right_wheel_visual'&gt;\n        &lt;pose&gt;0 0 0 1.5708 -0 0&lt;/pose&gt;\n        &lt;geometry&gt;\n          &lt;cylinder&gt;\n            &lt;length&gt;0.02&lt;/length&gt;\n            &lt;radius&gt;0.04&lt;/radius&gt;\n          &lt;/cylinder&gt;\n        &lt;/geometry&gt;\n        &lt;material&gt;\n          &lt;script&gt;\n            &lt;name&gt;Gazebo/Grey&lt;/name&gt;\n            &lt;uri&gt;file://media/materials/scripts/gazebo.material&lt;/uri&gt;\n          &lt;/script&gt;\n        &lt;/material&gt;\n      &lt;/visual&gt;\n      &lt;gravity&gt;1&lt;/gravity&gt;\n      &lt;velocity_decay/&gt;\n    &lt;/link&gt;\n    &lt;plugin name='gazebo_ros_control' filename='libgazebo_ros_control.so'&gt;\n      &lt;robotNamespace&gt;/diffbot&lt;/robotNamespace&gt;\n      &lt;robotSimType&gt;gazebo_ros_control/DefaultRobotHWSim&lt;/robotSimType&gt;\n    &lt;/plugin&gt;\n    &lt;static&gt;0&lt;/static&gt;\n    &lt;plugin name='differential_drive_controller' filename='libgazebo_ros_diff_drive.so'&gt;\n      &lt;legacyMode&gt;1&lt;/legacyMode&gt;\n      &lt;rosDebugLevel&gt;Debug&lt;/rosDebugLevel&gt;\n      &lt;publishWheelTF&gt;0&lt;/publishWheelTF&gt;\n      &lt;robotNamespace&gt;/&lt;/robotNamespace&gt;\n      &lt;publishTf&gt;1&lt;/publishTf&gt;\n      &lt;publishWheelJointState&gt;0&lt;/publishWheelJointState&gt;\n      &lt;alwaysOn&gt;1&lt;/alwaysOn&gt;\n      &lt;updateRate&gt;100.0&lt;/updateRate&gt;\n      &lt;leftJoint&gt;front_left_wheel_joint&lt;/leftJoint&gt;\n      &lt;rightJoint&gt;front_right_wheel_joint&lt;/rightJoint&gt;\n      &lt;wheelSeparation&gt;0.3&lt;/wheelSeparation&gt;\n      &lt;wheelDiameter&gt;0.08&lt;/wheelDiameter&gt;\n      &lt;broadcastTF&gt;1&lt;/broadcastTF&gt;\n      &lt;wheelTorque&gt;30&lt;/wheelTorque&gt;\n      &lt;wheelAcceleration&gt;1.8&lt;/wheelAcceleration&gt;\n      &lt;commandTopic&gt;cmd_vel&lt;/commandTopic&gt;\n      &lt;odometryFrame&gt;odom&lt;/odometryFrame&gt;\n      &lt;odometryTopic&gt;odom&lt;/odometryTopic&gt;\n      &lt;robotBaseFrame&gt;base_footprint&lt;/robotBaseFrame&gt;\n    &lt;/plugin&gt;\n  &lt;/model&gt;\n&lt;/sdf&gt;\n</code></pre> <p>In case the output looks like the following, there are most certainly missing <code>&lt;inertial&gt;</code> tags in the <code>&lt;link&gt;</code> tag. For the Gazebo simulator the <code>&lt;inertial&gt;</code> must be present, in order to simulate the dynamics of the robot.  See also http://wiki.ros.org/urdf/XML/link and the Gazebo tutorials on URDF.</p> <pre><code>&lt;sdf version='1.7'&gt;\n  &lt;model name='diffbot'/&gt;\n&lt;/sdf&gt;\n</code></pre>"},{"location":"diffbot_mbf/","title":"Move Base Flex","text":""},{"location":"diffbot_mbf/#diffbot-move-base-flex","title":"DiffBot Move Base Flex","text":"<p>As described in the <code>move_base_flex</code> ROS wiki:</p> <p>Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the <code>mbf_abstract_nav</code> and the <code>mbf_abstract_core</code>. For navigation on costmaps see  <code>mbf_costmap_nav</code> and  <code>mbf_costmap_core</code>.</p> <p>This <code>diffbot_mbf</code> package was created using <code>catkin-tools</code> using the following command:</p> <pre><code>catkin create pkg diffbot_mbf                                       \nCreating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"...\nCreated file diffbot_mbf/package.xml\nCreated file diffbot_mbf/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf.\n</code></pre> <p>Additionally the following Ubuntu packages are required dependencies of <code>move_base_flex</code>:</p> <pre><code>sudo apt install ros-noetic-mbf-costmap-nav\n</code></pre> <p>Another working example for turtlebot3 can be found in the <code>turtlebot3_mbf</code> package.</p>"},{"location":"diffbot_msgs/","title":"DiffBot Messages Package","text":"<p>As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics.  ROS provides the <code>std_msgs</code> package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays.  Note howerver, the following from the <code>std_msgs</code> documentation:</p> <p>Quote</p> <p>The types in <code>std_msgs</code> do not convey semantic meaning about their contents: every message simply has a field called \"data\".  Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage.  For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s).</p>"},{"location":"diffbot_msgs/#diffbot-messages","title":"Diffbot Messages","text":"<p>Therefore, we create a package that contains message definitions specific to DiffBot.  The following command uses <code>catkin-tools</code> to create the <code>diffbot_msgs</code> package:</p> <pre><code>ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs                                                           \nCreating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"...\nWARNING: Packages with messages or services should depend on both message_generation and message_runtime\nCreated file diffbot_msgs/package.xml\nCreated file diffbot_msgs/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs.\n</code></pre> <p>Note</p> <p>The following is based on ROS Tutorials Creating Msg And Srv. In this tutorial you can find the required configurations for the <code>package.xml</code> and <code>CMakeLists.txt</code>.</p>"},{"location":"diffbot_msgs/#encoders","title":"Encoders","text":"<p>Currently there is no encoder message definition in ROS (see the <code>sensor_msgs</code> package)  which is why a dedicated message is created for the encoders. For this, a simple msg description file, named <code>Encoders.msg</code> is created in the <code>msg/</code> subdirectory of this <code>diffbot_msgs</code> package:</p> <pre><code># This is a message to hold number of ticks from Encoders\nHeader header\n\n# Use an array of size two of type int32 for the two encoders.\n# int32 is used instead of int64 because it is not supporte by Arduino/Teensy.\n# An overflow is also unlikely with the encoders of the DG01D-E \n# motor with its encoder because of its low encoder resolution\nint32[2] ticks\n</code></pre> <p>The message includes the message type <code>Header</code>  (see also Header msg) which includes common metadata fileds such as timestamp that is automatically  set by ROS client libraries.</p> <p>Having this encoder message description gives semantic meaning to the encoder messages  and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems.</p> <p>There exists also the <code>common_msgs</code> meta package for common, generic robot-specific message types. From the <code>common_msgs</code> DiffBot uses for example the <code>nav_msgs</code> for navigation with the navigation stack. Other relevant message definitions are the <code>sensor_msgs/Imu</code>  and <code>sensor_msgs/LaserScan</code>,  where both are definitions from the <code>sensor_msgs</code> package.</p>"},{"location":"diffbot_msgs/#wheel-commands","title":"Wheel Commands","text":"<p>To command a joint velocity for each wheel <code>diffbot_msgs</code> provides the <code>WheelCmd.msg</code>. This specifies the <code>Header</code> and a float64 array for the angular wheel joint velocities.</p> <pre><code># This is a message that holds commanded angular joint velocity\nHeader header\n\n# Use an array of type float32 for the two wheel joint velocities.\n# float32 is used instead of float64 because it is not supporte by Arduino/Teensy.\nfloat32[] velocities\n</code></pre>"},{"location":"diffbot_msgs/#using-rosmsg","title":"Using rosmsg","text":"<p>After building the package and its messages using <code>catkin build</code> let's make sure that ROS can see it using the rosmsg show command.</p> <pre><code>$ rosmsg show diffbot_msgs/Encoders\nstd_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nint32[2] encoders\n</code></pre> <p>Tip</p> <p>When using the a ros command such as <code>rosmsg</code> make use of the Tab key to auto complete the message name. </p>"},{"location":"diffbot_msgs/#rosserial","title":"ROSSerial","text":"<p>The generated messages in this packages are used on the Teensy microcontroller, which is using <code>rosserial</code>. Integrating these messages requires the following steps.</p> <ul> <li>Generate rosserial libraries in a temporary folder using the <code>make_libraries</code> script:</li> </ul> <pre><code>rosrun rosserial_client make_libraries ~/Arduino/tmp/\n</code></pre> <p>This will generate all messages for ALL installed packages, but in our case only the <code>diffbot_msgs</code> package is needed to avoid missing includes.</p> <ul> <li>Copy the generated <code>~/Arduino/tmp/diffbot_msgs</code> message folder to the <code>src</code> folder of the <code>rosserial</code> Arduino library.   When <code>rosserial</code> was installed with the Arduino Library Manager, the location is <code>~/Arduino/libraries/Rosserial_Arduino_Library/</code>.</li> </ul>"},{"location":"diffbot_msgs/#usage","title":"Usage","text":"<p>The new messages, specific to DiffBot, can be used by including the generated header, for example <code>#include &lt;diffbot_msgs/Encoders.h&gt;</code>.</p>"},{"location":"diffbot_msgs/#references","title":"References","text":"<ul> <li>Tutorials Arduino IDE Setup, specifically Install ros_lib into the Arduino Environment</li> <li><code>rosserial</code> limitations: <code>float64</code> is not supported on Arduino.</li> </ul>"},{"location":"diffbot_navigation/","title":"Navigation","text":""},{"location":"diffbot_navigation/#diffbot-navigation-package","title":"DiffBot Navigation Package","text":"Navigation Stack Overview. <pre><code>fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup                           \nCreating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"...\nCreated file diffbot_navigation/package.xml\nCreated file diffbot_navigation/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation.\n</code></pre> <p>We also need the following ROS packages that can be installed from the ROS Ubuntu packages:</p> <pre><code>$ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base\n</code></pre> <p>After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo.</p>"},{"location":"diffbot_navigation/#launch-files","title":"Launch files","text":"<p>All launch files are in the folder named <code>launch</code> of the <code>diffbot_navigation</code> package.</p> <p>Inside the <code>move_base.launch</code> it is important to remap the following topics:</p> <pre><code>  &lt;!-- Arguments --&gt;\n  &lt;arg name=\"cmd_vel_topic\" default=\"/diffbot/mobile_base_controller/cmd_vel\" /&gt;\n  &lt;arg name=\"odom_topic\" default=\"/diffbot/mobile_base_controller/odom\" /&gt;\n...\n    &lt;!-- remappings of move_base node --&gt;\n    &lt;remap from=\"cmd_vel\" to=\"$(arg cmd_vel_topic)\"/&gt;\n    &lt;remap from=\"odom\" to=\"$(arg odom_topic)\"/&gt;\n</code></pre>"},{"location":"diffbot_navigation/#parameter-configuration","title":"Parameter Configuration","text":"<p>The parameters for the navigation package go into the <code>config</code> (for some robots named <code>param</code>) folder. Most of them can be changed during runtime using dynamic reconfigure with the <code>rqt_reconfigure</code> gui.</p> <ul> <li> <p>Setup and Configuration of the Navigation Stack on a Robot</p> </li> <li> <p><code>amcl</code>: amcl is a probabilistic localization system for a robot moving in 2D.  It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox),  which uses a particle filter to track the pose of a robot against a known map.</p> </li> <li> <p><code>map_server</code>: provides the <code>map_server</code> ROS Node, which offers map data as a ROS Service.  It also provides the <code>map_saver</code> command-line utility, which allows dynamically generated maps to be saved to file.</p> </li> <li> <p><code>move_base</code>: The <code>move_base</code> package provides an implementation of an action  (see the <code>actionlib</code> package) that, given a goal in the world, will attempt to reach it with a mobile base.  The <code>move_base</code> node links together a global and local planner to accomplish its global navigation task.  It supports any global planner adhering to the <code>nav_core::BaseGlobalPlanner</code> interface specified in the <code>nav_core</code> package  and any local planner adhering to the <code>nav_core::BaseLocalPlanner</code> interface specified in the <code>nav_core</code> package.  The <code>move_base</code> node also maintains two costmaps, one for the global planner, and one for a local planner (see the <code>costmap_2d</code> package)  that are used to accomplish navigation tasks.</p> </li> <li> <p><code>gmapping</code>: This package contains a ROS wrapper for OpenSlam's Gmapping.  The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping.  Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot.</p> </li> <li> <p>ROS cartographer</p> </li> <li><code>slam_toolbox</code></li> </ul> <p>Examples - TurtleBot3 Navigation</p>"},{"location":"diffbot_navigation/#navigation-in-gazebo-with-available-map","title":"Navigation in Gazebo with available Map","text":"<p>To navigate the robot in the simulation run the following command but make sure to first download the  turtlebot3_world to your <code>~/.gazebo/models/</code> folder. This is required because the <code>turtlebot3_world.world</code> file references the <code>turtlebot3_world</code> model.</p> <pre><code>roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\n</code></pre> <p>This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz.</p> Navigation demo of DiffBot (click to view the demo on Youtube). <p>To navigate the robot using the default DWA planner in the known map,  coming from the running <code>map_server</code>, you can use the  2D Nav Goal in RViz. Just select the navigation arrow to where the robot should move as shown in the animation above.</p> <p>Note</p> <p>The DWA local planner is working for differential drive robots, like DiffBot.  For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used.  See for example <code>teb_local_planner</code>.</p>"},{"location":"diffbot_navigation/#resources","title":"Resources","text":"<p>Global Planners: - <code>global_planner</code></p> <p>Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner</p>"},{"location":"diffbot_perception/","title":"Diffbot perception","text":""},{"location":"diffbot_perception/#perception","title":"Perception","text":"<p>To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like  OpenCV or PCL.</p> <p>Quote</p> <p>OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision.  Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel).  The library is cross-platform and free for use under the open-source Apache 2 License.  Starting with 2011, OpenCV features GPU acceleration for real-time operations.</p> <p>Quote</p> <p>The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing.  PCL is released under the terms of the BSD license, and thus free for commercial and research use.</p> <p>When working with OpenCV and ROS we have to install required dependency using <code>ros-noetic-vision-opencv</code>. This will install additional dependencies like OpenCV (<code>libopencv-dv</code>) and <code>ros-noetic-cv-bridge</code>. There is no need to install OpenCV from source or the Ubuntu binaries (deb package).</p> <p>If you want to use OpenCV without ROS you should consider installing it from source using CMake.  This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary <code>libopencv-dev</code>.</p> <p>Similar to work with PCL and ROS you need to install the dependency <code>ros-noetic-perception-pcl</code>, which is the interface between ROS and PCL.</p> <p>To work with PCL without ROS you should consider installing it from source using CMake.  This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary <code>libpcl-dev</code>.</p> <p><code>perception_pcl</code> is a meta package for PCL (Point Cloud Library) ROS interface stack.  PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.</p>"},{"location":"diffbot_robot/","title":"DiffBot Robot","text":"<p>The <code>diffbot_robot</code> package is a  ROS metapackage that references all related packages to DiffBot. A metapackage can be created with:</p> <pre><code>fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta    \nCreated file diffbot_robot/package.xml\nCreated file diffbot_robot/CMakeLists.txt\nSuccessfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml.\n</code></pre> <p>To release a package see the bloom page and the listed tutorials there. Specifically the following ones:</p> <ul> <li>To index the package follow the Indexing Your ROS Repository for Documentation Generation.</li> <li>Release a package using bloom, see First Time Release tutorial.</li> </ul> <p>If you released a package and create a PR to the rosdistro repository your PR might be marked as \"held for sync\", which means the following (source):</p> <p>The syncs are how we get updated packages out to end users. The pipeline is essentially:</p> <ul> <li>Package maintainers make source changes to their repository.</li> <li>Once they have enough fixes/features, they do a source release.</li> <li>After a source release, they do a bloom-release to release the package into the distribution (this opens up the rosdistro PRs).</li> <li>After review, the ROS team merges those PRs into GitHub - ros/rosdistro: This repo maintains a lists of repositories for each ROS distribution.</li> <li>The buildfarm notices this, and rebuilds the package (and any packages that depend on this package).</li> <li>Assuming the packages built properly, they are now available at http://packages.ros.org/ros-testing/ubuntu/ for testing. But this is not the same as general availability.</li> <li>Periodically, the ROS team will put a particular ROS distribution into a \"sync freeze\", so that changes stop happening to that distribution.</li> <li>After a testing period, the \u201csync\u201d will be performed from the testing to the main repository. The packages are now available to end-users.</li> </ul>"},{"location":"diffbot_robot/#install-bloom","title":"Install Bloom","text":"<p>Bloom will be used to release our packages.</p> <pre><code>\u279c  diffbot git:(noetic-devel) sudo apt-get install python3-bloom\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following package was automatically installed and is no longer required:\n  libfwupdplugin1\nUse 'sudo apt autoremove' to remove it.\nThe following NEW packages will be installed:\n  python3-bloom\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 68.4 kB of archives.\nAfter this operation, 519 kB of additional disk space will be used.\nGet:1 http://packages.ros.org/ros/ubuntu focal/main amd64 python3-bloom all 0.10.7-100 [68.4 kB]\nFetched 68.4 kB in 2s (38.9 kB/s)\nSelecting previously unselected package python3-bloom.\n(Reading database ... 152404 files and directories currently installed.)\nPreparing to unpack .../python3-bloom_0.10.7-100_all.deb ...\nUnpacking python3-bloom (0.10.7-100) ...\nSetting up python3-bloom (0.10.7-100) ...\n</code></pre>"},{"location":"diffbot_robot/#prepare-and-create-a-new-release","title":"Prepare and Create a New Release","text":"<p>The steps to do create a new release are the following</p> <ol> <li><code>cd</code> to the <code>diffbot</code> repository which contains all the modified packages and make sure there are no uncommitted or untracked changes.</li> <li> <p>Use <code>catkin_generate_changelog</code> to create updated changelogs (populated with the previous commit messages):</p> <p><pre><code>\u279c  diffbot git:(noetic-devel) \u2717 catkin_generate_changelog\nFound packages: diffbot_base, diffbot_bringup, diffbot_control, diffbot_description, diffbot_gazebo, diffbot_mbf, diffbot_msgs, diffbot_navigation, diffbot_robot, diffbot_slam\nQuerying commit information since latest tag...\nUpdating forthcoming section of changelog files...\n- updating 'diffbot_base/CHANGELOG.rst'\n- updating 'diffbot_bringup/CHANGELOG.rst'\n- updating 'diffbot_control/CHANGELOG.rst'\n- updating 'diffbot_description/CHANGELOG.rst'\n- updating 'diffbot_gazebo/CHANGELOG.rst'\n- updating 'diffbot_mbf/CHANGELOG.rst'\n- updating 'diffbot_msgs/CHANGELOG.rst'\n- updating 'diffbot_navigation/CHANGELOG.rst'\n- updating 'diffbot_robot/CHANGELOG.rst'\n- updating 'diffbot_slam/CHANGELOG.rst'\nDone.\nPlease review the extracted commit messages and consolidate the changelog entries before committing the files!\n</code></pre> 3. Clean up the Changelog: Go through every package's CHANGELOG.rst file and modify the commit messages if needed. 4. Commit the updated CHANGELOG.rst files</p> <p><pre><code>\u279c  diffbot git:(noetic-devel) \u2717 git status\nOn branch noetic-devel\nYour branch is up to date with 'origin/noetic-devel'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   diffbot_base/CHANGELOG.rst\n        modified:   diffbot_bringup/CHANGELOG.rst\n        modified:   diffbot_control/CHANGELOG.rst\n        modified:   diffbot_description/CHANGELOG.rst\n        modified:   diffbot_gazebo/CHANGELOG.rst\n        modified:   diffbot_mbf/CHANGELOG.rst\n        modified:   diffbot_msgs/CHANGELOG.rst\n        modified:   diffbot_navigation/CHANGELOG.rst\n        modified:   diffbot_robot/CHANGELOG.rst\n        modified:   diffbot_slam/CHANGELOG.rst\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\u279c  diffbot git:(noetic-devel) \u2717 git add .\n\u279c  diffbot git:(noetic-devel) \u2717 git status\nOn branch noetic-devel\nYour branch is up to date with 'origin/noetic-devel'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   diffbot_base/CHANGELOG.rst\n        modified:   diffbot_bringup/CHANGELOG.rst\n        modified:   diffbot_control/CHANGELOG.rst\n        modified:   diffbot_description/CHANGELOG.rst\n        modified:   diffbot_gazebo/CHANGELOG.rst\n        modified:   diffbot_mbf/CHANGELOG.rst\n        modified:   diffbot_msgs/CHANGELOG.rst\n        modified:   diffbot_navigation/CHANGELOG.rst\n        modified:   diffbot_robot/CHANGELOG.rst\n        modified:   diffbot_slam/CHANGELOG.rst\n\u279c  diffbot git:(noetic-devel) \u2717 git commit -m \"prepare release: updating CHANGELOG.rst files\"\n[noetic-devel d2849d9] prepare release: updating CHANGELOG.rst files\n 10 files changed, 70 insertions(+)\n</code></pre> 5. Bump up the package.xml version</p> <pre><code>\u279c  diffbot git:(noetic-devel) catkin_prepare_release --bump minor\nPrepare the source repository for a release.\nRepository type: git\nFound packages: diffbot_base, diffbot_bringup, diffbot_control, diffbot_description, diffbot_gazebo, diffbot_mbf, diffbot_msgs, diffbot_navigation, diffbot_robot, diffbot_slam\nPrepare release of version '1.1.0' [Y/n]?\nTrying to push to remote repository (dry run)...\nEverything up-to-date\nChecking if working copy is clean (no staged changes, no modified files, no untracked files)...\nRename the forthcoming section of the following packages to version '1.1.0': diffbot_base, diffbot_bringup, diffbot_control, diffbot_description, diffbot_gazebo, diffbot_mbf, diffbot_msgs, diffbot_navigation, diffbot_robot, diffbot_slam\nBump version of all packages from '1.0.0' to '1.1.0'\nCommitting the package.xml files...\n[noetic-devel e7b6c8c] 1.1.0\n 20 files changed, 30 insertions(+), 30 deletions(-)\nCreating tag '1.1.0'...\nThe following commands will be executed to push the changes and tag to the remote repository:\n  /usr/bin/git push origin noetic-devel\n  /usr/bin/git push origin 1.1.0\nExecute commands to push the local commits and tags to the remote repository [Y/n]?\nEnumerating objects: 54, done.\nCounting objects: 100% (54/54), done.\nDelta compression using up to 48 threads\nCompressing objects: 100% (32/32), done.\nWriting objects: 100% (32/32), 5.78 KiB | 227.00 KiB/s, done.\nTotal 32 (delta 24), reused 0 (delta 0)\nremote: Resolving deltas: 100% (24/24), completed with 21 local objects.\nTo https://github.com/ros-mobile-robots/diffbot.git\n   d2849d9..e7b6c8c  noetic-devel -&gt; noetic-devel\nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/ros-mobile-robots/diffbot.git\n * [new tag]         1.1.0 -&gt; 1.1.0\nThe source repository has been released successfully. The next step will be 'bloom-release'.\n</code></pre> </li> </ol> <p>As mentioned in the final output, you are now ready to create a release repository (if not already done)) and follow the rest of the steps to releasing your package.</p>"},{"location":"diffbot_robot/#releasing-your-package-for-the-first-time","title":"Releasing your Package for the first time","text":"<pre><code>\u279c  diffbot git:(noetic-devel) bloom-release --rosdistro noetic --track noetic diffbot --edit\nROS Distro index file associate with commit '725def91e71e2e1a9520416feb916c802ed75314'\nNew ROS Distro index url: 'https://raw.githubusercontent.com/ros/rosdistro/725def91e71e2e1a9520416feb916c802ed75314/index-v4.yaml'\nSpecified repository 'diffbot' is not in the distribution file located at 'https://raw.githubusercontent.com/ros/rosdistro/725def91e71e2e1a9520416feb916c802ed75314/noetic/distribution.yaml'\nDid you mean one of these: 'audibot', 'ifopt', 'iotbot'?\nCould not determine release repository url for repository 'diffbot' of distro 'noetic'\nYou can continue the release process by manually specifying the location of the RELEASE repository.\nTo be clear this is the url of the RELEASE repository not the upstream repository.\nFor release repositories on GitHub, you should provide the `https://` url which should end in `.git`.\nHere is the url for a typical release repository on GitHub: https://github.com/ros-gbp/rviz-release.git\n==&gt; Looking for a release of this repository in a different distribution...\nNo reasonable default release repository url could be determined from previous releases.\nRelease repository url [press enter to abort]: https://github.com/ros-mobile-robots-release/diffbot-release.git\n==&gt; Fetching 'diffbot' repository from 'https://github.com/ros-mobile-robots-release/diffbot-release.git'\nCloning into '/tmp/tmpn6xiovfa'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 592 bytes | 592.00 KiB/s, done.\nWARNING [vcstools] Command failed: 'git checkout master'\n run at: '/tmp/tmpn6xiovfa'\n errcode: 1:\nerror: pathspec 'master' did not match any file(s) known to git\n[/vcstools]\nCreating 'master' branch.\nCreating track 'noetic'...\nRepository Name:\n  &lt;name&gt;\n    Name of the repository (used in the archive name)\n  upstream\n    Default value, leave this as upstream if you are unsure\n  ['upstream']: diffbot\nUpstream Repository URI:\n  &lt;uri&gt;\n    Any valid URI. This variable can be templated, for example an svn url\n    can be templated as such: \"https://svn.foo.com/foo/tags/foo-:{version}\"\n    where the :{version} token will be replaced with the version for this release.\n  [None]: https://github.com/ros-mobile-robots/diffbot.git\nUpstream VCS Type:\n  git\n    Upstream URI is a git repository\n  hg\n    Upstream URI is a hg repository\n  svn\n    Upstream URI is a svn repository\n  tar\n    Upstream URI is a tarball\n  ['git']:\nVersion:\n  :{auto}\n    This means the version will be guessed from the devel branch.\n    This means that the devel branch must be set, the devel branch must exist,\n    and there must be a valid package.xml in the upstream devel branch.\n  :{ask}\n    This means that the user will be prompted for the version each release.\n    This also means that the upstream devel will be ignored.\n  &lt;version&gt;\n    This will be the version used.\n    It must be updated for each new upstream version.\n  [':{auto}']:\nRelease Tag:\n  :{version}\n    This means that the release tag will match the :{version} tag.\n    This can be further templated, for example: \"foo-:{version}\" or \"v:{version}\"\n\n    This can describe any vcs reference. For git that means {tag, branch, hash},\n    for hg that means {tag, branch, hash}, for svn that means a revision number.\n    For tar this value doubles as the sub directory (if the repository is\n    in foo/ of the tar ball, putting foo here will cause the contents of\n    foo/ to be imported to upstream instead of foo itself).\n  :{ask}\n    This means the user will be prompted for the release tag on each release.\n  :{none}\n    For svn and tar only you can set the release tag to :{none}, so that\n    it is ignored.  For svn this means no revision number is used.\n  [':{version}']:\nUpstream Devel Branch:\n  &lt;vcs reference&gt;\n    Branch in upstream repository on which to search for the version.\n    This is used only when version is set to ':{auto}'.\n  [None]:\nROS Distro:\n  &lt;ROS distro&gt;\n    This can be any valid ROS distro, e.g. indigo, kinetic, lunar, melodic\n  ['noetic']:\nPatches Directory:\n  &lt;path in bloom branch&gt;\n    This can be any valid relative path in the bloom branch. The contents\n    of this folder will be overlaid onto the upstream branch after each\n    import-upstream.  Additionally, any package.xml files found in the\n    overlay will have the :{version} string replaced with the current\n    version being released.\n  :{none}\n    Use this if you want to disable overlaying of files.\n  [None]:\nRelease Repository Push URL:\n  &lt;url&gt;\n    (optional) Used when pushing to remote release repositories. This is only\n    needed when the release uri which is in the rosdistro file is not writable.\n    This is useful, for example, when a releaser would like to use a ssh url\n    to push rather than a https:// url.\n  :{none}\n    This indicates that the default release url should be used.\n  [None]:\nCreated 'noetic' track.\n==&gt; Testing for push permission on release repository\n==&gt; git remote -v\norigin  https://github.com/ros-mobile-robots-release/diffbot-release.git (fetch)\norigin  https://github.com/ros-mobile-robots-release/diffbot-release.git (push)\n==&gt; git push --dry-run\nEverything up-to-date\n==&gt; Releasing 'diffbot' using release track 'noetic'\n==&gt; git-bloom-release noetic\nProcessing release track settings for 'noetic'\nChecking upstream devel branch '&lt;default&gt;' for package.xml(s)\nCloning into '/tmp/tmpa99ewrj1/upstream'...\nremote: Enumerating objects: 3242, done.\nremote: Counting objects: 100% (974/974), done.\nremote: Compressing objects: 100% (557/557), done.\nremote: Total 3242 (delta 554), reused 732 (delta 359), pack-reused 2268\nReceiving objects: 100% (3242/3242), 8.26 MiB | 1.99 MiB/s, done.\nResolving deltas: 100% (1801/1801), done.\nLooking for packages in 'noetic-devel' branch... found 10 packages.\nDetected version '1.1.0' from package(s): ['diffbot_navigation', 'diffbot_control', 'diffbot_robot', 'diffbot_bringup', 'diffbot_slam', 'diffbot_gazebo', 'diffbot_msgs', 'diffbot_mbf', 'diffbot_description', 'diffbot_base']\n\nExecuting release track 'noetic'\n==&gt; bloom-export-upstream /tmp/tmpa99ewrj1/upstream git --tag 1.1.0 --display-uri https://github.com/ros-mobile-robots/diffbot.git --name diffbot --output-dir /tmp/tmpig6hnbdi\nChecking out repository at 'https://github.com/ros-mobile-robots/diffbot.git' to reference '1.1.0'.\nExporting to archive: '/tmp/tmpig6hnbdi/diffbot-1.1.0.tar.gz'\nmd5: ea1e312dd269bac40fe06fcc2f7aa323\n\n==&gt; git-bloom-import-upstream /tmp/tmpig6hnbdi/diffbot-1.1.0.tar.gz  --release-version 1.1.0 --replace\nCreating upstream branch.\nImporting archive into upstream branch...\nCreating tag: 'upstream/1.1.0'\nI'm happy.  You should be too.\n\n==&gt; git-bloom-generate -y rosrelease noetic --source upstream -i 1\nReleasing packages: ['diffbot_navigation', 'diffbot_control', 'diffbot_robot', 'diffbot_bringup', 'diffbot_slam', 'diffbot_gazebo', 'diffbot_msgs', 'diffbot_mbf', 'diffbot_description', 'diffbot_base']\nReleasing package 'diffbot_navigation' for 'noetic' to: 'release/noetic/diffbot_navigation'\nReleasing package 'diffbot_control' for 'noetic' to: 'release/noetic/diffbot_control'\nReleasing package 'diffbot_robot' for 'noetic' to: 'release/noetic/diffbot_robot'\nReleasing package 'diffbot_bringup' for 'noetic' to: 'release/noetic/diffbot_bringup'\nReleasing package 'diffbot_slam' for 'noetic' to: 'release/noetic/diffbot_slam'\nReleasing package 'diffbot_gazebo' for 'noetic' to: 'release/noetic/diffbot_gazebo'\nReleasing package 'diffbot_msgs' for 'noetic' to: 'release/noetic/diffbot_msgs'\nReleasing package 'diffbot_mbf' for 'noetic' to: 'release/noetic/diffbot_mbf'\nReleasing package 'diffbot_description' for 'noetic' to: 'release/noetic/diffbot_description'\nReleasing package 'diffbot_base' for 'noetic' to: 'release/noetic/diffbot_base'\n\n==&gt; git-bloom-generate -y rosdebian --prefix release/noetic noetic -i 1 --os-name ubuntu\nGenerating source debs for the packages: ['diffbot_mbf', 'diffbot_description', 'diffbot_bringup', 'diffbot_slam', 'diffbot_robot', 'diffbot_base', 'diffbot_navigation', 'diffbot_gazebo', 'diffbot_control', 'diffbot_msgs']\nDebian Incremental Version: 1\nDebian Distributions: ['focal']\nReleasing for rosdistro: noetic\n\nPre-verifying Debian dependency keys...\nRunning 'rosdep update'...\nAll keys are OK\n\nPlacing debian template files into 'debian/noetic/diffbot_mbf' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_mbf' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-mbf' has dependencies:\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  catkin               =&gt; ['ros-noetic-catkin']\nROS Distro index file associate with commit '725def91e71e2e1a9520416feb916c802ed75314'\nNew ROS Distro index url: 'https://raw.githubusercontent.com/ros/rosdistro/725def91e71e2e1a9520416feb916c802ed75314/index-v4.yaml'\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-mbf_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_mbf' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_description' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_description' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-description' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  joint_state_publisher =&gt; ['ros-noetic-joint-state-publisher']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  rviz                 =&gt; ['ros-noetic-rviz']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  joint_state_publisher =&gt; ['ros-noetic-joint-state-publisher']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  rviz                 =&gt; ['ros-noetic-rviz']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-description_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_description' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_bringup' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_bringup' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-bringup' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  teleop_twist_keyboard =&gt; ['ros-noetic-teleop-twist-keyboard']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-bringup_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_bringup' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_slam' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_slam' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-slam' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  gmapping             =&gt; ['ros-noetic-gmapping']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-slam_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_slam' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_robot' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_robot' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-robot' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  diffbot_base         =&gt; ['ros-noetic-diffbot-base']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  diffbot_control      =&gt; ['ros-noetic-diffbot-control']\n  diffbot_description  =&gt; ['ros-noetic-diffbot-description']\n  diffbot_gazebo       =&gt; ['ros-noetic-diffbot-gazebo']\n  diffbot_navigation   =&gt; ['ros-noetic-diffbot-navigation']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-robot_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_robot' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_base' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_base' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-base' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  diagnostic_updater   =&gt; ['ros-noetic-diagnostic-updater']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  control_toolbox      =&gt; ['ros-noetic-control-toolbox']\n  dynamic_reconfigure  =&gt; ['ros-noetic-dynamic-reconfigure']\n  urdf                 =&gt; ['ros-noetic-urdf']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  diffbot_msgs         =&gt; ['ros-noetic-diffbot-msgs']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  rosserial            =&gt; ['ros-noetic-rosserial']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  diagnostic_updater   =&gt; ['ros-noetic-diagnostic-updater']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  control_toolbox      =&gt; ['ros-noetic-control-toolbox']\n  dynamic_reconfigure  =&gt; ['ros-noetic-dynamic-reconfigure']\n  urdf                 =&gt; ['ros-noetic-urdf']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  diffbot_msgs         =&gt; ['ros-noetic-diffbot-msgs']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-base_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_base' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_navigation' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_navigation' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-navigation' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  amcl                 =&gt; ['ros-noetic-amcl']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  map_server           =&gt; ['ros-noetic-map-server']\n  move_base            =&gt; ['ros-noetic-move-base']\n  base_local_planner   =&gt; ['ros-noetic-base-local-planner']\n  dwa_local_planner    =&gt; ['ros-noetic-dwa-local-planner']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  amcl                 =&gt; ['ros-noetic-amcl']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  map_server           =&gt; ['ros-noetic-map-server']\n  move_base            =&gt; ['ros-noetic-move-base']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-navigation_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_navigation' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_gazebo' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_gazebo' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-gazebo' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  gazebo_plugins       =&gt; ['ros-noetic-gazebo-plugins']\n  gazebo_ros           =&gt; ['ros-noetic-gazebo-ros']\n  gazebo_ros_control   =&gt; ['ros-noetic-gazebo-ros-control']\n  diffbot_control      =&gt; ['ros-noetic-diffbot-control']\n  diffbot_description  =&gt; ['ros-noetic-diffbot-description']\n  xacro                =&gt; ['ros-noetic-xacro']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-gazebo_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_gazebo' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_control' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_control' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-control' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  joint_state_controller =&gt; ['ros-noetic-joint-state-controller']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  transmission_interface =&gt; ['ros-noetic-transmission-interface']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  transmission_interface =&gt; ['ros-noetic-transmission-interface']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-control_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_control' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_msgs' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'focal' debian for package 'diffbot_msgs' at version '1.1.0-1'\n####\nGenerating debian for focal...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-msgs' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; focal key\n  std_msgs             =&gt; ['ros-noetic-std-msgs']\n  message_runtime      =&gt; ['ros-noetic-message-runtime']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; focal key\n  message_generation   =&gt; ['ros-noetic-message-generation']\n  std_msgs             =&gt; ['ros-noetic-std-msgs']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-msgs_1.1.0-1_focal\n####\n#### Successfully generated 'focal' debian for package 'diffbot_msgs' at version '1.1.0-1'\n####\n\n\n==&gt; git-bloom-generate -y rosdebian --prefix release/noetic noetic -i 1 --os-name debian --os-not-required\nGenerating source debs for the packages: ['diffbot_msgs', 'diffbot_gazebo', 'diffbot_description', 'diffbot_robot', 'diffbot_slam', 'diffbot_base', 'diffbot_control', 'diffbot_bringup', 'diffbot_mbf', 'diffbot_navigation']\nDebian Incremental Version: 1\nDebian Distributions: ['buster']\nReleasing for rosdistro: noetic\n\nPre-verifying Debian dependency keys...\nRunning 'rosdep update'...\nAll keys are OK\n\nPlacing debian template files into 'debian/noetic/diffbot_msgs' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_msgs' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-msgs' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  std_msgs             =&gt; ['ros-noetic-std-msgs']\n  message_runtime      =&gt; ['ros-noetic-message-runtime']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  message_generation   =&gt; ['ros-noetic-message-generation']\n  std_msgs             =&gt; ['ros-noetic-std-msgs']\n  catkin               =&gt; ['ros-noetic-catkin']\nROS Distro index file associate with commit '725def91e71e2e1a9520416feb916c802ed75314'\nNew ROS Distro index url: 'https://raw.githubusercontent.com/ros/rosdistro/725def91e71e2e1a9520416feb916c802ed75314/index-v4.yaml'\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-msgs_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_msgs' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_gazebo' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_gazebo' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-gazebo' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  gazebo_plugins       =&gt; ['ros-noetic-gazebo-plugins']\n  gazebo_ros           =&gt; ['ros-noetic-gazebo-ros']\n  gazebo_ros_control   =&gt; ['ros-noetic-gazebo-ros-control']\n  diffbot_control      =&gt; ['ros-noetic-diffbot-control']\n  diffbot_description  =&gt; ['ros-noetic-diffbot-description']\n  xacro                =&gt; ['ros-noetic-xacro']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-gazebo_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_gazebo' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_description' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_description' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-description' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  joint_state_publisher =&gt; ['ros-noetic-joint-state-publisher']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  rviz                 =&gt; ['ros-noetic-rviz']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  joint_state_publisher =&gt; ['ros-noetic-joint-state-publisher']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  rviz                 =&gt; ['ros-noetic-rviz']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-description_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_description' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_robot' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_robot' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-robot' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  diffbot_base         =&gt; ['ros-noetic-diffbot-base']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  diffbot_control      =&gt; ['ros-noetic-diffbot-control']\n  diffbot_description  =&gt; ['ros-noetic-diffbot-description']\n  diffbot_gazebo       =&gt; ['ros-noetic-diffbot-gazebo']\n  diffbot_navigation   =&gt; ['ros-noetic-diffbot-navigation']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-robot_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_robot' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_slam' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_slam' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-slam' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  gmapping             =&gt; ['ros-noetic-gmapping']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-slam_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_slam' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_base' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_base' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-base' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  diagnostic_updater   =&gt; ['ros-noetic-diagnostic-updater']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  control_toolbox      =&gt; ['ros-noetic-control-toolbox']\n  dynamic_reconfigure  =&gt; ['ros-noetic-dynamic-reconfigure']\n  urdf                 =&gt; ['ros-noetic-urdf']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  diffbot_msgs         =&gt; ['ros-noetic-diffbot-msgs']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  rosserial            =&gt; ['ros-noetic-rosserial']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  diagnostic_updater   =&gt; ['ros-noetic-diagnostic-updater']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  control_toolbox      =&gt; ['ros-noetic-control-toolbox']\n  dynamic_reconfigure  =&gt; ['ros-noetic-dynamic-reconfigure']\n  urdf                 =&gt; ['ros-noetic-urdf']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  diffbot_msgs         =&gt; ['ros-noetic-diffbot-msgs']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-base_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_base' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_control' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_control' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-control' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  joint_state_controller =&gt; ['ros-noetic-joint-state-controller']\n  robot_state_publisher =&gt; ['ros-noetic-robot-state-publisher']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  transmission_interface =&gt; ['ros-noetic-transmission-interface']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  controller_manager   =&gt; ['ros-noetic-controller-manager']\n  diff_drive_controller =&gt; ['ros-noetic-diff-drive-controller']\n  hardware_interface   =&gt; ['ros-noetic-hardware-interface']\n  roscpp               =&gt; ['ros-noetic-roscpp']\n  rosparam_shortcuts   =&gt; ['ros-noetic-rosparam-shortcuts']\n  sensor_msgs          =&gt; ['ros-noetic-sensor-msgs']\n  transmission_interface =&gt; ['ros-noetic-transmission-interface']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-control_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_control' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_bringup' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_bringup' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-bringup' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  teleop_twist_keyboard =&gt; ['ros-noetic-teleop-twist-keyboard']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-bringup_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_bringup' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_mbf' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_mbf' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-mbf' has dependencies:\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-mbf_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_mbf' at version '1.1.0-1'\n####\n\nPlacing debian template files into 'debian/noetic/diffbot_navigation' branch.\n==&gt; Placing templates files in the 'debian' folder.\n\n####\n#### Generating 'buster' debian for package 'diffbot_navigation' at version '1.1.0-1'\n####\nGenerating debian for buster...\nNo homepage set, defaulting to ''\nNo historical releaser history, using current maintainer name and email for each versioned changelog entry.\nPackage 'diffbot-navigation' has dependencies:\nRun Dependencies:\n  rosdep key           =&gt; buster key\n  amcl                 =&gt; ['ros-noetic-amcl']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  map_server           =&gt; ['ros-noetic-map-server']\n  move_base            =&gt; ['ros-noetic-move-base']\n  base_local_planner   =&gt; ['ros-noetic-base-local-planner']\n  dwa_local_planner    =&gt; ['ros-noetic-dwa-local-planner']\nBuild and Build Tool Dependencies:\n  rosdep key           =&gt; buster key\n  amcl                 =&gt; ['ros-noetic-amcl']\n  diffbot_bringup      =&gt; ['ros-noetic-diffbot-bringup']\n  map_server           =&gt; ['ros-noetic-map-server']\n  move_base            =&gt; ['ros-noetic-move-base']\n  catkin               =&gt; ['ros-noetic-catkin']\n==&gt; In place processing templates in 'debian' folder.\nExpanding 'debian/copyright.em' -&gt; 'debian/copyright'\nExpanding 'debian/changelog.em' -&gt; 'debian/changelog'\nExpanding 'debian/compat.em' -&gt; 'debian/compat'\nExpanding 'debian/rules.em' -&gt; 'debian/rules'\nExpanding 'debian/source/options.em' -&gt; 'debian/source/options'\nExpanding 'debian/source/format.em' -&gt; 'debian/source/format'\nExpanding 'debian/gbp.conf.em' -&gt; 'debian/gbp.conf'\nExpanding 'debian/control.em' -&gt; 'debian/control'\nCreating tag: debian/ros-noetic-diffbot-navigation_1.1.0-1_buster\n####\n#### Successfully generated 'buster' debian for package 'diffbot_navigation' at version '1.1.0-1'\n####\n\n\n==&gt; git-bloom-generate -y rosrpm --prefix release/noetic noetic -i 1 --os-name fedora\nNo platforms defined for os 'fedora' in release file for the 'noetic' distro.\nNot performing RPM generation.\n\n==&gt; git-bloom-generate -y rosrpm --prefix release/noetic noetic -i 1 --os-name rhel\nNo platforms defined for os 'rhel' in release file for the 'noetic' distro.\nNot performing RPM generation.\n\n\n\n\nTip: Check to ensure that the debian tags created have the same version as the upstream version you are releasing.\nEverything went as expected, you should check that the new tags match your expectations, and then push to the release repo with:\n  git push --all &amp;&amp; git push --tags  # You might have to add --force to the second command if you are over-writing existing tags\n&lt;== Released 'diffbot' using release track 'noetic' successfully\n==&gt; git remote -v\norigin  https://github.com/ros-mobile-robots-release/diffbot-release.git (fetch)\norigin  https://github.com/ros-mobile-robots-release/diffbot-release.git (push)\nReleasing complete, push to release repository?\nContinue [Y/n]? Y\n==&gt; Pushing changes to release repository for 'diffbot'\n==&gt; git push --all\nEnumerating objects: 1108, done.\nCounting objects: 100% (1108/1108), done.\nDelta compression using up to 48 threads\nCompressing objects: 100% (996/996), done.\nWriting objects: 100% (1108/1108), 4.85 MiB | 1.61 MiB/s, done.\nTotal 1108 (delta 243), reused 0 (delta 0)\nremote: Resolving deltas: 100% (243/243), done.\nTo https://github.com/ros-mobile-robots-release/diffbot-release.git\n * [new branch]      debian/noetic/buster/diffbot_base -&gt; debian/noetic/buster/diffbot_base\n * [new branch]      debian/noetic/buster/diffbot_bringup -&gt; debian/noetic/buster/diffbot_bringup\n * [new branch]      debian/noetic/buster/diffbot_control -&gt; debian/noetic/buster/diffbot_control\n * [new branch]      debian/noetic/buster/diffbot_description -&gt; debian/noetic/buster/diffbot_description\n * [new branch]      debian/noetic/buster/diffbot_gazebo -&gt; debian/noetic/buster/diffbot_gazebo\n * [new branch]      debian/noetic/buster/diffbot_mbf -&gt; debian/noetic/buster/diffbot_mbf\n * [new branch]      debian/noetic/buster/diffbot_msgs -&gt; debian/noetic/buster/diffbot_msgs\n * [new branch]      debian/noetic/buster/diffbot_navigation -&gt; debian/noetic/buster/diffbot_navigation\n * [new branch]      debian/noetic/buster/diffbot_robot -&gt; debian/noetic/buster/diffbot_robot\n * [new branch]      debian/noetic/buster/diffbot_slam -&gt; debian/noetic/buster/diffbot_slam\n * [new branch]      debian/noetic/diffbot_base -&gt; debian/noetic/diffbot_base\n * [new branch]      debian/noetic/diffbot_bringup -&gt; debian/noetic/diffbot_bringup\n * [new branch]      debian/noetic/diffbot_control -&gt; debian/noetic/diffbot_control\n * [new branch]      debian/noetic/diffbot_description -&gt; debian/noetic/diffbot_description\n * [new branch]      debian/noetic/diffbot_gazebo -&gt; debian/noetic/diffbot_gazebo\n * [new branch]      debian/noetic/diffbot_mbf -&gt; debian/noetic/diffbot_mbf\n * [new branch]      debian/noetic/diffbot_msgs -&gt; debian/noetic/diffbot_msgs\n * [new branch]      debian/noetic/diffbot_navigation -&gt; debian/noetic/diffbot_navigation\n * [new branch]      debian/noetic/diffbot_robot -&gt; debian/noetic/diffbot_robot\n * [new branch]      debian/noetic/diffbot_slam -&gt; debian/noetic/diffbot_slam\n * [new branch]      debian/noetic/focal/diffbot_base -&gt; debian/noetic/focal/diffbot_base\n * [new branch]      debian/noetic/focal/diffbot_bringup -&gt; debian/noetic/focal/diffbot_bringup\n * [new branch]      debian/noetic/focal/diffbot_control -&gt; debian/noetic/focal/diffbot_control\n * [new branch]      debian/noetic/focal/diffbot_description -&gt; debian/noetic/focal/diffbot_description\n * [new branch]      debian/noetic/focal/diffbot_gazebo -&gt; debian/noetic/focal/diffbot_gazebo\n * [new branch]      debian/noetic/focal/diffbot_mbf -&gt; debian/noetic/focal/diffbot_mbf\n * [new branch]      debian/noetic/focal/diffbot_msgs -&gt; debian/noetic/focal/diffbot_msgs\n * [new branch]      debian/noetic/focal/diffbot_navigation -&gt; debian/noetic/focal/diffbot_navigation\n * [new branch]      debian/noetic/focal/diffbot_robot -&gt; debian/noetic/focal/diffbot_robot\n * [new branch]      debian/noetic/focal/diffbot_slam -&gt; debian/noetic/focal/diffbot_slam\n * [new branch]      master -&gt; master\n * [new branch]      patches/debian/noetic/buster/diffbot_base -&gt; patches/debian/noetic/buster/diffbot_base\n * [new branch]      patches/debian/noetic/buster/diffbot_bringup -&gt; patches/debian/noetic/buster/diffbot_bringup\n * [new branch]      patches/debian/noetic/buster/diffbot_control -&gt; patches/debian/noetic/buster/diffbot_control\n * [new branch]      patches/debian/noetic/buster/diffbot_description -&gt; patches/debian/noetic/buster/diffbot_description\n * [new branch]      patches/debian/noetic/buster/diffbot_gazebo -&gt; patches/debian/noetic/buster/diffbot_gazebo\n * [new branch]      patches/debian/noetic/buster/diffbot_mbf -&gt; patches/debian/noetic/buster/diffbot_mbf\n * [new branch]      patches/debian/noetic/buster/diffbot_msgs -&gt; patches/debian/noetic/buster/diffbot_msgs\n * [new branch]      patches/debian/noetic/buster/diffbot_navigation -&gt; patches/debian/noetic/buster/diffbot_navigation\n * [new branch]      patches/debian/noetic/buster/diffbot_robot -&gt; patches/debian/noetic/buster/diffbot_robot\n * [new branch]      patches/debian/noetic/buster/diffbot_slam -&gt; patches/debian/noetic/buster/diffbot_slam\n * [new branch]      patches/debian/noetic/diffbot_base -&gt; patches/debian/noetic/diffbot_base\n * [new branch]      patches/debian/noetic/diffbot_bringup -&gt; patches/debian/noetic/diffbot_bringup\n * [new branch]      patches/debian/noetic/diffbot_control -&gt; patches/debian/noetic/diffbot_control\n * [new branch]      patches/debian/noetic/diffbot_description -&gt; patches/debian/noetic/diffbot_description\n * [new branch]      patches/debian/noetic/diffbot_gazebo -&gt; patches/debian/noetic/diffbot_gazebo\n * [new branch]      patches/debian/noetic/diffbot_mbf -&gt; patches/debian/noetic/diffbot_mbf\n * [new branch]      patches/debian/noetic/diffbot_msgs -&gt; patches/debian/noetic/diffbot_msgs\n * [new branch]      patches/debian/noetic/diffbot_navigation -&gt; patches/debian/noetic/diffbot_navigation\n * [new branch]      patches/debian/noetic/diffbot_robot -&gt; patches/debian/noetic/diffbot_robot\n * [new branch]      patches/debian/noetic/diffbot_slam -&gt; patches/debian/noetic/diffbot_slam\n * [new branch]      patches/debian/noetic/focal/diffbot_base -&gt; patches/debian/noetic/focal/diffbot_base\n * [new branch]      patches/debian/noetic/focal/diffbot_bringup -&gt; patches/debian/noetic/focal/diffbot_bringup\n * [new branch]      patches/debian/noetic/focal/diffbot_control -&gt; patches/debian/noetic/focal/diffbot_control\n * [new branch]      patches/debian/noetic/focal/diffbot_description -&gt; patches/debian/noetic/focal/diffbot_description\n * [new branch]      patches/debian/noetic/focal/diffbot_gazebo -&gt; patches/debian/noetic/focal/diffbot_gazebo\n * [new branch]      patches/debian/noetic/focal/diffbot_mbf -&gt; patches/debian/noetic/focal/diffbot_mbf\n * [new branch]      patches/debian/noetic/focal/diffbot_msgs -&gt; patches/debian/noetic/focal/diffbot_msgs\n * [new branch]      patches/debian/noetic/focal/diffbot_navigation -&gt; patches/debian/noetic/focal/diffbot_navigation\n * [new branch]      patches/debian/noetic/focal/diffbot_robot -&gt; patches/debian/noetic/focal/diffbot_robot\n * [new branch]      patches/debian/noetic/focal/diffbot_slam -&gt; patches/debian/noetic/focal/diffbot_slam\n * [new branch]      patches/release/noetic/diffbot_base -&gt; patches/release/noetic/diffbot_base\n * [new branch]      patches/release/noetic/diffbot_bringup -&gt; patches/release/noetic/diffbot_bringup\n * [new branch]      patches/release/noetic/diffbot_control -&gt; patches/release/noetic/diffbot_control\n * [new branch]      patches/release/noetic/diffbot_description -&gt; patches/release/noetic/diffbot_description\n * [new branch]      patches/release/noetic/diffbot_gazebo -&gt; patches/release/noetic/diffbot_gazebo\n * [new branch]      patches/release/noetic/diffbot_mbf -&gt; patches/release/noetic/diffbot_mbf\n * [new branch]      patches/release/noetic/diffbot_msgs -&gt; patches/release/noetic/diffbot_msgs\n * [new branch]      patches/release/noetic/diffbot_navigation -&gt; patches/release/noetic/diffbot_navigation\n * [new branch]      patches/release/noetic/diffbot_robot -&gt; patches/release/noetic/diffbot_robot\n * [new branch]      patches/release/noetic/diffbot_slam -&gt; patches/release/noetic/diffbot_slam\n * [new branch]      release/noetic/diffbot_base -&gt; release/noetic/diffbot_base\n * [new branch]      release/noetic/diffbot_bringup -&gt; release/noetic/diffbot_bringup\n * [new branch]      release/noetic/diffbot_control -&gt; release/noetic/diffbot_control\n * [new branch]      release/noetic/diffbot_description -&gt; release/noetic/diffbot_description\n * [new branch]      release/noetic/diffbot_gazebo -&gt; release/noetic/diffbot_gazebo\n * [new branch]      release/noetic/diffbot_mbf -&gt; release/noetic/diffbot_mbf\n * [new branch]      release/noetic/diffbot_msgs -&gt; release/noetic/diffbot_msgs\n * [new branch]      release/noetic/diffbot_navigation -&gt; release/noetic/diffbot_navigation\n * [new branch]      release/noetic/diffbot_robot -&gt; release/noetic/diffbot_robot\n * [new branch]      release/noetic/diffbot_slam -&gt; release/noetic/diffbot_slam\n * [new branch]      upstream -&gt; upstream\n&lt;== Pushed changes successfully\n==&gt; Pushing tags to release repository for 'diffbot'\n==&gt; git push --tags\nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/ros-mobile-robots-release/diffbot-release.git\n * [new tag]         debian/ros-noetic-diffbot-base_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-base_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-base_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-base_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-bringup_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-bringup_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-bringup_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-bringup_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-control_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-control_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-control_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-control_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-description_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-description_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-description_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-description_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-gazebo_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-gazebo_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-gazebo_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-gazebo_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-mbf_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-mbf_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-mbf_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-mbf_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-msgs_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-msgs_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-msgs_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-msgs_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-navigation_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-navigation_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-navigation_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-navigation_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-robot_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-robot_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-robot_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-robot_1.1.0-1_focal\n * [new tag]         debian/ros-noetic-diffbot-slam_1.1.0-1_buster -&gt; debian/ros-noetic-diffbot-slam_1.1.0-1_buster\n * [new tag]         debian/ros-noetic-diffbot-slam_1.1.0-1_focal -&gt; debian/ros-noetic-diffbot-slam_1.1.0-1_focal\n * [new tag]         release/noetic/diffbot_base/1.1.0-1 -&gt; release/noetic/diffbot_base/1.1.0-1\n * [new tag]         release/noetic/diffbot_bringup/1.1.0-1 -&gt; release/noetic/diffbot_bringup/1.1.0-1\n * [new tag]         release/noetic/diffbot_control/1.1.0-1 -&gt; release/noetic/diffbot_control/1.1.0-1\n * [new tag]         release/noetic/diffbot_description/1.1.0-1 -&gt; release/noetic/diffbot_description/1.1.0-1\n * [new tag]         release/noetic/diffbot_gazebo/1.1.0-1 -&gt; release/noetic/diffbot_gazebo/1.1.0-1\n * [new tag]         release/noetic/diffbot_mbf/1.1.0-1 -&gt; release/noetic/diffbot_mbf/1.1.0-1\n * [new tag]         release/noetic/diffbot_msgs/1.1.0-1 -&gt; release/noetic/diffbot_msgs/1.1.0-1\n * [new tag]         release/noetic/diffbot_navigation/1.1.0-1 -&gt; release/noetic/diffbot_navigation/1.1.0-1\n * [new tag]         release/noetic/diffbot_robot/1.1.0-1 -&gt; release/noetic/diffbot_robot/1.1.0-1\n * [new tag]         release/noetic/diffbot_slam/1.1.0-1 -&gt; release/noetic/diffbot_slam/1.1.0-1\n * [new tag]         upstream/1.1.0 -&gt; upstream/1.1.0\n&lt;== Pushed tags successfully\n==&gt; Generating pull request to distro file located at 'https://raw.githubusercontent.com/ros/rosdistro/725def91e71e2e1a9520416feb916c802ed75314/noetic/distribution.yaml'\nWould you like to add documentation information for this repository? [Y/n]? Y\n==&gt; Looking for a doc entry for this repository in a different distribution...\nNo existing doc entries found for use as defaults.\nPlease enter your repository information for the doc generation job.\nThis information should point to the repository from which documentation should be generated.\nVCS Type must be one of git, svn, hg, or bzr.\nVCS type: git\nVCS url: https://github.com/ros-mobile-robots/diffbot.git\nVCS version must be a branch, tag, or commit, e.g. master or 0.1.0\nVCS version: noetic-devel\nWould you like to add source information for this repository? [Y/n]? Y\n==&gt; Looking for a source entry for this repository in a different distribution...\nNo existing source entries found for use as defaults.\nPlease enter information which points to the active development branch for this repository.\nThis information is used to run continuous integration jobs and for developers to checkout from.\nVCS Type must be one of git, svn, hg, or bzr.\nVCS type: git\nVCS url: https://github.com/ros-mobile-robots/diffbot.git\nVCS version must be a branch, tag, or commit, e.g. master or 0.1.0\nVCS version: noetic-devel\nSince you are on github we can add a job to run your tests on each pull request.If you would like to turn this on please see http://wiki.ros.org/buildfarm/Pull%20request%20testing for more information. There is more setup required to setup the hooks correctly.\nWould you like to turn on pull request testing? [y/N]? N\nWould you like to add a maintenance status for this repository? [Y/n]? Y\nPlease enter a maintenance status.\nValid maintenance statuses:\n- developed: active development is in progress\n- maintained: no new development, but bug fixes and pull requests are addressed\n- unmaintained: looking for new maintainer, bug fixes and pull requests will not be addressed\n- end-of-life: should not be used, will disappear at some point\nStatus: developed\nYou can also enter a status description.\nThis is usually reserved for giving a reason when a status is 'end-of-life'.\nStatus Description [press Enter for no change]:\nUnified diff for the ROS distro file located at '/tmp/tmpgx40geam/diffbot-1.1.0-1.patch':\n--- 725def91e71e2e1a9520416feb916c802ed75314/noetic/distribution.yaml\n+++ 725def91e71e2e1a9520416feb916c802ed75314/noetic/distribution.yaml\n@@ -1609,6 +1609,32 @@\n       url: https://github.com/ros/diagnostics.git\n       version: noetic-devel\n     status: maintained\n+  diffbot:\n+    doc:\n+      type: git\n+      url: https://github.com/ros-mobile-robots/diffbot.git\n+      version: 1.1.0\n+    release:\n+      packages:\n+      - diffbot_base\n+      - diffbot_bringup\n+      - diffbot_control\n+      - diffbot_description\n+      - diffbot_gazebo\n+      - diffbot_mbf\n+      - diffbot_msgs\n+      - diffbot_navigation\n+      - diffbot_robot\n+      - diffbot_slam\n+      tags:\n+        release: release/noetic/{package}/{version}\n+      url: https://github.com/ros-mobile-robots-release/diffbot-release.git\n+      version: 1.1.0-1\n+    source:\n+      type: git\n+      url: https://github.com/ros-mobile-robots/diffbot.git\n+      version: 1.1.0\n+    status: developed\n   dingo:\n     doc:\n       type: git\n==&gt; Checking on GitHub for a fork to make the pull request from...\n==&gt; Using this fork to make a pull request from: fjp/rosdistro\n==&gt; Cloning fjp/rosdistro...\n==&gt; mkdir -p rosdistro\n==&gt; git init\nInitialized empty Git repository in /tmp/zis9xzky/rosdistro/.git/\nPull Request Title: diffbot: 1.1.0-1 in 'noetic/distribution.yaml' [bloom]\nPull Request Body :\nIncreasing version of package(s) in repository `diffbot` to `1.1.0-1`:\n\n- upstream repository: https://github.com/ros-mobile-robots/diffbot.git\n- release repository: https://github.com/ros-mobile-robots-release/diffbot-release.git\n- distro file: `noetic/distribution.yaml`\n- bloom version: `0.10.7`\n- previous version for package: `null`\n\n## diffbot_base\n\n```\n* fix: missing subscriber initialization (#54 &lt;https://github.com/ros-mobile-robots/diffbot/issues/54&gt;)\n* feat: warn if eStop() is called on mcu\n* Change base config: switch motor pins\n* feat: add callbacks for low level pids on mcu #54 &lt;https://github.com/ros-mobile-robots/diffbot/issues/54&gt;\n  For MCU firmware\n  - Add subscribers for left and right motor pid controllers using\n  diffbot_msgs::PIDStamped custom message\n  - Update debug logging message (different formatting)\n  - Update PID controller interface (provide proportional_, integral_ and\n  derivative_ values)\n* fix: missing diffbot namespace in test\n* fix: test_encoders.cpp include name\n* feat: add documentation link for unity\n* Merge pull request #42 &lt;https://github.com/ros-mobile-robots/diffbot/issues/42&gt; from joeuser846/motor-defines\n  Use MOTOR_LEFT/RIGHT from diffbot_base_config.h instead of hardcoding pin numbers\n* Fix filenames\n* Use MOTOR_LEFT/RIGHT instead of hardcoding pin numbers\n* Contributors: Franz Pucher, Joe User\n```\n\n## diffbot_bringup\n\n```\n* fix rplidar_laser_link name issue (#40 &lt;https://github.com/ros-mobile-robots/diffbot/issues/40&gt;, #53 &lt;https://github.com/ros-mobile-robots/diffbot/issues/53&gt;)\n  - Rename rplidar_gpu_laser_link to rplidar_laser_link in bringup_with_laser.launch\n  - Add rplidar.launch to diffbot_bringup to support framed_id argument\n  - Make use of new diffbot_bringup/launch/rplidar.launch in bringup_with_laser.launch\n  This solves issues in RViz:\n  Transform [sender=unknown_publisher]\n  For frame [rplidar_gpu_laser_link]: Frame [rplidar_gpu_laser_link] does not exist\n  and in the terminal from whic diffbot_slam is launched:\n  [ WARN] [1635345613.864692611]: MessageFilter [target=odom ]: Dropped 100.00% of messages so far. Please turn the [ros.gmapping.message_filter] rosconsole logger to DEBUG for more information.\n* Contributors: Franz Pucher\n```\n\n## diffbot_control\n\n```\n* fix robot spawning in world origin using pid gains (#57 &lt;https://github.com/ros-mobile-robots/diffbot/issues/57&gt;)\n* include pid.yaml to avoid Gazebo error messages\n* Contributors: Franz Pucher\n```\n\n## diffbot_description\n\n```\n* fix deprecated warning using load_yaml (#53 &lt;https://github.com/ros-mobile-robots/diffbot/issues/53&gt;)\n  Using xacro.load_yaml instead of just load_yaml\n* Contributors: Franz Pucher\n```\n\n## diffbot_gazebo\n\n```\n* Update diffbot_gazebo/diffbot_view.launch\n  Use db_world as default instead of corridor world\n* Contributors: Franz Pucher\n```\n\n## diffbot_mbf\n\n- No changes\n\n## diffbot_msgs\n\n- No changes\n\n## diffbot_navigation\n\n```\n* Fix minor error in amcl.launch\n  The \"kld_err\" was initialized twice, and with the wrong value. Corrected to initialize the \"kld_error\" parameter to 0.01 and \"kld_z\" to 0.99.\n* Contributors: Rodrigo Silverio\n```\n\n## diffbot_robot\n\n- No changes\n\n## diffbot_slam\n\n- No changes\n\nOpen a pull request from 'fjp/rosdistro:bloom-diffbot-0' into 'ros/rosdistro:master'?\nContinue [Y/n]?\n==&gt; git checkout -b bloom-diffbot-0\nSwitched to a new branch 'bloom-diffbot-0'\n==&gt; Pulling latest rosdistro branch\nremote: Enumerating objects: 170051, done.\nremote: Counting objects: 100% (86/86), done.\nremote: Compressing objects: 100% (65/65), done.\nremote: Total 170051 (delta 41), reused 0 (delta 0), pack-reused 169965\nReceiving objects: 100% (170051/170051), 105.06 MiB | 4.12 MiB/s, done.\nResolving deltas: 100% (101945/101945), done.\nFrom https://github.com/ros/rosdistro\n * branch            master     -&gt; FETCH_HEAD\n==&gt; git reset --hard 725def91e71e2e1a9520416feb916c802ed75314\nHEAD is now at 725def91e Retarget ament_acceleration repos to rolling branch (#32587)\n==&gt; Writing new distribution file: noetic/distribution.yaml\n==&gt; git add noetic/distribution.yaml\n==&gt; git commit -m \"diffbot: 1.1.0-1 in 'noetic/distribution.yaml' [bloom]\"\n[bloom-diffbot-0 57ef230a8] diffbot: 1.1.0-1 in 'noetic/distribution.yaml' [bloom]\n 1 file changed, 26 insertions(+)\n==&gt; Pushing changes to fork\nEnumerating objects: 7, done.\nCounting objects: 100% (7/7), done.\nDelta compression using up to 48 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (4/4), 574 bytes | 574.00 KiB/s, done.\nTotal 4 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nremote:\nremote: Create a pull request for 'bloom-diffbot-0' on GitHub by visiting:\nremote:      https://github.com/fjp/rosdistro/pull/new/bloom-diffbot-0\nremote:\nTo https://github.com/fjp/rosdistro.git\n * [new branch]          bloom-diffbot-0 -&gt; bloom-diffbot-0\n&lt;== Pull request opened at: https://github.com/ros/rosdistro/pull/32590\n</code></pre> <p>View the pull request at https://github.com/ros/rosdistro/pull/32590 and check that the tests (e.g. Nose test) pass.</p>"},{"location":"diffbot_slam/","title":"SLAM","text":""},{"location":"diffbot_slam/#diffbot-slam-package","title":"DiffBot Slam Package","text":"<p>This package contains launch files and configurations for different  simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D.</p> <pre><code>fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping\nCreating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"...\nCreated file diffbot_slam/package.xml\nCreated file diffbot_slam/CMakeLists.txt\nSuccessfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam.\n</code></pre> <p>Additional runtime dependencies are: <code>cartographer_ros</code>, <code>hector_slam</code>, <code>frontier_exploration</code> and <code>explore_lite</code>. These are added to this workspace using <code>vcstool</code> (TODO).</p> <p>As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as <code>exec_depend</code> in the <code>package.xml</code>. Currently only <code>gmapping</code> provides a ROS Noetic Ubuntu package that can be installed directly with:</p> <pre><code>sudo apt install ros-noetic-gmapping\n</code></pre> <p>In case you want to try more advanced SLAM algorithms, such as <code>karto_slam</code> or <code>cartographer_ros</code> you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace.</p> <p>Take the required installation size into account. For example <code>karto_slam</code> needs approximately 125MB because it will also install <code>ros-noetic-open-karto</code>.</p> <pre><code>sudo apt install ros-noetic-slam-karto\n</code></pre>"},{"location":"diffbot_slam/#slam","title":"SLAM","text":"<p>SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM.</p> <p>To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted,  fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser <code>/scan</code> and the <code>/tf</code> topics.  Transforms are necessary to relate frames for laser, base, and odometry. The only exception is <code>hector_slam</code> which doesn't require odometry for mapping.</p> <p>The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with <code>gmapping</code> which is used by default.</p> <ul> <li><code>gmapping</code>: This package contains a ROS wrapper for OpenSlam's Gmapping.  The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called <code>slam_gmapping</code>.  Using <code>slam_gmapping</code>, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot.</li> <li><code>cartographer</code>: Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an  algorithm walkthrough.</li> <li><code>karto</code>: This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example <code>gmapping</code> (note: for ROS noetic, see <code>slam_karto</code>) and became open source in 2010.</li> <li><code>hector_slam</code>: metapackage that installs <code>hector_mapping</code> and related packages.  The <code>hector_mapping</code> is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs.</li> </ul> <p>Unlike <code>gmapping</code> which uses a particle filter,  <code>karto</code>, <code>cartographer</code> and <code>hector_slam</code> are all graph-based SLAM algorithms. The least accurate SLAM algorithm is <code>gmapping</code> but it works fine for smaller maps. Use other algorithms, such as <code>karto</code> if you operate your robot in larger environments or you want more accuracy.</p> <p>Another interesing package is <code>slam_toolbox</code> which provides ROS1 and ROS2  support and is based on the easy to use <code>karto</code> algorithm. <code>karto</code> is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, <code>slam_toolbox</code> provides tools to edit a generated map and even create a high quality  map using stored data (offline).</p> <p>The <code>cartographer</code> package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors.</p>"},{"location":"diffbot_slam/#launch-files","title":"Launch files","text":"<p>This package provides a main launch file named <code>diffbot_slam.launch</code> which accepts an argument <code>slam_method</code>. Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the <code>config</code> folder.</p> <p>As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named <code>/scan</code>. To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace <code>/diffbot/scan</code>. Therefore, its necessary to remap the <code>/scan</code> topic to <code>/diffbot/scan</code>. The following shows how this was done for the <code>gmapping</code> launch file.</p> <p>Inside this package in the <code>launch/gmapping.launch</code>  it is important to map the <code>scan</code> topic to laser scanner topic published by Diffbot. Remappings are done in the node tag. Here, for the <code>gmapping.launch</code> in the <code>gmapping</code> node: </p> <pre><code>&lt;launch&gt;\n  &lt;!-- Arguments --&gt;\n  &lt;arg name=\"scan_topic\"  default=\"diffbot/scan\"/&gt;\n...\n  &lt;!-- Gmapping --&gt;\n  &lt;node pkg=\"gmapping\" type=\"slam_gmapping\" name=\"diffbot_slam_gmapping\" output=\"screen\"&gt;\n    ...\n    &lt;!-- remapping of gmapping node --&gt;\n    &lt;remap from=\"scan\" to=\"$(arg scan_topic)\"/&gt;\n  &lt;/node&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"diffbot_slam/#parameter-configurations","title":"Parameter Configurations","text":"<p>Most of the configrations are the same as <code>turtlebot3_slam/config</code>. For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods. </p>"},{"location":"diffbot_slam/#gazebo-simulation-tests","title":"Gazebo Simulation Tests","text":"<p>To test SLAM in the Gazebo simulator run the following two launch files in separate terminals.</p> <p>First run the simulation with:</p> <pre><code>roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\n</code></pre> <p>and in a second terminal execute the SLAM algorithm:</p> <pre><code>roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping\n</code></pre> <p>Here you can choose between different algorithms by changing the value of the <code>slam_method</code> argument. Possible values are <code>gmapping</code> (the default), <code>karto</code>, <code>hector</code> and <code>cartographer</code>.</p> <p>The ROS node graph will look like the following:</p> ROS Node graph after launching Gazebo and gmapping. <p>In the figure we can see that <code>gmapping</code> subscribes and publishes to <code>tf</code>. </p> <p>It requires the transformation from <code>&lt;the frame attached to incoming scans&gt;</code> to the <code>base_link</code>, which is usually a fixed value,  broadcast periodically by the <code>robot_state_publisher</code>. Aditionally, it requires the transform from <code>base_link</code> to <code>odom</code>. This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU.  The provided tf transforms are <code>map</code> to <code>odom</code> that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation.</p>"},{"location":"diffbot_slam/#field-tests","title":"Field Tests","text":"<p>In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation.</p>"},{"location":"diffbot_slam/#frontier-exploration","title":"Frontier Exploration","text":"<p>The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its  map into new territory until the entire environment has been explored. </p> <p>The ROS wiki provides a good tutorial using Husky robot how to use the <code>frontier_exploration</code> package. A lightweight alternative is the <code>explore_lite</code> package.</p>"},{"location":"diffbot_slam/#other-slam-packages-for-3d-mapping","title":"Other SLAM Packages (for 3D Mapping)","text":"<ul> <li><code>hdl_graph_slam</code>: Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR.  It is based on 3D Graph SLAM with  NDT  scan matching-based odometry estimation and loop detection. This method is useful for outdoor.</li> <li>RTAB-Map: stands for Real-Time Appearance-Based Mapping and  is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints.  This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation.  To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result.</li> <li>Loam Velodyne: Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for <code>loam_velodyne</code>. Note that this is not supported officially anymore because it became closed source.</li> <li>ORB-SLAM2: Real-Time SLAM for Monocular, Stereo and RGB-D Cameras,  with Loop Detection and Relocalization Capabilities. See <code>orb_slam2_ros</code> for the ROS wrapper.</li> <li>slam_toolbox: This package provides a sped up improved slam karto with updated  SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video.</li> </ul>"},{"location":"diffbot_slam/#references","title":"References","text":"<ul> <li><code>slam_toolbox</code>, Slam Toolbox ROSCon 2019 pdf</li> </ul> <p>Papers:</p> <ul> <li>A Tutorial on Graph-Based SLAM</li> <li><code>cartographer</code> Real-Time Loop Closure in 2D LIDAR SLAM</li> <li><code>hector_slam</code> A flexible and scalable SLAM system with full 3D motion estimation.</li> <li>A practical introduction to to pose graph slam</li> <li>The Normal Distributions Transform: A New Approach to Laser Scan Matching</li> <li>RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation</li> <li>LOAM: Lidar Odometry and Mapping in Real-time</li> <li>ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras</li> </ul>"},{"location":"grove_motor_driver/","title":"Motor Driver","text":""},{"location":"grove_motor_driver/#grove-i2c-motor-driver-v13","title":"Grove - I2C Motor Driver V1.3","text":"<p>The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with <code>catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]]</code>:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs\nCreating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"...\nCreated file grove_motor_driver/CMakeLists.txt\nCreated file grove_motor_driver/package.xml\nCreated folder grove_motor_driver/include/grove_motor_driver\nCreated folder grove_motor_driver/src\nSuccessfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver.\n</code></pre> <p>The package depends on the two ROS client libraries <code>rospy</code> and <code>roscpp</code>.  To control the two motors the package will use the <code>geometry_msgs/Twist</code> message. The interface to the motor driver is done with the Python library from DexterInd  which is a rewerite of the Seeed Studio Arduino library.</p> <p>This library requires the following two python libraries</p> <ul> <li>RPi.GPIO</li> <li>smbus SMBus (System Management Bus) is a subset from the I2C protocol</li> </ul> <p>These libraries should be installed with <code>pip3</code>, Python's package manager:</p> <pre><code>pip3 install RPi.GPIO\npip3 install smbus\n</code></pre> <p>Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to  operate for this purpose. For a development environment it is best practice to use a python virtual environment like  <code>venv</code> and install the packages inside it.</p>"},{"location":"grove_motor_driver/#connection","title":"Connection","text":"<p>Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B.  Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector.  Only the SDA (data) and SCL (clock) wires are required.</p> <p>Make sure to set the address with the dip switches on the motor driver to <code>0x0f</code> because this is the default address used in the library files.</p> <p>To test the physical I2C connection use <code>i2cdetect</code> described in Hardware Interfaces: The output should list <code>0f</code> in the address table:</p> <pre><code>$ i2cdetect -y 1\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- 0f \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: -- -- -- -- -- -- -- --\n</code></pre>"},{"location":"grove_motor_driver/#test-motor-driver","title":"Test Motor Driver","text":"<p>Test the motor driver by running one of the python files:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py \nForward\nBack\nStop\nSpeed: 0\nSpeed: 1\nSpeed: 2\nSpeed: 3\nSpeed: 4\nSpeed: 5\nSpeed: 6\nSpeed: 7\nSpeed: 8\nSpeed: 9\nSpeed: 10\nSpeed: 11\nSpeed: 12\n...\nSpeed: 25\nSpeed: 26\nSpeed: 27\nSpeed: 28\nSpeed: 29\nSpeed: 30\nSpeed: 31\nSpeed: 32\nSpeed: 33\n...\nSpeed: 55\nSpeed: 56\nSpeed: 57\nSpeed: 58\nSpeed: 59\nSpeed: 60\nSpeed: 61\nSpeed: 62\nSpeed: 63\nSpeed: 64\nSpeed: 65\nSpeed: 66\n...\nSpeed: 75\nSpeed: 76\nSpeed: 77\nSpeed: 78\nSpeed: 79\nSpeed: 80\nSpeed: 81\nSpeed: 82\nSpeed: 83\nSpeed: 84\nSpeed: 85\nSpeed: 86\nSpeed: 87\nSpeed: 88\n...\nSpeed: 97\nSpeed: 98\nSpeed: 99\nStop\n</code></pre>"},{"location":"grove_motor_driver/#troubleshooting","title":"Troubleshooting","text":"<p>If you get errors like the following, make sure the I2C cables from the motor driver to the  Raspberry Pi are connected (see Hardware Interfaces for more infos)  and use the <code>RESET</code> button on the motor driver.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py \nTraceback (most recent call last):\n  File \"grove_i2c_motor_driver.py\", line 68, in &lt;module&gt;\n    m.MotorSpeedSetAB(100,100)\n  File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB\n    bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB])\nIOError: [Errno 110] Connection timed out\nfjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py \nTraceback (most recent call last):\n  File \"grove_i2c_motor_driver.py\", line 68, in &lt;module&gt;\n    m.MotorSpeedSetAB(100,100)\n  File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB\n    bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB])\nIOError: [Errno 121] Remote I/O error\n</code></pre> <p>Try pressing the <code>RESET</code> button and release it right before executing one of the scripts. You should also be able to detect the motor driver with <code>i2cdetect -y 1</code>:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- 0f \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: -- -- -- -- -- -- -- --\n</code></pre> <p>As you can see the address of the motor driver is detected at <code>0x0f</code>.</p> <p>In case of the following output, where every address of the I2C bus seems to be taken, it is most likely that the SDA (data) and SCL (clock) signal cables are switched:</p> <pre><code>i2cdetect -y 1\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f \n10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f \n20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f \n30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f \n40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f \n50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f \n60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f \n70: 70 71 72 73 74 75 76 77\n</code></pre> <p>To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos.</p> <p>Another solution is to restart the Raspberry Pi while making sure that the motor driver is powerd on by connecting it to the battery pack.</p>"},{"location":"grove_motor_driver/#ros-node-for-motor-driver","title":"ROS Node for Motor Driver","text":"<p>To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called <code>motor_driver</code>. The node subscribes to the topic <code>/2wd_robot/cmd_vel</code> which is of type Twist message  from the geometry_msgs header.  To send commands to the motor the . These topics can be published with nodes from the navigation stack or with <code>rostopic pub</code> for test purposes.</p> <p>https://en.wikipedia.org/wiki/Differential_wheeled_robot</p> <p>http://wiki.ros.org/differential_drive</p> <p>After the node has been implemented, we need to build the workspace with <code>catkin build</code>:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ catkin build\n----------------------------------------------------------------\nProfile:                     default\nExtending:          [cached] /opt/ros/melodic\nWorkspace:                   /home/fjp/git/2wd-robot/ros\n----------------------------------------------------------------\nBuild Space:        [exists] /home/fjp/git/2wd-robot/ros/build\nDevel Space:        [exists] /home/fjp/git/2wd-robot/ros/devel\nInstall Space:      [unused] /home/fjp/git/2wd-robot/ros/install\nLog Space:          [exists] /home/fjp/git/2wd-robot/ros/logs\nSource Space:       [exists] /home/fjp/git/2wd-robot/ros/src\nDESTDIR:            [unused] None\n----------------------------------------------------------------\nDevel Space Layout:          linked\nInstall Space Layout:        None\n----------------------------------------------------------------\nAdditional CMake Args:       None\nAdditional Make Args:        None\nAdditional catkin Make Args: None\nInternal Make Job Server:    True\nCache Job Environments:      False\n----------------------------------------------------------------\nWhitelisted Packages:        None\nBlacklisted Packages:        None\n----------------------------------------------------------------\nWorkspace configuration appears valid.\n----------------------------------------------------------------\n[build] Found '2' packages in 0.0 seconds.                                                                          \n[build] Updating package table.                                                                                     \nStarting  &gt;&gt;&gt; grove_motor_driver                                                                                    \nStarting  &gt;&gt;&gt; grove_ultrasonic_ranger                                                                                                                                                                              \nFinished  &lt;&lt;&lt; grove_motor_driver                     [ 1.0 seconds ]                                                \nFinished  &lt;&lt;&lt; grove_ultrasonic_ranger                [ 1.0 seconds ]                                                                                             \n[build] Summary: All 2 packages succeeded!                                                                          \n[build]   Ignored:   None.                                                                                          \n[build]   Warnings:  None.                                                                                          \n[build]   Abandoned: None.                                                                                          \n[build]   Failed:    None.                                                                                          \n[build] Runtime: 2.0 seconds total. \n</code></pre> <p>As the final note of the build output suggests, we have to <code>source</code> the <code>setup.bash</code> files in the <code>devel</code> space.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash\n</code></pre> <p>To make the <code>ranger</code> node executable we have to modify the <code>ranger.py</code> file:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py\n</code></pre>"},{"location":"grove_ultrasonic_ranger/","title":"Grove ultrasonic ranger","text":""},{"location":"grove_ultrasonic_ranger/#grove-ultrasonic-ranger","title":"Grove - Ultrasonic Ranger","text":"<p>To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with <code>catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]]</code>:</p> <p><pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs\nCreating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\"...\nCreated file grove_ultrasonic_ranger/CMakeLists.txt\nCreated file grove_ultrasonic_ranger/package.xml\nCreated folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger\nCreated folder grove_ultrasonic_ranger/src\n</code></pre> The package depends on the two ROS client libraries <code>rospy</code> and <code>roscpp</code>.  To signalise the current distance to obstacles the <code>sensor_msgs/Range</code> message is used.</p> <p>After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available  python script <code>ultrasonic.py</code> from Seed Studio.</p> <p>The following shows the truncated output of the <code>ultrasonic.py</code> script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. </p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py\nSeeedStudio Grove Ultrasonic get data and print\nUltrasonic Measurement\nDistance : 2.0 CM\nUltrasonic Measurement\nDistance : 2.1 CM\nUltrasonic Measurement\nDistance : 2.0 CM\nUltrasonic Measurement\nDistance : 3.5 CM\nUltrasonic Measurement\nDistance : 3.5 CM\nUltrasonic Measurement\nDistance : 2.4 CM\nUltrasonic Measurement\nDistance : 12.9 CM\n...\nUltrasonic Measurement\nDistance : 45.8 CM\nUltrasonic Measurement\nDistance : 510.7 CM\nUltrasonic Measurement\nDistance : 510.7 CM\nUltrasonic Measurement\nDistance : 45.7 CM\nUltrasonic Measurement\nDistance : 161.8 CM\nUltrasonic Measurement\nDistance : 26.9 CM\nUltrasonic Measurement\nDistance : 18.3 CM\nUltrasonic Measurement\nDistance : 160.9 CM\nUltrasonic Measurement\nDistance : 158.3 CM\nUltrasonic Measurement\nDistance : 159.4 CM\n...\n</code></pre>"},{"location":"grove_ultrasonic_ranger/#modified-groveultrasonicranger-library","title":"Modified GroveUltrasonicRanger Library","text":"<p>To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger (API) Therefore I copied the core functionality of the <code>ultrasonic.py</code> script from Seeed Studio in a class named <code>GroveUltrasonicRanger</code>.</p> <p>Executing the <code>grove_ultrasonic_ranger.py</code> will result in the following output:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py \nSeeedStudio Grove Ultrasonic get data and print\nDistance : 0.051 m\nDistance : 0.069 m\nDistance : 0.098 m\nDistance : 0.131 m\nDistance : 0.153 m\nDistance : 0.172 m\nDistance : 0.207 m\nDistance : 0.210 m\nDistance : 0.234 m\nDistance : 0.256 m\nGPIO.cleanup()\nGPIO.cleanup() done\n</code></pre>"},{"location":"grove_ultrasonic_ranger/#ros-node-for-ultrasonic-ranger","title":"ROS Node for Ultrasonic Ranger","text":"<p>ROS provides the Range Message in the sensor_msgs header. This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor.</p> <p>To design this node we will send out measurements periodically over a topic of type <code>sensor_msgs/Range</code>. The code for this node is in <code>ranger.py</code>.</p> <p>After writing the node we need to build the packages in the workspace with <code>catkin build</code>.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ catkin build\n----------------------------------------------------------------\nProfile:                     default\nExtending:             [env] /opt/ros/melodic\nWorkspace:                   /home/fjp/git/2wd-robot/ros\n----------------------------------------------------------------\nBuild Space:        [exists] /home/fjp/git/2wd-robot/ros/build\nDevel Space:        [exists] /home/fjp/git/2wd-robot/ros/devel\nInstall Space:      [unused] /home/fjp/git/2wd-robot/ros/install\nLog Space:         [missing] /home/fjp/git/2wd-robot/ros/logs\nSource Space:       [exists] /home/fjp/git/2wd-robot/ros/src\nDESTDIR:            [unused] None\n----------------------------------------------------------------\nDevel Space Layout:          linked\nInstall Space Layout:        None\n----------------------------------------------------------------\nAdditional CMake Args:       None\nAdditional Make Args:        None\nAdditional catkin Make Args: None\nInternal Make Job Server:    True\nCache Job Environments:      False\n----------------------------------------------------------------\nWhitelisted Packages:        None\nBlacklisted Packages:        None\n----------------------------------------------------------------\nWorkspace configuration appears valid.\n\nNOTE: Forcing CMake to run for each package.\n----------------------------------------------------------------\n[build] Found '1' package in 0.0 seconds.                                                                                                \n[build] Updating package table.                                                                                                           \nStarting  &gt;&gt;&gt; catkin_tools_prebuild                                                                                                       \nFinished  &lt;&lt;&lt; catkin_tools_prebuild                  [ 9.9 seconds ]                                                                                                                                                                                         \nStarting  &gt;&gt;&gt; grove_ultrasonic_ranger                                                                                                                                                                                                                                                                                                                                                         \nFinished  &lt;&lt;&lt; grove_ultrasonic_ranger                [ 12.0 seconds ]                                                                     \n[build] Summary: All 2 packages succeeded!                                                                                                \n[build]   Ignored:   None.                                                                                                                \n[build]   Warnings:  None.                                                                                                                \n[build]   Abandoned: None.                                                                                                                \n[build]   Failed:    None.                                                                                                                \n[build] Runtime: 21.9 seconds total.                                                                                                      \n[build] Note: Workspace packages have changed, please re-source setup files to use them.\n</code></pre> <p>As the final note of the build output suggests, we have to <code>source</code> the <code>setup.bash</code> files in the <code>devel</code> space.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash\n</code></pre> <p>To make the <code>ranger</code> node executable we have to modify the <code>ranger.py</code> file:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py\n</code></pre> <p>Then we can test the node using <code>rosrun</code>:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ sudo su\n[sudo] password for fjp:\nroot@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash \nroot@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py \nDistance : 1.617 m\nDistance : 1.617 m\nDistance : 1.617 m\nDistance : 0.108 m\nDistance : 0.092 m\nDistance : 0.099 m\n</code></pre> <p>This lets the node publish range messages which we can capture in another terminal window using <code>rostopic</code>. First we use <code>rostopic list</code> to find the name of the topic we are interested in:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list\n/distance\n/rosout\n/rosout_agg\n</code></pre> <p>We named our topic <code>/distance</code> which we can use with the <code>rostopic echo</code> command to see the published messages:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance\nheader: \n  seq: 1\n  stamp: \n    secs: 1576778377\n    nsecs: 746809959\n  frame_id: \"ranger_distance\"\nradiation_type: 0\nfield_of_view: 0.261799007654\nmin_range: 0.019999999553\nmax_range: 3.5\nrange: 1.61674261093\n---\nheader: \n  seq: 2\n  stamp: \n    secs: 1576778378\n    nsecs: 459048986\n  frame_id: \"ranger_distance\"\nradiation_type: 0\nfield_of_view: 0.261799007654\nmin_range: 0.019999999553\nmax_range: 3.5\nrange: 1.61261284351\n---\nheader: \n  seq: 3\n  stamp: \n    secs: 1576778379\n    nsecs: 172172069\n  frame_id: \"ranger_distance\"\nradiation_type: 0\nfield_of_view: 0.261799007654\nmin_range: 0.019999999553\nmax_range: 3.5\nrange: 1.61657905579\n---\nheader: \n  seq: 4\n  stamp: \n    secs: 1576778379\n    nsecs: 884002923\n  frame_id: \"ranger_distance\"\nradiation_type: 0\nfield_of_view: 0.261799007654\nmin_range: 0.019999999553\nmax_range: 3.5\nrange: 1.61277639866\n---\nheader: \n  seq: 5\n  stamp: \n    secs: 1576778380\n    nsecs: 596549034\n  frame_id: \"ranger_distance\"\nradiation_type: 0\nfield_of_view: 0.261799007654\nmin_range: 0.019999999553\nmax_range: 3.5\nrange: 1.67693090439\n---\n</code></pre>"},{"location":"grove_ultrasonic_ranger/#informational-distance-measurements","title":"Informational Distance Measurements","text":"<p>To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline.</p> <p>To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form:</p> <pre><code>if(minimum_range &lt;= value &amp;&amp; value &lt;= maximum_range){  // Represents expected pre-REP logic and is the only necessary condition for most applications.\n    // This is a valid measurement.\n} else if( !isfinite(value) &amp;&amp; value &lt; 0){\n    // Object too close to measure.\n} else if( !isfinite(value) &amp;&amp; value &gt; 0){\n    // No objects detected in range.\n} else if( isnan(value) ){\n    // This is an erroneous, invalid, or missing measurement.\n} else {\n    // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range.\n}\n</code></pre>"},{"location":"laser-range-scanner/","title":"Laser Range Scanner","text":""},{"location":"laser-range-scanner/#laser-range-scanner","title":"Laser Range Scanner","text":"<p>For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical  requirement to use the ROS navigation stack (see also <code>diffbot_navigation</code>).  DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range.</p>"},{"location":"laser-range-scanner/#mounting","title":"Mounting","text":"<p>When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the <code>origin</code> alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description.</p> Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar)."},{"location":"legal-notice/","title":"Legal notice","text":"Legal Notice"},{"location":"legal-notice/#m46","title":"Provider","text":"<p>Franz Pucher</p> <p>Carinagasse 8</p> <p>6800 Feldkirch</p> <p>Austria</p>"},{"location":"legal-notice/#m56","title":"Contact Options","text":"<p>E-Mail Address: ros@fjp.at</p> <p>Phone: +4369917251989</p>"},{"location":"legal-notice/#m172","title":"Social Media Accounts and other Online Profiles","text":""},{"location":"legal-notice/#m65","title":"Liability and Intellectual Property Rights Information","text":"<p>Liability Disclaimer: While the content of this website has been put together with great care and reflects our current knowledge, it is provided for information purposes without being legally binding, unless the disclosure of this information is required by law (e.g. the legal information), the privacy policy, terms and conditions or mandatory instructions for consumers) . We reserve the right to modify or delete the content, whether in full or in part, provided this does not affect our existing contractual obligations. All website content is subject to change and non-binding. </p> <p>Link Disclaimer: We do not accept any responsibility for or endorse the content of external websites we link to, whether directly or indirectly. The providers of the linked websites are solely responsible for all content presented on their websites and in particular, any damage resulting from the use the information offered on their websites.</p> <p>Copyrights and Trademarks: All contents presented on this website, such as texts, photographs, graphics, brands and trademarks are protected by the respective intellectual property rights (copyrights, trademark rights). The use, reproduction, etc. are subject to our rights or the rights of the respective authors or rights owners.</p> <p>Information on legal infringements: Please notify us if you notice any rights violations on our website. Once notified, we will promptly remove any illegal content or links.</p> <p></p>"},{"location":"lm393_speed_sensor/","title":"Lm393 speed sensor","text":""},{"location":"lm393_speed_sensor/#lm393-speed-sensor-odometry","title":"LM393 Speed Sensor - Odometry","text":"<p>Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. </p> <p>To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor.  First, we will create a ROS package with <code>catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]]</code>:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs\nCreating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\"...\nCreated file lm393_speed_sensor/CMakeLists.txt\nCreated file lm393_speed_sensor/package.xml\nCreated folder lm393_speed_sensor/include/lm393_speed_sensor\nCreated folder lm393_speed_sensor/src\nSuccessfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor.\n</code></pre> <p>The package depends on the two ROS client libraries <code>rospy</code> and <code>roscpp</code>. The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead.  To signalise the current pose of the robot in the odometry frame, the <code>nav_msgs/Range</code> message is used.</p>"},{"location":"lm393_speed_sensor/#connection","title":"Connection","text":"<p>To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi.</p>"},{"location":"lm393_speed_sensor/#lm393-speed-sensor-library","title":"LM393 Speed Sensor Library","text":"<p>To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor (API) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. </p> <p>The sensor API is implemented in the <code>lm393_speed_sensor.py</code> python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\), measured in RPM (revolutions per minute) changes.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py\nTODO\n</code></pre> <p>The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels.</p> \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\]"},{"location":"lm393_speed_sensor/#ros-node-for-lm393-speed-sensor","title":"ROS Node for LM393 Speed Sensor","text":"<p>ROS provides the Odometry Message in the  nav_msgs header.  This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor.</p> <p>To design this node we will send out measurements periodically over a topic of type <code>nav_msgs/Odometry</code>. The code for this node is in <code>speed_sensor.py</code>.</p> <p>After writing the node we need to build the packages in the workspace with <code>catkin build</code>.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ catkin build\nTODO\n</code></pre> <p>As the final note of the build output suggests, we have to <code>source</code> the <code>setup.bash</code> files in the <code>devel</code> space.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash\n</code></pre> <p>To make the <code>speed_sensor</code> node executable we have to modify the <code>speed_sensor.py</code> file:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py\n</code></pre> <p>Then we can test the node using <code>rosrun</code>:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ sudo su\n[sudo] password for fjp:\nroot@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash \nroot@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py \n</code></pre> <p>This lets the node publish range messages which we can capture in another terminal window using <code>rostopic</code>. First we use <code>rostopic list</code> to find the name of the topic we are interested in:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list\nTODO\n</code></pre> <p>We named our topic <code>/odom</code> which we can use with the <code>rostopic echo</code> command to see the published messages:</p>"},{"location":"oak/","title":"OpenCV AI Kit","text":"<p>The OpenCV AI Kit (OAK) is an open source hardware and software project for spatial AI and is a result of a very successful Kickstarter Campaign. The hardware can basically be separated into two devices:</p> <ul> <li> <p>OAK\u2014D is a spatial AI powerhouse, capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center.   The OAK\u2014D hardware comes with a 1 meter USB-C cable and a 5V power supply.</p> </li> <li> <p>OAK\u20141 is the tiny-but-mighty 4K camera capable of running the same advanced neural networks as OAK\u2014D, but in an even more minuscule form factor for projects where space and power are at a premium.   Each OAK\u20141 kit includes the OAK\u20141 module with aluminum enclosure, 1 meter USB 3 Type-A to Type-C cable, and getting started guide.</p> </li> </ul> <p>Note</p> <p>There are also options for onboard Wifi and Power over Ethernet (POE).</p> <p>!!! quote What is spacial AI and 3D Object Localization?     First, it is necessary to define what 'Object Detection' is:     It is the technical term for finding the bounding box of an object of interest, in pixel space (i.e. pixel coordinates), in an image.</p> <pre><code>3D Object Localization (or 3D Object Detection), is all about finding such objects in physical space, instead of pixel space. \nThis is useful when trying to real-time measure or interact with the physical world.\n</code></pre> <p>This part of the documentation explains the following</p> <ul> <li>Installing software for OAK on your host (Linux, macOS, Windows, Raspberry Pi)</li> <li>Interacting with OAK and how to run the demo programs</li> <li>Deploy a custom model with OpenVINO Toolkit</li> <li>Training your own Model with SuperAnnotate</li> <li>Train your own Model on a GPU, either locally or in the Cloud, using e.g. Tensorflow Keras.</li> <li>Project using OAK</li> </ul>"},{"location":"oak/#setup-of-oak-1-and-oak-d","title":"Setup of OAK-1 and OAK-D","text":"<p>The documentation of OAK can be found at docs.luxonis.com. To install the software for both devices on your different host platforms please follow the instructions in the Python API.</p> <ul> <li>TODO install command for  different os</li> <li>Test installation instructions</li> </ul>"},{"location":"oak/#interaction-with-oak","title":"Interaction with OAK","text":"<ul> <li>Demo Program Overview</li> <li>Examples</li> <li>Object Detection with Yolo-v3</li> <li>...</li> </ul>"},{"location":"oak/#python-api-depthai","title":"Python API: DepthAI","text":""},{"location":"oak/#deplyinig-neural-network-models-to-oak","title":"Deplyinig Neural Network Models to OAK","text":""},{"location":"oak/#superannotate","title":"SuperAnnotate","text":""},{"location":"oak/#training-custom-models","title":"Training Custom Models","text":""},{"location":"power-supply/","title":"Power Supply","text":""},{"location":"power-supply/#power-supply","title":"Power Supply","text":"<p>DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the  DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds.</p> <p>In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the  List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable  Lithium-ion battery provides 3.7 V with a capacity of  approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the  XTAR VC4. DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh.</p> <p>Note that four standard AA bateries with 1.5 V each is perfectly fine.  Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible.</p>"},{"location":"privacy-policy/","title":"Privacy policy","text":"Privacy Policy Preamble <p>With the following privacy policy we would like to inform you which types of your personal data (hereinafter also abbreviated as \"data\") we process for which purposes and in which scope. The privacy statement applies to all processing of personal data carried out by us, both in the context of providing our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as \"online services\").</p> <p>The terms used are not gender-specific.</p> <p>Last Update: 5. July 2021</p> Table of contents <ul><li>Preamble</li><li>Controller</li><li>Overview of processing operations</li><li>Legal Bases for the Processing</li><li>Security Precautions</li><li>Transmission of Personal Data</li><li>Data Processing in Third Countries</li><li>Erasure of data</li><li>Use of Cookies</li><li>Business services</li><li>Payment Procedure</li><li>Provision of online services and web hosting</li><li>Newsletter and Electronic Communications</li><li>Web Analysis, Monitoring and Optimization</li><li>Online Marketing</li><li>Affiliate-Programms und Affiliate-Links</li><li>Profiles in Social Networks (Social Media)</li><li>Plugins and embedded functions and content</li><li>Changes and Updates to the Privacy Policy</li><li>Rights of Data Subjects</li><li>Terminology and Definitions</li></ul>"},{"location":"privacy-policy/#m3","title":"Controller","text":"<p>Franz PucherCarinagasse 86800, FeldkirchAustria</p> <p>E-mail address: ros@fjp.at.</p> <p>Legal Notice: impressum url.</p>"},{"location":"privacy-policy/#mOverview","title":"Overview of processing operations","text":"<p>The following table summarises the types of data processed, the purposes for which they are processed and the concerned data subjects.</p> Categories of Processed Data <ul><li>Inventory data (e.g. names, addresses).</li><li>Content data (e.g. text input, photographs, videos).</li><li>Contact data (e.g. e-mail, telephone numbers).</li><li>Meta/communication data (e.g. device information, IP addresses).</li><li>Usage data (e.g. websites visited, interest in content, access times).</li><li>Contract data (e.g. contract object, duration, customer category).</li><li>Payment Data (e.g. bank details, invoices, payment history).</li></ul> Categories of Data Subjects <ul><li>Business and contractual partners.</li><li>Prospective customers.</li><li>Communication partner (Recipients of e-mails, letters, etc.).</li><li>Customers.</li><li>Users (e.g. website visitors, users of online services).</li><li>Students/ Participants.</li></ul> Purposes of Processing <ul><li>Affiliate Tracking.</li><li>Provision of our online services and usability.</li><li>Conversion tracking (Measurement of the effectiveness of marketing activities).</li><li>Office and organisational procedures.</li><li>Direct marketing  (e.g. by e-mail or postal).</li><li>Feedback (e.g. collecting feedback via online form).</li><li>Marketing.</li><li>Contact requests and communication.</li><li>Profiles with user-related information (Creating user profiles).</li><li>Web Analytics (e.g. access statistics, recognition of returning visitors).</li><li>Security measures.</li><li>Provision of contractual services and customer support.</li><li>Managing and responding to inquiries.</li></ul>"},{"location":"privacy-policy/#m13","title":"Legal Bases for the Processing","text":"<p>In the following, you will find an overview of the legal basis of the GDPR on which we base the processing of personal data. Please note that in addition to the provisions of the GDPR, national data protection provisions of your or our country of residence or domicile may apply. If, in addition, more specific legal bases are applicable in individual cases, we will inform you of these in the data protection declaration.</p> <ul><li>Consent (Article 6 (1) (a) GDPR) - The data subject has given consent to the processing of his or her personal data for one or more specific purposes.</li><li>Performance of a contract and prior requests (Article 6 (1) (b) GDPR) - Performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract.</li><li>Compliance with a legal obligation (Article 6 (1) (c) GDPR) - Processing is necessary for compliance with a legal obligation to which the controller is subject.</li><li>Legitimate Interests (Article 6 (1) (f) GDPR) - Processing is necessary for the purposes of the legitimate interests pursued by the controller or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data.</li></ul> <p>National data protection regulations in Austria: In addition to the data protection regulations of the General Data Protection Regulation, national regulations apply to data protection in Austria. This includes in particular the Federal Act on the Protection of Individuals with regard to the Processing of Personal Data (Data Protection Act - DSG). In particular, the Data Protection Act contains special provisions on the right of access, rectification or cancellation, processing of special categories of personal data, processing for other purposes and transmission and automated decision making in individual cases.</p>"},{"location":"privacy-policy/#m27","title":"Security Precautions","text":"<p>We take appropriate technical and organisational measures in accordance with the legal requirements, taking into account the  state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, in order to ensure a level of security appropriate to the risk.</p> <p>The measures include, in particular, safeguarding the confidentiality, integrity and availability of data by controlling physical and electronic access to the data as well as access to, input, transmission, securing and separation of the data. In addition, we have established procedures to ensure that data subjects' rights are respected, that data is erased, and that we are prepared to respond to data threats rapidly. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software and service providers, in accordance with the principle of privacy by design and privacy by default.</p> <p>Masking of the IP address: If IP addresses are processed by us or by the service providers and technologies used and the processing of a complete IP address is not necessary, the IP address is shortened (also referred to as \"IP masking\"). In this process, the last two digits or the last part of the IP address after a full stop are removed or replaced by wildcards. The masking of the IP address is intended to prevent the identification of a person by means of their IP address or to make such identification significantly more difficult.</p> <p>SSL encryption (https): In order to protect your data transmitted via our online services in the best possible way, we use SSL encryption. You can recognize such encrypted connections by the prefix https:// in the address bar of your browser. </p>"},{"location":"privacy-policy/#m25","title":"Transmission of Personal Data","text":"<p>In the context of our processing of personal data, it may happen that the data is transferred to other places, companies or persons or that it is disclosed to them. Recipients of this data may include, for example, service providers commissioned with IT tasks or providers of services and content that are embedded in a website. In such a case, the legal requirements will be respected and in particular corresponding contracts or agreements, which serve the protection of your data, will be concluded with the recipients of your data.</p>"},{"location":"privacy-policy/#m24","title":"Data Processing in Third Countries","text":"<p>If we process data in a third country (i.e. outside the European Union (EU), the European Economic Area (EEA)) or the processing takes place in the context of the use of third party services or disclosure or transfer of data to other persons, bodies or companies, this will only take place in accordance with the legal requirements. </p> <p>Subject to express consent or transfer required by contract or law, we process or have processed the data only in third countries with a recognised level of data protection, on the basis of special guarantees, such as a contractual obligation through so-called standard protection clauses of the EU Commission or if certifications or binding internal data protection regulations justify the processing (Article 44 to 49 GDPR, information page of the EU Commission: https://ec.europa.eu/info/law/law-topic/data-protection/international-dimension-data-protection_en).</p>"},{"location":"privacy-policy/#m12","title":"Erasure of data","text":"<p>The data processed by us will be erased in accordance with the statutory provisions as soon as their processing is revoked or other permissions no longer apply (e.g. if the purpose of processing this data no longer applies or they are not required for the purpose).</p> <p>If the data is not deleted because they are required for other and legally permissible purposes, their processing is limited to these purposes. This means that the data will be restricted and not processed for other purposes. This applies, for example, to data that must be stored for commercial or tax reasons or for which storage is necessary to assert, exercise or defend legal claims or to protect the rights of another natural or legal person.</p> <p>In the context of our information on data processing, we may provide users with further information on the deletion and retention of data that is specific to the respective processing operation.</p>"},{"location":"privacy-policy/#m134","title":"Use of Cookies","text":"<p>Cookies are text files that contain data from visited websites or domains and are stored by a browser on the user's computer. A cookie is primarily used to store information about a user during or after his visit within an online service. The information stored can include, for example, the language settings on a website, the login status, a shopping basket or the location where a video was viewed. The term \"cookies\" also includes other technologies that fulfil the same functions as cookies (e.g. if user information is stored using pseudonymous online identifiers, also referred to as \"user IDs\").</p> <p>The following types and functions of cookies are distinguished:</p> <ul><li>Temporary cookies (also: session cookies):\u00a0Temporary cookies are deleted at the latest after a user has left an online service and closed his browser.</li><li>Permanent cookies:\u00a0Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user visits a website again. The interests of users who are used for range measurement or marketing purposes can also be stored in such a cookie.</li><li>First-Party-Cookies: First-Party-Cookies are set by ourselves.</li><li>Third party cookies: Third party cookies are mainly used by advertisers (so-called third parties) to process user information.</li><li>Necessary (also: essential) cookies: Cookies can be necessary for the operation of a website (e.g. to save logins or other user inputs or for security reasons).</li><li>Statistics, marketing and personalisation cookies: Cookies are also generally used to measure a website's reach and when a user's interests or behaviour (e.g. viewing certain content, using functions, etc.) are stored on individual websites in a user profile. Such profiles are used, for example, to display content to users that corresponds to their potential interests. This procedure is also referred to as \"tracking\", i.e. tracking the potential interests of users. If we use cookies or \"tracking\" technologies, we will inform you separately in our privacy policy or in the context of obtaining consent.</li></ul> <p>Information on legal basis: The legal basis on which we process your personal data with the help of cookies depends on whether we ask you for your consent. If this applies and you consent to the use of cookies, the legal basis for processing your data is your declared consent. Otherwise, the data processed with the help of cookies will be processed on the basis of our legitimate interests (e.g. in a business operation of our online service and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.</p> <p>Retention period: Unless we provide you with explicit information on the retention period of permanent cookies (e.g. within the scope of a so-called cookie opt-in), please assume that the retention period can be as long as two years.</p> <p>General information on Withdrawal of consent and objection (Opt-Out): Respective of whether processing is based on consent or legal permission, you have the option at any time to object to the processing of your data using cookie technologies or to revoke consent (collectively referred to as \"opt-out\"). You can initially explain your objection using the settings of your browser, e.g. by deactivating the use of cookies (which may also restrict the functionality of our online services). An objection to the use of cookies for online marketing purposes can be raised for a large number of services, especially in the case of tracking, via the websites https://www.aboutads.info/choices/ and https://www.youronlinechoices.com. In addition, you can receive further information on objections in the context of the information on the used service providers and cookies.</p> <p>Processing Cookie Data on the Basis of Consent: We use a cookie management solution in which users' consent to the use of cookies, or the procedures and providers mentioned in the cookie management solution, can be obtained, managed and revoked by the users. The declaration of consent is stored so that it does not have to be retrieved again and the consent can be proven in accordance with the legal obligation. Storage can take place server-sided and/or in a cookie (so-called opt-out cookie or with the aid of comparable technologies) in order to be able to assign the consent to a user or and/or his/her device.Subject to individual details of the providers of cookie management services, the following information applies: The duration of the storage of the consent can be up to two years. In this case, a pseudonymous user identifier is formed and stored with the date/time of consent, information on the scope of the consent (e.g. which categories of cookies and/or service providers) as well as the browser, system and used end device.</p> <ul><li>Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul>"},{"location":"privacy-policy/#m317","title":"Business services","text":"<p>We process data of our contractual and business partners, e.g. customers and interested parties (collectively referred to as \"contractual partners\") within the context of contractual and comparable legal relationships as well as associated actions and communication with the contractual partners or pre-contractually, e.g. to answer inquiries.</p> <p>We process this data in order to fulfil our contractual obligations, safeguard our rights and for the purposes of the administrative tasks associated with this data and the business-related organisation. We will only pass on the data of the contractual partners within the scope of the applicable law to third parties insofar as this is necessary for the aforementioned purposes or for the fulfilment of legal obligations or with the consent of data subjects concerned (e.g. telecommunications, transport and other auxiliary services as well as subcontractors, banks, tax and legal advisors, payment service providers or tax authorities). The contractual partners will be informed about further processing, e.g. for marketing purposes, as part of this privacy policy.</p> <p>Which data are necessary for the aforementioned purposes, we inform the contracting partners before or in the context of the data collection, e.g. in online forms by special marking (e.g. colors), and/or symbols (e.g. asterisks or the like), or personally.</p> <p>We delete the data after expiry of statutory warranty and comparable obligations, i.e. in principle after expiry of 4 years, unless the data is stored in a customer account or must be kept for legal reasons of archiving (e.g., as a rule 10 years for tax purposes). In the case of data disclosed to us by the contractual partner within the context of an assignment, we delete the data in accordance with the specifications of the assignment, in general after the end of the assignment.</p> <p>If we use third-party providers or platforms to provide our services, the terms and conditions and privacy policies of the respective third-party providers or platforms shall apply in the relationship between the users and the providers.</p> <p>Online Shop and E-Commerce: We process the data of our customers in order to enable them to select, purchase or order the selected products, goods and related services, as well as their payment and delivery, or performance of other services. If necessary for the execution of an order, we use service providers, in particular postal, freight and shipping companies, in order to carry out the delivery or execution to our customers. For the processing of payment transactions we use the services of banks and payment service providers. The required details are identified as such in the course of the ordering or comparable purchasing process and include the details required for delivery, or other way of making the product aviable and invoicing as well as contact information in order to be able to hold any consultation.</p> <p>Education and Training Services: We process the data of the participants of our education and training programmes (uniformly referred to as \" students\") in order to provide them with our educational and training services. The data processed, the type, scope and purpose of the processing and the necessity of its processing are determined by the underlying contractual and educational relationship. The processing also includes the performance evaluation and evaluation of our services and the teachers and instructors.</p> <p>As part of our activities, we may also process special categories of data, in particular information on the health of persons undergoing training or further training and data revealing ethnic origin, political opinions, religious or philosophical convictions. To this end, we obtain, if necessary, the express consent of the students to be trained and further educated and process the special categories of data otherwise only if it is necessary for the provision of training services, for purposes of health care, social protection or protection of vital interests of the students to be trained and further educated.</p> <p>Insofar as it is necessary for the fulfilment of our contract, for the protection of vital interests or by law, or with the consent of the trainees, we disclose or transfer the data of the students to third parties or agents, e.g. public authorities or in the field of IT, office or comparable services, in compliance with the requirements of professional law.</p> <p>Consulting: We process the data of our clients, clients as well as interested parties and other clients or contractual partners (uniformly referred to as \"clients\") in order to provide them with our consulting services. The data processed, the type, scope and purpose of the processing and the necessity of its processing are determined by the underlying contractual and client relationship.</p> <p>Insofar as it is necessary for the fulfilment of our contract, for the protection of vital interests or by law, or with the consent of the client, we disclose or transfer the client's data to third parties or agents, such as authorities, courts, subcontractors or in the field of IT, office or comparable services, taking into account the professional requirements.</p> <ul><li>Processed data types: Inventory data (e.g. names, addresses), Payment Data (e.g. bank details, invoices, payment history), Contact data (e.g. e-mail, telephone numbers), Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Prospective customers, Business and contractual partners, Customers, Students/ Participants.</li><li>Purposes of Processing: Provision of contractual services and customer support, Contact requests and communication, Office and organisational procedures, Managing and responding to inquiries, Security measures.</li><li>Legal Basis: Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Compliance with a legal obligation (Article 6 (1) (c) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul>"},{"location":"privacy-policy/#m326","title":"Payment Procedure","text":"<p>Within the framework of contractual and other legal relationships, due to legal obligations or otherwise on the basis of our legitimate interests, we offer data subjects efficient and secure payment options and use other service providers for this purpose in addition to banks and credit institutions (collectively referred to as \"payment service providers\").</p> <p>The data processed by the payment service providers includes inventory data, such as the name and address, bank data, such as account numbers or credit card numbers, passwords, TANs and checksums, as well as the contract, total and recipient-related information. The information is required to carry out the transactions. However, the data entered is only processed by the payment service providers and stored with them. I.e. we do not receive any account or credit card related information, but only information with confirmation or negative information of the payment. Under certain circumstances, the data may be transmitted by the payment service providers to credit agencies. The purpose of this transmission is to check identity and creditworthiness. Please refer to the terms and conditions and data protection information of the payment service providers.</p> <p>The terms and conditions and data protection information of the respective payment service providers apply to the payment transactions and can be accessed within the respective websites or transaction applications. We also refer to these for further information and the assertion of revocation, information and other data subject rights.</p> <ul><li>Processed data types: Inventory data (e.g. names, addresses), Payment Data (e.g. bank details, invoices, payment history), Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Customers, Prospective customers.</li><li>Purposes of Processing: Provision of contractual services and customer support.</li><li>Legal Basis: Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>Amazon Payments: Payment service provider; Service provider: Amazon Payments Europe S.C.A. 38 avenue J.F. Kennedy, L-1855 Luxembourg; Website: https://pay.amazon.com; Privacy Policy: https://pay.amazon.com/us/help/201212490.</li><li>PayPal: Payment service provider (e.g. PayPal, PayPal Plus, Braintree, Braintree); Service provider: PayPal (Europe) S.\u00e0 r.l. et Cie, S.C.A., 22-24 Boulevard Royal, L-2449 Luxembourg; Website: https://www.paypal.com; Privacy Policy: https://www.paypal.com/de/webapps/mpp/ua/privacy-full.</li><li>Stripe: Payment service provider; Service provider: Stripe, Inc., 510 Townsend Street, San Francisco, CA 94103, USA; Website: https://stripe.com/de; Privacy Policy: https://stripe.com/en-de/privacy.</li></ul>"},{"location":"privacy-policy/#m225","title":"Provision of online services and web hosting","text":"<p>In order to provide our online services securely and efficiently, we use the services of one or more web hosting providers from whose servers (or servers they manage) the online services can be accessed. For these purposes, we may use infrastructure and platform services, computing capacity, storage space and database services, as well as security and technical maintenance services.</p> <p>The data processed within the framework of the provision of the hosting services may include all information relating to the users of our online services that is collected in the course of use and communication. This regularly includes the IP address, which is necessary to be able to deliver the contents of online services to browsers, and all entries made within our online services or from websites.</p> <p>Collection of Access Data and Log Files: We, ourselves or our web hosting provider, collect data on the basis of each access to the server (so-called server log files). Server log files may include the address and name of the web pages and files accessed, the date and time of access, data volumes transferred, notification of successful access, browser type and version, the user's operating system, referrer URL (the previously visited page) and, as a general rule, IP addresses and the requesting provider.</p> <p>The server log files can be used for security purposes, e.g. to avoid overloading the servers (especially in the case of abusive attacks, so-called DDoS attacks) and to ensure the stability and optimal load balancing of the servers .</p> <ul><li>Processed data types: Content data (e.g. text input, photographs, videos), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Provision of our online services and usability.</li><li>Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>JSDelivr: Content Delivery Network (CDN) that helps deliver media and files quickly and efficiently, especially under heavy load. Service provider: ProspectOne, Kr\u00f3lewska 65A/1, 30-081, Krak\u00f3w, Poland; Website: https://www.jsdelivr.com; Privacy Policy: https://www.jsdelivr.com/terms/privacy-policy-jsdelivr-net.</li></ul>"},{"location":"privacy-policy/#m17","title":"Newsletter and Electronic Communications","text":"<p>We send newsletters, e-mails and other electronic communications (hereinafter referred to as \"newsletters\") only with the consent of the recipient or a legal permission. Insofar as the contents of the newsletter are specifically described within the framework of registration, they are decisive for the consent of the user. Otherwise, our newsletters contain information about our services and us.</p> <p>In order to subscribe to our newsletters, it is generally sufficient to enter your e-mail address. We may, however, ask you to provide a name for the purpose of contacting you personally in the newsletter or to provide further information if this is required for the purposes of the newsletter.</p> <p>Double opt-in procedure: The registration to our newsletter takes place in general in a so-called Double-Opt-In procedure. This means that you will receive an e-mail after registration asking you to confirm your registration. This confirmation is necessary so that no one can register with external e-mail addresses. </p> <p>The registrations for the newsletter are logged in order to be able to prove the registration process according to the legal requirements. This includes storing the login and confirmation times as well as the IP address. Likewise the changes of your data stored with the dispatch service provider are logged.</p> <p>Deletion and restriction of processing: We may store the unsubscribed email addresses for up to three years based on our legitimate interests before deleting them to provide evidence of prior consent. The processing of these data is limited to the purpose of a possible defense against claims. An individual deletion request is possible at any time, provided that the former existence of a consent is confirmed at the same time. In the case of an obligation to permanently observe an objection, we reserve the right to store the e-mail address solely for this purpose in a blocklist.</p> <p>Information on legal bases: The sending of the newsletter is based on the consent of the recipients or, if consent is not required, on the basis of our legitimate interests in direct marketing. Insofar as we engage a service provider for sending e-mails, this is done on the basis of our legitimate interests. The registration procedure is recorded on the basis of our legitimate interests for the purpose of demonstrating that it has been conducted in accordance with the law.</p> <p>Contents: Information about us, our services, promotions and offers.</p> <p>Measurement of opening rates and click rates: The newsletters contain a so-called \"web-beacon\", i.e. a pixel-sized file, which is retrieved from our server when the newsletter is opened or, if we use a mailing service provider, from its server. Within the scope of this retrieval, technical information such as information about the browser and your system, as well as your IP address and time of retrieval are first collected. </p> <p>This information is used for the technical improvement of our newsletter on the basis of technical data or target groups and their reading behaviour on the basis of their retrieval points (which can be determined with the help of the IP address) or access times. This analysis also includes determining whether newsletters are opened, when they are opened and which links are clicked. This information is assigned to the individual newsletter recipients and stored in their profiles until the profiles are deleted. The evaluations serve us much more to recognize the reading habits of our users and to adapt our content to them or to send different content according to the interests of our users.</p> <p>The measurement of opening rates and click rates as well as the storage of the measurement results in the profiles of the users and their further processing are based on the consent of the users. </p> <p>A separate objection to the performance measurement is unfortunately not possible, in this case the entire newsletter subscription must be cancelled or objected to. In this case, the stored profile information will be deleted.</p> <ul><li>Processed data types: Inventory data (e.g. names, addresses), Contact data (e.g. e-mail, telephone numbers), Meta/communication data (e.g. device information, IP addresses), Usage data (e.g. websites visited, interest in content, access times).</li><li>Data subjects: Communication partner (Recipients of e-mails, letters, etc.), Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Direct marketing  (e.g. by e-mail or postal), Web Analytics (e.g. access statistics, recognition of returning visitors), Conversion tracking (Measurement of the effectiveness of marketing activities), Profiles with user-related information (Creating user profiles).</li><li>Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li><li>Opt-Out: You can cancel the receipt of our newsletter at any time, i.e. revoke your consent or object to further receipt. You will find a link to cancel the newsletter either at the end of each newsletter or you can otherwise use one of the contact options listed above, preferably e-mail. </li></ul> <p>Services and service providers being used:</p> <ul><li>Google Analytics: Measuring the success of email campaigns and building user profiles with a storage period of up to two years; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/; Privacy Policy: https://policies.google.com/privacy; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en, Settings for the Display of Advertisements: https://adssettings.google.com/authenticated.</li><li>HubSpot: Email marketing platform; Service provider: HubSpot, Inc., 25 First St., 2nd floor, Cambridge, Massachusetts 02141, USA; Website: https://www.hubspot.com; Privacy Policy: https://legal.hubspot.com/privacy-policy.</li></ul>"},{"location":"privacy-policy/#m263","title":"Web Analysis, Monitoring and Optimization","text":"<p>Web analysis is used to evaluate the visitor traffic on our website and may include the behaviour, interests or demographic information of users, such as age or gender, as pseudonymous values. With the help of web analysis we can e.g. recognize, at which time our online services or their functions or contents are most frequently used or requested for repeatedly, as well as which areas require optimization.</p> <p>In addition to web analysis, we can also use test procedures, e.g. to test and optimize different versions of our online services or their components.</p> <p>For these purposes, so-called user profiles can be created and stored in a file (so-called \"cookie\") or similar procedures in which the relevant user information for the aforementioned analyses is stored. This information may include, for example, content viewed, web pages visited and elements and technical data used there, such as the browser used, computer system used and information on times of use. If users have consented to the collection of their location data, these may also be processed, depending on the provider.</p> <p>The IP addresses of the users are also stored. However, we use any existing IP masking procedure (i.e. pseudonymisation by shortening the IP address) to protect the user. In general, within the framework of web analysis, A/B testing and optimisation, no user data (such as e-mail addresses or names) is stored, but pseudonyms. This means that we, as well as the providers of the software used, do not know the actual identity of the users, but only the information stored in their profiles for the purposes of the respective processes.</p> <p>Information on legal basis: If we ask the users for their consent to the use of third party providers, the legal basis of the processing is consent. Furthermore, the processing can be a component of our (pre)contractual services, provided that the use of the third party was agreed within this context. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in efficient, economic and recipient friendly services). In this context, we would also like to refer you to the information on the use of cookies in this privacy policy.</p> <ul><li>Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Web Analytics (e.g. access statistics, recognition of returning visitors), Profiles with user-related information (Creating user profiles).</li><li>Security measures: IP Masking (Pseudonymization of the IP address).</li><li>Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>Google Analytics: Web analytics; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/; Privacy Policy: https://policies.google.com/privacy.</li></ul>"},{"location":"privacy-policy/#m264","title":"Online Marketing","text":"<p>We process personal data for the purposes of online marketing, which may include in particular the marketing of advertising space or the display of advertising and other content (collectively referred to as \"Content\") based on the potential interests of users and the measurement of their effectiveness. </p> <p>For these purposes, so-called user profiles are created and stored in a file (so-called \"cookie\") or similar procedure in which the relevant user information for the display of the aforementioned content is stored. This information may include, for example, content viewed, websites visited, online networks used, communication partners and technical information such as the browser used, computer system used and information on usage times. If users have consented to the collection of their sideline data, these can also be processed.</p> <p>The IP addresses of the users are also stored. However, we use provided IP masking procedures (i.e. pseudonymisation by shortening the IP address) to ensure the protection of the user's by using a pseudonym. In general, within the framework of the online marketing process, no clear user data (such as e-mail addresses or names) is secured, but pseudonyms. This means that we, as well as the providers of online marketing procedures, do not know the actual identity of the users, but only the information stored in their profiles.</p> <p>The information in the profiles is usually stored in the cookies or similar memorizing procedures. These cookies can later, generally also on other websites that use the same online marketing technology, be read and analyzed for purposes of content display, as well as supplemented with other data and stored on the server of the online marketing technology provider.</p> <p>Exceptionally, clear data can be assigned to the profiles. This is the case, for example, if the users are members of a social network whose online marketing technology we use and the network links the profiles of the users in the aforementioned data. Please note that users may enter into additional agreements with the social network providers or other service providers, e.g. by consenting as part of a registration process.</p> <p>As a matter of principle, we only gain access to summarised information about the performance of our advertisements. However, within the framework of so-called conversion measurement, we can check which of our online marketing processes have led to a so-called conversion, i.e. to the conclusion of a contract with us. The conversion measurement is used alone for the performance analysis of our marketing activities.</p> <p>Unless otherwise stated, we kindly ask you to consider that cookies used will be stored for a period of two years.</p> <p>Information on legal basis: If we ask users for their consent (e.g. in the context of a so-called \"cookie banner consent\"), the legal basis for processing data for online marketing purposes is this consent. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in the analysis, optimisation and economic operation of our online services. In this context, we would also like to refer you to the information on the use of cookies in this privacy policy.</p> <ul><li>Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Marketing, Profiles with user-related information (Creating user profiles).</li><li>Security measures: IP Masking (Pseudonymization of the IP address).</li><li>Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li><li>Opt-Out: We refer to the privacy policies of the respective service providers and the possibilities for objection (so-called \"opt-out\"). If no explicit opt-out option has been specified, it is possible to deactivate cookies in the settings of your browser. However, this may restrict the functions of our online offer. We therefore recommend the following additional opt-out options, which are offered collectively for each area:  a) Europe: https://www.youronlinechoices.eu.  b) Canada: https://www.youradchoices.ca/choices.  c) USA: https://www.aboutads.info/choices.  d) Cross-regional: https://optout.aboutads.info.</li></ul> <p>Services and service providers being used:</p> <ul><li>Google Analytics: Online marketing and web analytics; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/; Privacy Policy: https://policies.google.com/privacy; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en, Settings for the Display of Advertisements: https://adssettings.google.com/authenticated.</li><li>Google Adsense with personalized ads: We use the Google Adsense service with personalized ads, which helps us to display ads within our online services and we receive a remuneration for their display or other use. ; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com; Privacy Policy: https://policies.google.com/privacy.</li><li>Amazon: Marketing of advertising media and advertising spaces; Service provider: Amazon Europe Core S.\u00e0.r.l., Amazon EU S.\u00e0.r.l., Amazon Services Europe S.\u00e0.r.l. and Amazon Media EU S.\u00e0.r.l., all four located at 38, avenue John F. Kennedy, L-1855 Luxembourg, and Amazon Instant Video Germany GmbH, Domagkstr. 28, 80807 Munich (together \"Amazon Europe\"), parent company: Amazon.com, Inc., 2021 Seventh Ave, Seattle, Washington 98121, USA; Website: https://www.amazon.com; Privacy Policy: https://www.amazon.com/gp/help/customer/display.html?nodeId=201909010.</li><li>HubSpot: Marketing software for lead generation, marketing automation and analysis of marketing activities; Service provider: HubSpot, Inc., 25 First St., 2nd floor, Cambridge, Massachusetts 02141, USA; Website: https://www.hubspot.de; Privacy Policy: https://legal.hubspot.com/privacy-policy.</li></ul>"},{"location":"privacy-policy/#m135","title":"Affiliate-Programms und Affiliate-Links","text":"<p>Within our online services, we include so-called affiliate links or other references (which for example may include search forms, widgets or discount codes) to the offers and services of third parties (collectively referred to as \"affiliate links\"). When users follow affiliate links or subsequently take advantage of offers, we may receive commission or other benefits (collectively referred to as \"commission\") from these third parties.</p> <p>In order to be able to track whether the users have followed the offers of an affiliate link used by us, it is necessary for the respective third party to know that the users have followed an affiliate link used within our online services. The assignment of affiliate links to the respective business transactions or other actions (e.g., purchases) serves the sole purpose of commission settlement and is removed as soon as it is no longer required for this purpose.</p> <p>For the purposes of the aforementioned affiliate link assignment, the affiliate links may be supplemented by certain values that may be a component of the link or otherwise stored, for example, in a cookie. The values may include in particular the source website (referrer), time, an online identifier of the operator of the website on which the affiliate link was located, an online identifier of the respective offer, the type of link used, the type of offer and an online identifier of the user.</p> <p>Information on legal basis: If we ask the users for their consent to the use of third party providers, the legal basis of the processing is consent. Furthermore, the processing can be a component of our (pre)contractual services, provided that the use of the third party was agreed within this context. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in efficient, economic and recipient friendly services). In this context, we would also like to refer you to the information on the use of cookies in this privacy policy.</p> <ul><li>Processed data types: Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Affiliate Tracking.</li><li>Legal Basis: Consent (Article 6 (1) (a) GDPR), Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>Amazon Affiliate Program: Amazon Affiliate Program - Amazon and the Amazon logo are trademarks of Amazon.com, Inc. or one of its affiliates. Service provider: Amazon Europe Core S.\u00e0.r.l., Amazon EU S.\u00e0.r.l., Amazon Services Europe S.\u00e0.r.l. and Amazon Media EU S.\u00e0.r.l., all four located at 38, avenue John F. Kennedy, L-1855 Luxembourg, and Amazon Instant Video Germany GmbH, Domagkstr. 28, 80807 Munich (together \"Amazon Europe\"), parent company: Amazon.com, Inc., 2021 Seventh Ave, Seattle, Washington 98121, USA.; Website: https://www.amazon.com; Privacy Policy: https://www.amazon.com/gp/help/customer/display.html?nodeId=201909010.</li></ul>"},{"location":"privacy-policy/#m136","title":"Profiles in Social Networks (Social Media)","text":"<p>We maintain online presences within social networks and process user data in this context in order to communicate with the users active there or to offer information about us.</p> <p>We would like to point out that user data may be processed outside the European Union. This may entail risks for users, e.g. by making it more difficult to enforce users' rights.</p> <p>In addition, user data is usually processed within social networks for market research and advertising purposes. For example, user profiles can be created on the basis of user behaviour and the associated interests of users. The user profiles can then be used, for example, to place advertisements within and outside the networks which are presumed to correspond to the interests of the users. For these purposes, cookies are usually stored on the user's computer, in which the user's usage behaviour and interests are stored. Furthermore, data can be stored in the user profiles independently of the devices used by the users (especially if the users are members of the respective networs or will become members later on).</p> <p>For a detailed description of the respective processing operations and the opt-out options, please refer to the respective data protection declarations and information provided by the providers of the respective networks.</p> <p>Also in the case of requests for information and the exercise of rights of data subjects, we point out that these can be most effectively pursued with the providers. Only the providers have access to the data of the users and can directly take appropriate measures and provide information. If you still need help, please do not hesitate to contact us.</p> <p>Facebook: We are jointly responsible (so called \"joint controller\") with Facebook Ireland Ltd. for the collection (but not the further processing) of data of visitors to our Facebook page. This data includes information about the types of content users view or interact with, or the actions they take (see \"Things that you and others do and provide\" in the Facebook Data Policy: https://www.facebook.com/policy), and information about the devices used by users (e.g., IP addresses, operating system, browser type, language settings, cookie information; see \"Device Information\" in the Facebook Data Policy: https://www.facebook.com/policy). As explained in the Facebook Data Policy under \"How we use this information?\" Facebook also collects and uses information to provide analytics services, known as \"page insights,\" to site operators to help them understand how people interact with their pages and with content associated with them. We have concluded a special agreement with Facebook (\"Information about Page-Insights\", https://www.facebook.com/legal/terms/page_controller_addendum), which regulates in particular the security measures that Facebook must observe and in which Facebook has agreed to fulfill the rights of the persons concerned (i.e. users can send information access or deletion requests directly to Facebook). The rights of users (in particular to access to information, erasure, objection and complaint to the competent supervisory authority) are not restricted by the agreements with Facebook. Further information can be found in the \"Information about Page Insights\" (https://www.facebook.com/legal/terms/information_about_page_insights_data).</p> <ul><li>Processed data types: Contact data (e.g. e-mail, telephone numbers), Content data (e.g. text input, photographs, videos), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Contact requests and communication, Feedback (e.g. collecting feedback via online form), Marketing.</li><li>Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>Instagram: Social network; Service provider: Instagram Inc., 1601 Willow Road, Menlo Park, CA, 94025, USA, , Mutterunternehmen: Facebook, 1 Hacker Way, Menlo Park, CA 94025, USA; Website: https://www.instagram.com; Privacy Policy: https://instagram.com/about/legal/privacy.</li><li>Facebook: Social network; Service provider: Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Irland, parent company: Facebook, 1 Hacker Way, Menlo Park, CA 94025, USA; Website: https://www.facebook.com; Privacy Policy: https://www.facebook.com/about/privacy; Opt-Out: Settings for advertisements: https://www.facebook.com/adpreferences/ad_settings (login at Facebook is required).</li><li>LinkedIn: Social network; Service provider: LinkedIn Ireland Unlimited Company, Wilton Place, Dublin 2, Ireland; Website: https://www.linkedin.com; Privacy Policy: https://www.linkedin.com/legal/privacy-policy; Opt-Out: https://www.linkedin.com/psettings/guest-controls/retargeting-opt-out.</li><li>Twitter: Social network; Service provider: Twitter International Company, One Cumberland Place, Fenian Street, Dublin 2 D02 AX07, Ireland, parent company: Twitter Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, USA; Privacy Policy: https://twitter.com/de/privacy, (Settings) https://twitter.com/personalization.</li><li>Vimeo: Social network and video platform; Service provider: Vimeo Inc., Attention: Legal Department, 555 West 18th Street New York, New York 10011, USA; Website: https://vimeo.com; Privacy Policy: https://vimeo.com/privacy.</li><li>YouTube: Social network and video platform; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Privacy Policy: https://policies.google.com/privacy; Opt-Out: https://adssettings.google.com/authenticated.</li></ul>"},{"location":"privacy-policy/#m328","title":"Plugins and embedded functions and content","text":"<p>Within our online services, we integrate functional and content elements that are obtained from the servers of their respective providers (hereinafter referred to as \"third-party providers\"). These may, for example, be graphics, videos or city maps (hereinafter uniformly referred to as \"Content\").</p> <p>The integration always presupposes that the third-party providers of this content process the IP address of the user, since they could not send the content to their browser without the IP address. The IP address is therefore required for the presentation of these contents or functions. We strive to use only those contents, whose respective offerers use the IP address only for the distribution of the contents. Third parties may also use so-called pixel tags (invisible graphics, also known as \"web beacons\") for statistical or marketing purposes. The \"pixel tags\" can be used to evaluate information such as visitor traffic on the pages of this website. The pseudonymous information may also be stored in cookies on the user's device and may include technical information about the browser and operating system, referring websites, visit times and other information about the use of our website, as well as may be linked to such information from other sources.</p> <p>Information on legal basis: If we ask users for their consent (e.g. in the context of a so-called \"cookie banner consent\"), the legal basis for processing is this consent. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in the analysis, optimisation and economic operation of our online services. We refer you to the note on the use of cookies in this privacy policy.</p> <ul><li>Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses), Inventory data (e.g. names, addresses), Contact data (e.g. e-mail, telephone numbers), Content data (e.g. text input, photographs, videos).</li><li>Data subjects: Users (e.g. website visitors, users of online services).</li><li>Purposes of Processing: Provision of our online services and usability, Provision of contractual services and customer support.</li><li>Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR), Consent (Article 6 (1) (a) GDPR), Performance of a contract and prior requests (Article 6 (1) (b) GDPR).</li></ul> <p>Services and service providers being used:</p> <ul><li>Font Awesome: Display of fonts and symbols; Service provider:  Fonticons, Inc. ,6 Porter Road Apartment 3R, Cambridge, MA 02140, USA; Website: https://fontawesome.com/; Privacy Policy: https://fontawesome.com/privacy.</li><li>Google Fonts: We integrate the fonts (\"Google Fonts\") of the provider Google, whereby the data of the users are used solely for purposes of the representation of the fonts in the browser of the users. The integration takes place on the basis of our legitimate interests in a technically secure, maintenance-free and efficient use of fonts, their uniform presentation and consideration of possible licensing restrictions for their integration. Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://fonts.google.com/; Privacy Policy: https://policies.google.com/privacy.</li><li>YouTube videos: Video contents; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, , parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://www.youtube.com; Privacy Policy: https://policies.google.com/privacy; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en,  Settings for the Display of Advertisements: https://adssettings.google.com/authenticated.</li><li>YouTube-Videos: Video content; YouTube is integrated via the domain https://www.youtube-nocookie.com in the so-called \" enhanced data protection mode\", whereby no cookies on user activities are collected in order to personalise the video playback. Nevertheless, information on the user's interaction with the video (e.g. remembering the last playback point) may be stored. Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, , parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://www.youtube.com; Privacy Policy: https://policies.google.com/privacy.</li></ul>"},{"location":"privacy-policy/#m15","title":"Changes and Updates to the Privacy Policy","text":"<p>We kindly ask you to inform yourself regularly about the contents of our data protection declaration. We will adjust the privacy policy as changes in our data processing practices make this necessary. We will inform you as soon as the changes require your cooperation (e.g. consent) or other individual notification.</p> <p>If we provide addresses and contact information of companies and organizations in this privacy policy, we ask you to note that addresses may change over time and to verify the information before contacting us.</p>"},{"location":"privacy-policy/#m10","title":"Rights of Data Subjects","text":"<p>As data subject, you are entitled to various rights under the GDPR, which arise in particular from Articles 15 to 21 of the GDPR:</p> <ul><li>Right to Object: You have the right, on grounds arising from your particular situation, to object at any time to the processing of your personal data which is based on letter (e) or (f) of Article 6(1) GDPR , including profiling based on those provisions.<p>Where personal data are processed for direct marketing purposes, you have the right to object at any time to the processing of the personal data concerning you for the purpose of such marketing, which includes profiling to the extent that it is related to such direct marketing.</p></li><li>Right of withdrawal for consents: You have the right to revoke consents at any time.</li><li>Right of access: You have the right to request confirmation as to whether the data in question will be processed and to be informed of this data and to receive further information and a copy of the data in accordance with the provisions of the law.</li><li>Right to rectification: You have the right, in accordance with the law, to request the completion of the data concerning you or the rectification of the incorrect data concerning you.</li><li>Right to Erasure and Right to Restriction of Processing: In accordance with the statutory provisions, you have the right to demand that the relevant data be erased immediately or, alternatively, to demand that the processing of the data be restricted in accordance with the statutory provisions.</li><li>Right to data portability: You have the right to receive data concerning you which you have provided to us in a structured, common and machine-readable format in accordance with the legal requirements, or to request its transmission to another controller.</li><li>Complaint to the supervisory authority: In accordance with the law and without prejudice to any other administrative or judicial remedy, you also have the right to lodge a complaint with a data protection supervisory authority, in particular a supervisory authority in the Member State where you habitually reside, the supervisory authority of your place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the GDPR.</li></ul>"},{"location":"privacy-policy/#m42","title":"Terminology and Definitions","text":"<p>This section provides an overview of the terms used in this privacy policy. Many of the terms are drawn from the law and defined mainly in Article 4 GDPR. The legal definitions are binding. The following explanations, on the other hand, are intended above all for the purpose of comprehension. The terms are sorted alphabetically.</p> <ul><li>Affiliate Tracking: Affiliate tracking logs links that the linking websites use to refer users to websites with products or other offers. The owners of the respective linked websites can receive a commission if users follow these so-called \"affiliate links\" and subsequently take advantage of the offers (e.g. buy goods or use services). To this end, it is necessary for providers to be able to track whether users who are interested in certain offers subsequently follow the affiliate links. It is therefore necessary for the functionality of affiliate links that they are supplemented by certain values that become part of the link or are otherwise stored, e.g. in a cookie. The values include in particular the source website (referrer), time, an online identification of the owner of the website on which the affiliate link was located, an online identification of the respective offer, an online identifier of the user, as well as tracking specific values such as advertising media ID, partner ID and categorizations </li><li>Controller: \"Controller\" means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data. </li><li>Conversion tracking: Conversion tracking is a method used to evaluate the effectiveness of marketing measures. For this purpose, a cookie is usually stored on the devices of the users within the websites on which the marketing measures take place and then called up again on the target website (e.g. we can thus trace whether the advertisements placed by us on other websites were successful). </li><li>IP Masking: IP masking is a method by which the last octet, i.e. the last two numbers of an IP address, are deleted so that the IP address alone can no longer be used to uniquely identify a person. IP masking is therefore a means of pseudonymising processing methods, particularly in online marketing. </li><li>Personal Data: \"personal data\" means any information relating to an identified or identifiable natural person (\"data subject\"); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. </li><li>Processing: The term \"processing\" covers a wide range and practically every handling of data, be it collection, evaluation, storage, transmission or erasure. </li><li>Profiles with user-related information: The processing of \"profiles with user-related information\", or \"profiles\" for short, includes any kind of automated processing of personal data that consists of using these personal data to analyse, evaluate or predict certain personal aspects relating to a natural person (depending on the type of profiling, this may include different information concerning demographics, behaviour and interests, such as interaction with websites and their content, etc.) (e.g. interests in certain content or products, click behaviour on a website or location). Cookies and web beacons are often used for profiling purposes. </li><li>Web Analytics: Web Analytics serves the evaluation of visitor traffic of online services and can determine their behavior or interests in certain information, such as content of websites. With the help of web analytics, website owners, for example, can recognize at what time visitors visit their website and what content they are interested in. This allows them, for example, to optimize the content of the website to better meet the needs of their visitors. For purposes of web analytics, pseudonymous cookies and web beacons are frequently used in order to recognise returning visitors and thus obtain more precise analyses of the use of an online service. </li></ul> <p></p>"},{"location":"robot-description/","title":"Robot Description","text":""},{"location":"robot-description/#diffbot-robot-description","title":"DiffBot Robot Description","text":"<p>The description of the 2WD robot will be created in its own package named <code>diffbot_description</code>.  The description uses URDF and xacro.  For this we create a package with <code>catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]]</code>:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description\nCreating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"...\nCreated file diffbot_description/CMakeLists.txt\nCreated file diffbot_description/package.xml\nSuccessfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description.\n</code></pre> <p>Because this package contains only descriptions and launch files it doesn't require any dependencies. </p> <p>According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch\n</code></pre> <p>The <code>urdf</code> folder will be used to keep the <code>urdf</code> and <code>xacro</code> files.  The <code>meshes</code> folder keeps the <code>meshes</code> that are included in the <code>urdf</code> file, and the <code>launch</code> folder keeps the ROS launch files.</p>"},{"location":"robot-description/#robot-model","title":"Robot Model","text":"<p>To model the two wheeled differential drive robot we follow REP-120. It states to use a <code>base_link</code> and a <code>base_footprint</code>. The resulting description files can be found in the <code>diffbot_description</code> package.</p>"},{"location":"robot-description/#required-tools","title":"Required Tools","text":"<p>To check a urdf file we can make use of the tools <code>check_urdf</code> and <code>urdf_to_graphiz</code> in the <code>liburdfdom-tools</code> debian package.  Install it with the following command:</p> <pre><code>sudo apt install liburdfdom-tools\n</code></pre> <p>First we need to convert the robot description of DiffBot, which is present as <code>xacro</code> file, to a <code>urdf</code> file by issuing the following command: </p> <pre><code>fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro `rospack find diffbot_description`/urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf\n</code></pre> <p>After we've created the <code>urdf</code> from the <code>xacro</code> file we can check the <code>urdf</code> files for errors with:</p> <pre><code>fjp@ubuntu:/tmp$ check_urdf diffbot.urdf \nrobot name is: diffbot\n---------- Successfully Parsed XML ---------------\nroot Link: base_footprint has 1 child(ren)\n    child(1):  base_link\n        child(1):  caster_link\n        child(2):  front_left_wheel\n        child(3):  front_right_wheel\n</code></pre> <p>It is also helpful to output a graphviz diagram of the robot model:</p> <pre><code>fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf \nCreated file diffbot.gv\nCreated file diffbot.pdf\nfjp@ubuntu:/tmp$ evince diffbot.pdf\n</code></pre> Graphviz diagram of DiffBot URDF robot description. <p>To visualize the 3D model in RViz we first need to install the <code>joint-state-publisher-gui</code> which was separated from non-gui <code>joint-state-publisher</code>. There exists a debian package which can be installed with the following command:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui\n</code></pre> <p>After installing the required dependency, the <code>view_diffbot.launch</code> launch file can be executed using <code>roslaunch</code> command:</p> <pre><code>fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch\n</code></pre> <p>According to the launch file's configuration, this will show the robot in RViz together with the <code>joint-state-publisher-gui</code> to set the joint values:</p> DiffBot displayed in RViz. <p>With the robot descripton loaded on the ROS parameter server, it's possible to use the <code>TF Tree</code> <code>rqt</code> plugin to display the transformation tree (see image above).</p> <p>In the next section, Gazebo Simulation, the robot model is prepared for simulation inside of  Gazebo.</p>"},{"location":"robot-description/#urdf-in-gazebo","title":"URDF in Gazebo","text":"<p>http://gazebosim.org/tutorials?tut=ros_urdf&amp;cat=connect_ros#Tutorial:UsingaURDFinGazebo</p>"},{"location":"robot-description/#references","title":"References","text":"<ul> <li>http://gazebosim.org/tutorials?tut=ros_urdf&amp;cat=connect_ros#Tutorial:UsingaURDFinGazebo</li> <li>https://answers.ros.org/question/30539/choosing-the-right-coefficients-for-gazebo-simulation/</li> <li>https://answers.ros.org/question/231880/how-to-improve-amcl-pose-estimate/</li> </ul>"},{"location":"technical_requirements/","title":"Technical requirements","text":"<p>To get started with building a ROS-based mobile robot, this section outlines the essential software and hardware you'll need. We'll explore key resources from the https://github.com/ros-mobile-robots organization on GitHub and provide options for building your own robot or using a pre-designed platform. Later sections will delve deeper into specific software installation and hardware components.</p> <p>Info</p> <p>This technical requirements page is here to give you an overview of what is required to get your robot up and running. You can already follow the steps in practice but they will be also mentioned in later sections (in more detail) when they are really needed.</p>"},{"location":"technical_requirements/#software-requirements","title":"Software requirements","text":"<p>One of the main software repositories is https://github.com/ros-mobile-robots/diffbot. It includes packages for simulation and the configurations and software to operate a real robot and interact with it from a development PC (or dev machine).</p> <p>The following sections give an overview about which software will be used:</p> <ul> <li>Operating Systems</li> <li>Git</li> <li>Remote Control</li> <li>Hardware Interface</li> <li>Source Dependencies</li> <li>Binary Dependencies</li> <li>Build ROS Workspace</li> </ul> <p>More detailed software setup instructions are found in the chapter about Processing Units.</p>"},{"location":"technical_requirements/#operating-system","title":"Operating System","text":"<p>For the development PC, you should have ROS Noetic installed on Ubuntu 20.04 or using Windows Subsystem for Linux (WSL) 2 running on Windows 11.</p> <p>Note</p> <p>Windows 11 is required for GUI features, such as Gazebo and RViz. WSL 2 on Windows 10 only provides command line support, although you can install an X11 server, such as VcXsrv, explained here.</p> <p>On the Single Board Computer (SBC) (e.g. Raspberry Pi 4 B) that is mounted on Remo, we use Ubuntu Mate 20.04 for arm64 architecture.</p>"},{"location":"technical_requirements/#git","title":"Git","text":"<p>As the software is hosted on GitHub which uses git as a version control system it needs to be present in your used operating system. </p> UbuntuWindows <p>On Ubuntu this is usually the case, which you can check with:</p> <pre><code>$ git --version\ngit version 2.25.1\n</code></pre> <p>On Windows you need to install Git using a package manager such as chocolatey or downloading it from https://git-scm.com/downloads.</p> <p>To clone large stl files from the Git repository we use git-lfs.  On both Ubuntu flavors it needs to be installed with the following terminal command:</p> <pre><code>sudo apt install git-lfs\n</code></pre>"},{"location":"technical_requirements/#remote-control","title":"Remote Control","text":"<p>On both the development PC and the SBC of the robot, you need a connection to the same local network and to enable the ssh protocol, to connect from the development PC (client) to the robot, which is running an open-ssh server. Install it on Ubuntu Mate 20.04 with the following:</p> <pre><code>sudo apt install openssh-server\n</code></pre>"},{"location":"technical_requirements/#hardware-requirements","title":"Hardware requirements","text":"<p>For the hardware, you can build your own two- or four-wheeled differential drive robot similar to the one present in the  <code>diffbot_description</code> package or 3D print a more stable Remo robot with the stl files in https://github.com/ros-mobile-robots/remo_description.</p> <p>The repository at https://github.com/ros-mobile-robots/remo_description contains the  robot description of Remo. Remo is a modular mobile robot platform, which is based on  NVIDIA's JetBot. The currently available parts can be 3D printed using the provided  stl files in the <code>remo_description</code> repository.  To do this, you either need a 3D printer with a recommended build volume of 15x15x15 cm or to use a local or online 3D print service. Further details are found in hardware setup.</p> <p>On the following components page you find a bill of materials and more details about each part.</p>"},{"location":"hardware_setup/3D_print/","title":"3D Printing","text":"<p>This page provides help to 3D print the parts of Remo robot. It contains information about configuring your slicer, suggestions to orient the parts and where support material is recommended to avoid failing prints and wasted PLA material.</p> <p>The table below gives an overview of the required parts of Remo and the average printing time:</p> <p>Note</p> <p>Remo robot is a modular robotics platform which means you don't have to print all parts. It is often possible to choose between different variants. For example:</p> <ul> <li>SBC Deck</li> <li>LiDAR Platform</li> <li>Camera Mount</li> </ul> Qt Part File Material Time Notes 1 Chassis <code>chassis.stl</code> PLA 1 Caster wheel <code>caster_base_65mm.stl</code> and <code>caster_shroud_65mm.stl</code> PLA Print both parts of the caster wheel 1 SBC Decks <code>raspberry_pi_deck.stl</code> or <code>jetson_nano_deck.stl</code> PLA Select one depending on used SBC 1 LiDAR Platform <code>platfom_rplidar_a2.stl</code> or <code>platfom_rplidar_a1.stl</code> PLA Select one dependin on used LiDAR 1 SLAMTEC USB to Serial holder <code>slamtec_holder.stl</code> or <code>jetson_nano_deck.stl</code> PLA 1 Camera Mount <code>camera_mount.stl</code> PLA"},{"location":"hardware_setup/3D_print/#chassis","title":"Chassis","text":""},{"location":"hardware_setup/3D_print/#caster-wheel","title":"Caster Wheel","text":"Qty Part 3D View 1 <code>caster_base_65mm.stl</code>"},{"location":"hardware_setup/3D_print/#sbc-decks","title":"SBC Decks","text":"<p>Currently there are two different decks you can print, depending on which SBC you are will use:</p> RPi DeckJetson Nano Deck Qty Part 3D View 1 <code>platfom_rplidar_a2.stl</code> Qty Part 3D View 1 <code>jetson_nano_deck.stl</code>"},{"location":"hardware_setup/3D_print/#lidar-platform","title":"LiDAR Platform","text":"RPLiDAR A2 M8RPLiDAR A1 M8 Qty Part 3D View 1 <code>platfom_rplidar_a2.stl</code> Qty Part 3D View 1 <code>slamtec_holder.stl</code> <p>To be announced.</p>"},{"location":"hardware_setup/3D_print/#camera-mount","title":"Camera Mount","text":"Qty Part 3D View 1 <code>camera_mount.stl</code> <p>You can choose between the following camera adjustment mounts:</p> Raspi Camera v2OAK 1OAK D Qty Part 3D View 1 <code>Raspberry_pi_CAM_holder.stl</code> Qty Part 3D View 1 <code>OAK-1_adjustment_mount.stl</code> Qty Part 3D View 1 <code>OAK-D_adjustment_mount.stl</code>"},{"location":"hardware_setup/assembly/","title":"Assembly","text":"<p>The following video gives an overview of the robot's components and how it will be assembled:</p> <p>More detailed assembly instructions are found in the next few sections:</p>"},{"location":"hardware_setup/assembly/#frame","title":"Frame","text":""},{"location":"hardware_setup/assembly/#bread-board","title":"Bread Board","text":""},{"location":"hardware_setup/electronics/","title":"Electronics","text":"<p>The bread board view from Fritzing shows the connection schematic. Both models (DiffBot and Remo) use slgithly different hardware (e.g. motor driver) which you can see in the following:</p> RemoDiffBot <p></p> <p></p>"},{"location":"hardware_setup/install/","title":"Installation","text":"<p>After completing the hardware setup, the next steps that follows consist of</p> <ul> <li>Preparing the software on the processing units</li> <li>Installing the ROS software packages</li> </ul> <p>If you want to learn more about the theory behind robotics visit Robotics Theory pages.</p>"},{"location":"hardware_setup/overview/","title":"Hardware Setup Overview","text":"<p>The following pages guide you on how to setup the hardware of your robot (either DiffBot or Remo).</p> <ul> <li>3D Printing is only relevant for Remo robot (not DiffBot).    Here you will learn which parts to print and some suggestions to configure your slicer.</li> <li> <p>Electronics provides instructions to connect the single board computer (e.g. Raspberry Pi), microcontroller (e.g. Teensy) as well as   the other components such as the motor driver, motors and laser scanner.</p> Preview of connection schematic <p>The bread board view from Fritzing shows the connection schematic. Both models (DiffBot and Remo) use slgithly different hardware (e.g. motor driver) which you can see in the following:</p> RemoDiffBot <p></p> <p></p> </li> <li> <p>Assembly give instructions to assemble Remo robot.</p> </li> </ul>"},{"location":"insiders/","title":"Insiders","text":""},{"location":"insiders/#insiders","title":"Insiders","text":"<p>Diffbot and Remo follow the sponsorware release strategy, which means that new features are first exclusively released to sponsors as part of Insiders. Read on to learn what sponsorships achieve, how to become a sponsor to get access to Insiders, and what's in for you!</p>"},{"location":"insiders/#what-is-insiders","title":"What is Insiders?","text":"<p>DiffBot and Remo Insiders are private forks of DiffBot and Remo respectively, hosted as a private GitHub repository. Almost<sup>1</sup> all new features are developed as part of these forks, which means that they are immediately available to all eligible sponsors, as they are made collaborators of this repository.</p> <p>Every feature is tied to a [funding goal] in monthly subscriptions. When a funding goal is hit, the features that are tied to it are merged back into DiffBot or Remo and released for general availability, making them available to all users. Bugfixes are always released in tandem.</p> <p>Sponsorships start as low as $10 a month.<sup>2</sup></p>"},{"location":"insiders/#what-sponsorships-achieve","title":"What sponsorships achieve","text":"<p>Sponsorships make this project sustainable, as they buy the maintainers of this project time \u2013 a very scarce resource \u2013 which is spent on the development of new features, bug fixing, stability improvement, issue triage and general support. The biggest bottleneck in Open Source is time.<sup>3</sup></p>"},{"location":"insiders/#how-to-become-a-sponsor","title":"How to become a sponsor","text":"<p>Thanks for your interest in sponsoring! In order to become an eligible sponsor with your GitHub account, visit fjp's sponsor profile, and complete a sponsorship of $10 a month or more. You can use your individual or organization GitHub account for sponsoring.</p> <p>You can cancel your sponsorship anytime.<sup>4</sup></p> <p>:octicons-heart-fill-24:{ .mdx-heart } \u00a0 Join our  awesome sponsors</p>      If you sponsor publicly, you're automatically added here with a link to     your profile and avatar to show your support for DiffBot and Remo.     Alternatively, if you wish to keep your sponsorship private, you'll be a     silent +1. You can select visibility during checkout and change it     afterwards."},{"location":"insiders/#available-features","title":"Available features","text":"<p>The following features are solely available via DiffBot and Remo Insiders:</p> <ul> <li> [TBA] :material-new-box:</li> <li> [TBA] :material-new-box:</li> <li> [TBA]</li> </ul>"},{"location":"insiders/#funding","title":"Funding","text":""},{"location":"insiders/#goals","title":"Goals","text":"<p>The following section lists all funding goals. Each goal contains a list of features prefixed with a checkmark symbol, denoting whether a feature is :octicons-check-circle-fill-24:{ style=\"color: #00e676\" } already available or  :octicons-check-circle-fill-24:{ style=\"color: var(--md-default-fg-color--lightest)\" } planned, but not yet implemented. When the funding goal is hit, the features are released for general availability.</p>"},{"location":"insiders/#500-barglistock","title":"$ 500 \u2013 B\u00e4rglistock","text":"<ul> <li> PID tuning on MCU using topic instead of requiring re-flashing</li> <li> Remo: RPLidar 1 lidar platform</li> <li> Add tutorials to documentation</li> </ul>"},{"location":"insiders/#1000-mont-durand","title":"$ 1,000 \u2013 Mont Durand","text":"<ul> <li> Raspberry Pi Camera support</li> <li> Reset odometry</li> </ul>"},{"location":"insiders/#2000-eiger","title":"$ 2,000 \u2013 Eiger","text":"<ul> <li> IMU support</li> <li> OAK-1 support</li> <li> OAK-D support</li> </ul>"},{"location":"insiders/#2500-castor","title":"$ 2,500 \u2013 Castor","text":"<ul> <li> Display (HMI) support</li> <li> Add <code>diffbot_mbf</code> package using <code>move_base_flex</code>, the improved version of <code>move_base</code></li> </ul>"},{"location":"insiders/#5000-nordend","title":"$ 5,000 \u2013 Nordend","text":"<ul> <li> ROS 2</li> </ul>"},{"location":"insiders/#goals-completed","title":"Goals completed","text":"<p>This section lists all funding goals that were previously completed, which means that those features were part of Insiders, but are now generally available and can be used by all users.</p>"},{"location":"insiders/#0-schneestock","title":"$ 0 \u2013 Schneestock","text":"<ul> <li> Documentation using Material for MkDocs</li> <li> Make use of rosdep for system dependencies</li> <li> Parametrize sensor description when using standard laser instead of gpu laser</li> <li> vcstool to simplify external dependency installation</li> </ul>"},{"location":"insiders/#frequently-asked-questions","title":"Frequently asked questions","text":""},{"location":"insiders/#payment","title":"Payment","text":"<p>We don't want to pay for sponsorship every month. Are there any other options?</p> <p>Yes. You can sponsor on a yearly basis by switching your GitHub account to a yearly billing cycle. If for some reason you cannot do that, you could also create a dedicated GitHub account with a yearly billing cycle, which you only use for sponsoring (some sponsors already do that).</p> <p>If you have any problems or further questions, please reach out to ros@fjp.at.</p>"},{"location":"insiders/#terms","title":"Terms","text":"<p>Are we allowed to use Insiders under the same terms and conditions as DiffBot and Remo?</p> <p>Yes. Whether you're an individual or a company, you may use DiffBot and Remo Insiders precisely under the same terms as DiffBot and Remo, which are given by the BSD 3-Clause License. However, we kindly ask you to respect the following guidelines:</p> <ul> <li> <p>Please don't distribute the source code of Insiders. You may freely use   it for public, private or commercial projects, privately fork or mirror it,   but please don't make the source code public, as it would counteract the    sponsorware strategy.</p> </li> <li> <p>If you cancel your subscription, you're automatically removed as a   collaborator and will miss out on all future updates of Insiders. However, you   may use the latest version that's available to you as long as you like.   Just remember that GitHub deletes private forks.</p> </li> </ul> <ol> <li> <p>In general, every new feature is first exclusively released to sponsors, but sometimes upstream dependencies enhance existing features that must be supported by DiffBot.\u00a0\u21a9</p> </li> <li> <p>Note that $10 a month is the minimum amount to become eligible for Insiders. While GitHub Sponsors also allows to sponsor lower amounts or one-time amounts, those can't be granted access to Insiders due to technical reasons.\u00a0\u21a9</p> </li> <li> <p>Making an Open Source project sustainable is exceptionally hard: maintainers burn out, projects are abandoned. That's not great and very unpredictable. The sponsorware model ensures that if you decide to use DiffBot or Remo, you can be sure that bugs are fixed quickly and new features are added regularly.\u00a0\u21a9</p> </li> <li> <p>If you cancel your sponsorship, GitHub schedules a cancellation request which will become effective at the end of the billing cycle. This means that even though you cancel your sponsorship, you will keep your access to Insiders as long as your cancellation isn't effective. All charges are processed by GitHub through Stripe. As we don't receive any information regarding your payment, and GitHub doesn't offer refunds, sponsorships are non-refundable.\u00a0\u21a9</p> </li> </ol>"},{"location":"packages/","title":"ROS Software Packages","text":"<p>After having verified that the hardware requirements for the Navigation Stack are met, an overview of Remo's software follows.</p>"},{"location":"packages/#software-requirements-for-the-ros-navigation-stack","title":"Software requirements for the ROS Navigation Stack","text":"<p>The <code>diffbot</code> and <code>remo_description</code> repositories contain the following ROS packages:</p> <ul> <li><code>diffbot_base</code>: This package contains the platform-specific code for the base controller component required by the ROS Navigation Stack. It consists of the firmware based on rosserial for the Teensy MCU and the C++ node running on the SBC that instantiates the ROS Control hardware interface including the <code>controller_manager</code> control loop for the real robot. The low-level <code>base_controller</code> component reads the encoder ticks from the hardware, calculates angular joint positions and velocities, and publishes them to the ROS Control hardware interface. Using this interface makes it possible to use the <code>diff_drive_controller</code> package from ROS Control. It provides a controller (<code>DiffDriveController</code>) for a differential drive mobile base that computes target joint velocities from commands received by either a teleop node or the ROS Navigation Stack. The computed target joint velocities are forwarded to the low-level base controller, where they are compared to the measured velocities to compute suitable motor PWM signals using two separate PID controllers, one for each motor.</li> <li><code>diffbot_bringup</code>: Launch files to bring up the hardware driver nodes (camera, lidar, microcontroller, and so on) as well as the C++ nodes from the <code>diffbot_base</code> package for the real robot.</li> <li><code>diffbot_control</code>: Configurations for <code>DiffDriveController</code> and <code>JointStateController</code> of ROS Control used in the Gazebo simulation and the real robot. The parameter configurations are loaded onto the parameter server with the help of the launch files inside this package.</li> <li><code>remo_description</code>: This package contains the URDF description of Remo including its sensors. It allows you to pass arguments to visualize different camera and SBC types. It also defines the <code>gazebo_ros_control</code> plugin. Remo's description is based on the description at https://github.com/ros-mobile-robots/mobile_robot_description, which provides a modular URDF structure that makes it easier to model your own differential drive robot.</li> <li><code>diffbot_gazebo</code>: Simulation-specific launch and configuration files for Remo and Diffbot, to be used in the Gazebo simulator.</li> <li><code>diffbot_msgs</code>: Message definitions specific to Remo/Diffbot, for example, the message for encoder data is defined in this package.</li> <li><code>diffbot_navigation</code>: This package contains all the required configuration and launch files for the ROS Navigation Stack to work.</li> <li><code>diffbot_slam</code>: Configurations for simultaneous localization and mapping using implementations such as gmapping to create a map of the environment.</li> </ul> <p>After this overview of the ROS packages of a differential robot that fulfill the requirements of the Navigation Stack, the next pages explain those packages in more detail.</p>"},{"location":"packages/packages-setup/","title":"Diffbot ROS Packages","text":"<p>The following describes the easiest way to make use of diffbot's ROS packages inside the ros-mobile-robots/diffbot repository.</p> <p>The following steps will be performed on both, the workstation/development PC and the single board computer (SBC).</p>"},{"location":"packages/packages-setup/#git-clone-diffbot-repository","title":"Git: clone diffbot repository","text":"<p>After setting up ROS on your workstation PC and the SBC (either Raspberry Pi 4B or Jetson Nano), create a ros workspace in your users home folder and clone the <code>diffbot</code> repository:</p> <pre><code>mkdir -p ros_ws/src\ngit clone https://github.com/ros-mobile-robots/diffbot.git\n</code></pre>"},{"location":"packages/packages-setup/#obtain-system-dependencies","title":"Obtain (system) Dependencies","text":"<p>The <code>diffbot</code> repository relies on two sorts of dependencies:</p> <ul> <li>Source (non binary) dependencies from other (git) repositories.</li> <li>System dependencies available in the (ROS) Ubuntu package repositories. Also refered to as pre built binaries.</li> </ul>"},{"location":"packages/packages-setup/#source-dependencies","title":"Source Dependencies","text":"<p>Let's first obtain source dependencies from other repositories.  To do this the recommended tool to use is <code>vcstool</code> (see also https://github.com/dirk-thomas/vcstool for additional documentation and examples.).</p> <p>Note</p> <p><code>vcstool</code> replaces <code>wstool</code>.</p> <p>Inside the cloned <code>diffbot</code> repository,  make use of the <code>import</code> command and the <code>diffbot.repos</code> file containing the required source repositories:</p> <pre><code>vcs import &lt; diffbot.repos\n</code></pre> <p>This will clone all repositories which are stored in the <code>diffbot.repos</code> that get passed in via stdin in YAML format.</p> <p>Note</p> <p>The file <code>diffbot.repos</code> contains relative paths and will clone the listed repositories in the parent folder from where the <code>vcs import</code> command is called. When it is called from inside the <code>diffbot</code> repository, which should be located in the <code>src</code> folder of a catkin workspace, then the other repositories are also cloned in the <code>src</code> folder.</p> <p>For the SBC not all dependencies in <code>diffbot.repos</code> are needed. Instead the <code>diffbot_robot.repos</code> is here to clone the <code>rplidar_ros</code> repository.</p> <pre><code>vcs import &lt; diffbot_robot.repos\n</code></pre> <p>Now that additional packages are inside the catkin workspace it is time to install the system dependencies.</p>"},{"location":"packages/packages-setup/#system-dependencies","title":"System Dependencies","text":"<p>All the needed ROS system dependencies which are required by diffbot's packages can be installed using <code>rosdep</code> command, which was installed during the ROS setup. To install all system dependencies use the following command:</p> <pre><code>rosdep install --from-paths src --ignore-src -r -y\n</code></pre> <p>Info</p> <p>On the following packages pages it is explained that the dependencies of a ROS package are defined inside its <code>package.xml</code>.</p> <p>After the installation of all dependencies finished (which can take a while), it is time to build the catkin workspace.  Inside the workspace use <code>catkin-tools</code> to build the packages inside the <code>src</code> folder.</p> <p>Note</p> <p>The first time you run the following command, make sure to execute it inside your catkin workspace and not the <code>src</code> directory.</p> <pre><code>catkin build\n</code></pre> <p>Now source the catkin workspace either using the created alias or the full command for the bash shell:</p> <pre><code>source devel/setup.bash\n</code></pre>"},{"location":"packages/packages-setup/#examples","title":"Examples","text":"<p>Now you are ready to follow the examples listed in the readme.</p> <p>Info</p> <p>TODO extend documentation with examples</p>"},{"location":"packages/packages-setup/#optional-infos","title":"Optional Infos","text":""},{"location":"packages/packages-setup/#manual-dependency-installation","title":"Manual Dependency Installation","text":"<p>To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the <code>src</code> folder of a ros catkin workspace and execute the <code>catkin build</code> command. Also make sure to source the workspace after building new packages with <code>source devel/setup.bash</code>.</p> <pre><code>cd /homw/fjp/git/diffbot/ros/  # Navigate to the workspace\ncatkin build              # Build all the packages in the workspace\nls build                  # Show the resulting build space\nls devel                  # Show the resulting devel space\n</code></pre> <p>Note</p> <p>Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.</p>"},{"location":"packages/raspicam_node/","title":"Raspberry Pi Camera","text":"<p>The diffbot robot uses the <code>UbiquityRobotics/raspicam_node</code> to interface the Raspberry Pi Camera v2.</p> RPi Camera v2."},{"location":"packages/raspicam_node/#setup","title":"Setup","text":"<p>Currently there exists  no binary of the <code>raspicam_node</code> for ROS noetic (only for kinetic there is the <code>ros-kinetic-raspicam-node</code>). To work with the <code>raspicam_node</code> for ROS Noetic, you have to build it from source with steps  outlined in the build intstructions of the readme. For completeness, these steps are listed here:</p> <p>First go to your catkin_ws cd <code>~/ros_ws/src</code> and download the source for this node by running</p> <pre><code>git clone https://github.com/UbiquityRobotics/raspicam_node.git\n</code></pre> <p>There are some dependencies that are not recognized by ROS, so you need to create the file <code>/etc/ros/rosdep/sources.list.d/30-ubiquity.list</code> and add  the following to it:</p> <pre><code>yaml https://raw.githubusercontent.com/UbiquityRobotics/rosdep/master/raspberry-pi.yaml\n</code></pre> <p>This will add ROS dependency sources, which are relevant for the Raspberry Pi. </p> <p>For completeness you can list the content of this yaml file here:</p> Show content <pre><code>libraspberrypi0:\n  debian:\n    apt:\n      packages: [libraspberrypi0]\n  ubuntu:\n    apt:\n      packages: [libraspberrypi0]\nlibraspberrypi-dev:\n  debian:\n    apt:\n      packages: [libraspberrypi-dev]\n  ubuntu:\n    apt:\n      packages: [libraspberrypi-dev]\nlibpigpio:\n  debian:\n    apt:\n      packages: [libpigpio-dev]\n  ubuntu:\n    apt:\n      packages: [libpigpio-dev]\nlibpigpiod-if:\n  debian:\n    apt:\n      packages: [libpigpiod-if-dev]\n  ubuntu:\n    apt:\n      packages: [libpigpiod-if-dev]\n</code></pre> <p>To make use of these sources run </p> <pre><code>rosdep update\n</code></pre> <p>Next, we parse the <code>package.xml</code> file for required dependencies and also install them with the <code>rosdep</code> command:</p> <pre><code>cd ~/ros_ws\nrosdep install --from-paths src --ignore-src --rosdistro=noetic -y\n</code></pre> <p>Finally, you can compile the code with the <code>catkin build</code> command from <code>catkin-tools</code> or  use the legacy <code>catkin_make</code> command.</p>"},{"location":"packages/raspicam_node/#running-the-node","title":"Running the node","text":"<p>To publish the camera image on the <code>/raw/image/compressed</code> topic run the following on the SBC of the robot:</p> <pre><code>roslaunch raspicam_node camerav2_410x308_30fps.launch\n</code></pre> <p>Running <code>rostopic list</code> should show the topics of raspicam node:</p> <pre><code>/raspicam_node/camera_info\n/raspicam_node/image/compressed\n/raspicam_node/parameter_descriptions\n/raspicam_node/parameter_updates\n</code></pre> <p>To view the compressed image run the following command on your development PC:</p> <pre><code>rosrun image_view image_view image:=/raspicam_node/image _image_transport:=compressed\n</code></pre> <p>This will open up a window where you can see the camera image. If it is dark, check if you removed the lense protection ;-)</p> <p>Note</p> <p>There is also a <code>rpicamera.launch</code> in the <code>diffbot_bringup</code> package which you can make use of:</p> <pre><code>roslaunch diffbot_bringup rpicamera.launch\n</code></pre>"},{"location":"packages/remo_description/","title":"Remo Description","text":"<p>ROS URDF description package of REMO robot (Research Education Mobile/Modular robot) a highly modifiable and extendable autonomous mobile robot based on Nvidia's Jetbot. This ROS package is found in the <code>remo_description</code> repository contains the stl files to 3D print Remo robot.</p> <p></p> <p>You can explore the model in more detail through the following Fusion 360 viewer:</p>"},{"location":"packages/remo_description/#usage","title":"Usage","text":"<p>This is a ROS package which should be cloned in a catkin workspace. To use <code>remo_description</code> inside a Gazebo simulation or on a real 3D printed Remo robot, you can directly make use of the ROS packages in the ros-mobile-robots/diffbot repository. Most of the launch files you find in the <code>diffbot</code> repository accept a <code>model</code> argument. Just append <code>model:=remo</code> to the end of a <code>roslaunch</code> command to make use of this <code>remo_description</code> package.</p>"},{"location":"packages/remo_description/#git-lfs-and-bandwith-quota","title":"Git LFS and Bandwith Quota","text":"<p>The binary stl files are hosted on GitHub using Git Large File Storage (git lfs)  to avoid increasing the total size of the repository because of possible stl file changes. For open source repositories, GitHub has a bandwith limit of 1 GB (up to 1.5 GB) per month.  Depending on how many users clone/pull the stl files using git lfs per month, this bandwith can be exhausted after a few days.  If you are not able to clone/pull the stl files and only get the pointer files, you have to wait until the bandwith quota resets back to zero.  In case you need the stl files immediately, and to support this work you can get immediate access to the stl files:</p> <p>Access Remo STL files</p> <p>Also if you find this work useful please consider the funding options to support the development and design of this robot. However, you will always be able to clone/pull and use the Remo stl files once the bandwith quota resets.</p>"},{"location":"packages/remo_description/#assembly","title":"Assembly","text":"<p>For assembly instructions please watch the video below:</p> <p></p>"},{"location":"packages/remo_description/#camera-types","title":"Camera Types","text":"<p>The <code>remo.urdf.xacro</code> accepts a <code>camera_type</code> xacro arg which lets you choose between the following different camera types</p> Raspicam v2 with IMX219 OAK-1 OAK-D"},{"location":"packages/remo_description/#single-board-computer-types","title":"Single Board Computer Types","text":"<p>Another xacro argument is the <code>sbc_type</code> wher you can select between <code>jetson</code> and <code>rpi</code>.</p> Jetson Nano Raspberry Pi 4 B"},{"location":"packages/remo_description/#handshake-acknowledgment","title":":handshake: Acknowledgment","text":"<ul> <li>Louis Morandy-Rapin\u00e9 for his great work on REMO robot and designing it in Fusion 360.</li> </ul>"},{"location":"packages/remo_description/#references","title":"References","text":"<ul> <li>Nvidia Jetbot</li> </ul>"},{"location":"packages/diffbot_base/","title":"Autonomous Differential Drive Mobile Robot - Base Package","text":"","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/#diffbot-base-package","title":"DiffBot Base Package","text":"<p>This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with  ROS Control. </p> ROS Control Overview. <p>This package contains the platform-specific code for the base controller component required by the ROS Navigation Stack. It consists of the firmware based on rosserial for the Teensy MCU and the C++ node running on the SBC that instantiates the ROS Control hardware interface, which includes the <code>controller_manager</code> control loop for the real robot.</p> <p>The low-level <code>base_controller</code> component reads the encoder ticks from the hardware, calculates angular joint positions and velocities, and publishes them to the ROS Control hardware interface. Making use of this interface makes it possible to use the <code>diff_drive_controller</code> package from ROS Control. It provides a controller (<code>DiffDriveController</code>) for a differential drive mobile base that computes target joint velocities from commands received by either a teleop node or the ROS Navigation Stack. The computed target joint velocities are forwarded to the low-level base controller, where they are compared to the measured velocities to compute suitable motor PWM signals using two separate PID controllers, one for each motor.</p> <p>Another part of this package is a launch file that will</p> <ul> <li>Load the robot description from <code>diffbot_description</code> to the paramter server</li> <li>Run the hardware interface of this package <code>diffbot_base</code></li> <li>Load the controller configuration yaml from the <code>diffbot_control</code> package to the parameter server</li> <li>Load the controllers with the controller manager</li> <li>Load the value of the encoder resolution to the parameter server</li> </ul>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/#low-level-vs-high-level-pid","title":"Low-Level vs High-Level PID","text":"<p>There exist (at least) two commonly used approaches to control a robot base.</p> <p>The difference between the two presented approaches here is where the PID controller(s) that control each motor are kept. One possibility is to run these PIDs on the high level hardware interface on the SBC and sending the computed output commands to a motor driver node. Another option operates the PIDs on the low-level microcontroller hardware. For DiffBot these approaches are refered to as</p> <ul> <li>High-Level PIDs running on the hardware interface on the SBC</li> <li>Low-Level PIDs running on the firmware of the microcontroller</li> </ul> <p>The DiffBot project initially used the PID controllers in the high level hardware interface. From release 1.0.0 on two low-level PIDs are operating on the low-level base controller hardware.</p> <p>This section focuses on the current approach (low-level PIDs). The previous approach (high-level PID) is documented in High-Level Approach.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/#developing-a-low-level-controller-firmware-and-a-high-level-ros-control-hardware-interface-for-a-differential-drive-robot","title":"Developing a low-level controller firmware and a high-level ROS Control hardware interface for a differential drive robot","text":"<p>In the following two sections, the base controller, mentioned in the Navigation Stack, will be developed.</p> <p></p> <p>For DiffBot/Remo, this platform-specific node is split into two software components.</p> <p>The first component is the high-level <code>diffbot::DiffBotHWInterface</code> that inherits from <code>hardware_interface::RobotHW</code>, acting as an interface between robot hardware and the packages of ROS Control that communicate with the Navigation Stack and provide <code>diff_drive_controller</code> \u2013 one of many available controllers from ROS Control. With the <code>gazebo_ros_control</code> plugin, the same controller including its configuration can be used in simulation and the real robot.</p> <p>An overview of ROS Control in simulation and the real world is given in the following figure (http://gazebosim.org/tutorials/?tut=ros_control):</p> <p> </p> ROS Control in Simulation and Reality <p>The second component is the low-level base controller that measures angular wheel joint positions and velocities and applies the commands from the high-level interface to the wheel joints. The following figure shows the communication between the two components:</p> <p> </p> Block diagram of the low-level controller and the high-level hardware interface (ROS Control) <p>The low-level base controller uses two PID controllers to compute PWM signals for each motor based on the error between measured and target wheel velocities. <code>RobotHW</code> receives measured joint states (angular position (rad) and angular velocity (rad/s)) from which it updates its joint values. With these measured velocities and the desired command velocity (<code>geometry_msgs/Twist</code> message on the <code>cmd_vel</code> topic), from the Navigation Stack, the <code>diff_drive_controller</code> computes the target angular velocities for both wheel joints using the mathematical equations of a differential drive robot. This controller works with continuous wheel joints through a <code>VelocityJointInterface</code> class. The computed target commands are then published within the high-level hardware interface inside the robot's <code>RobotHW::write</code> method. Additionally, the controller computes and publishes the odometry on the odom topic (<code>nav_msgs/Odometry</code>) and the transform from <code>odom</code> to <code>base_footprint</code>.</p> <p>Having explained the two components of the base controller, the low-level firmware is implemented first. The high-level hardware interface follows the next section.</p> <p>But before this an introduction to the PID controllers are given in PID Controllers.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/#diffbot_base-package","title":"diffbot_base Package","text":"<p>The <code>diffbot_base</code> package was created with <code>catkin-tools</code>:</p> <pre><code>fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts                 \nCreating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"...\nCreated file diffbot_base/package.xml\nCreated file diffbot_base/CMakeLists.txt\nCreated folder diffbot_base/include/diffbot_base\nCreated folder diffbot_base/src\nSuccessfully created package files in /home/fjp/catkin_ws/src/diffbot_base.\n</code></pre> <p>To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies should have been install during the initial setup phase. These dependencies are not already part of the ROS Noetic desktop full installation but required for the <code>diffbot_base</code> package.</p> <p>Note</p> <p>Follow the instructions at ROS Noetic Setup on how to setup ROS and the obtain (system) dependencies section on how to install all required dependencies. Performing these steps avoids installing any dependencies manually.</p> Dependency Source Ubuntu/Debian Package <code>rosparam_shortcuts</code> https://github.com/PickNikRobotics/rosparam_shortcuts <code>ros-noetic-rosparam-shortcuts</code> <code>hardware_interface</code> https://github.com/ros-controls/ros_control <code>ros-noetic-ros-control</code> <code>diff_drive_controller</code> https://github.com/ros-controls/ros_controllers <code>ros-noetic-ros-controllers</code>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/","title":"Base Package - High Level Approach","text":"","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#diffbot-base-package-high-level-approach","title":"DiffBot Base Package - High Level Approach","text":"<p>This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with  ROS Control. </p> ROS Control Overview. <p>In the simpleste case all that is needed in this package is to write a class that inherits from <code>hardware_interface::RobotHW</code> and provide a launch file. The launch file will </p> <ul> <li>Load the robot description from <code>diffbot_description</code> to the paramter server</li> <li>Run the hardware interface of this package <code>diffbot_base</code></li> <li>Load the controller configuration yaml from the <code>diffbot_control</code> package to the parameter server</li> <li>Load the controllers with the controller manager</li> <li>Load the value of the encoder resolution to the parameter server</li> </ul>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#diffbot_base-package","title":"diffbot_base Package","text":"<p>The <code>diffbot_base</code> package is created with <code>catkin-tools</code>:</p> <pre><code>fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts                 \nCreating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"...\nCreated file diffbot_base/package.xml\nCreated file diffbot_base/CMakeLists.txt\nCreated folder diffbot_base/include/diffbot_base\nCreated folder diffbot_base/src\nSuccessfully created package files in /home/fjp/catkin_ws/src/diffbot_base.\n</code></pre> <p>To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. </p> Dependency Source Ubuntu/Debian Package <code>rosparam_shortcuts</code> https://github.com/PickNikRobotics/rosparam_shortcuts <code>ros-noetic-rosparam-shortcuts</code> <code>hardware_interface</code> https://github.com/ros-controls/ros_control <code>ros-noetic-ros-control</code> <code>diff_drive_controller</code> https://github.com/ros-controls/ros_controllers <code>ros-noetic-ros-controllers</code> <p>To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the <code>src</code> folder of a ros catkin workspace and execute the <code>catkin build</code> command. Also make sure to source the workspace after building new packages with <code>source devel/setup.bash</code>.</p> <pre><code>cd /homw/fjp/git/diffbot/ros/  # Navigate to the workspace\ncatkin build              # Build all the packages in the workspace\nls build                  # Show the resulting build space\nls devel                  # Show the resulting devel space\n</code></pre> <p>Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#hardware-interface","title":"Hardware Interface","text":"<p>See the <code>include</code> and <code>src</code> folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control, it gives more details and also this overview article about ROS Control.</p> <p>The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the <code>diff_drive_controller</code> that is configured in the <code>diffbot_control</code> package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the <code>gazebo_ros_control</code> package to communicate with the <code>diff_drive_controller</code>. For the real robot hardware, ROS Control uses an instance of type <code>hardware_interface::RobotHW</code> that is passed to the <code>controller_manager</code> to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded.</p> <p>The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from  the robot's description from the ROS parameter server:</p> <pre><code>namespace diffbot_base\n{\n    DiffBotHWInterface::DiffBotHWInterface(ros::NodeHandle &amp;nh, urdf::Model *urdf_model)\n        : name_(\"hardware_interface\")\n        , nh_(nh)\n    { \n        // Initialization of the robot's resources (joints, sensors, actuators) and\n        // interfaces can be done here or inside init().\n        // E.g. parse the URDF for joint names &amp; interfaces, then initialize them\n        // Check if the URDF model needs to be loaded\n        if (urdf_model == NULL)\n            loadURDF(nh, \"robot_description\");\n        else\n            urdf_model_ = urdf_model;\n\n        // Load rosparams\n        ros::NodeHandle rpnh(nh_, name_);\n        std::size_t error = 0;\n        // Code API of rosparam_shortcuts:\n        // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d\n        error += !rosparam_shortcuts::get(name_, rpnh, \"joints\", joint_names_);\n        error += !rosparam_shortcuts::get(name_, nh_, \"mobile_base_controller/wheel_radius\", wheel_radius_);\n        error += !rosparam_shortcuts::get(name_, nh_, \"mobile_base_controller/linear/x/max_velocity\", max_velocity_);\n        rosparam_shortcuts::shutdownIfError(name_, error);\n\n        wheel_diameter_ = 2.0 * wheel_radius_;\n        //max_velocity_ = 0.2; // m/s\n        // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s\n        max_velocity_ = linearToAngular(max_velocity_);\n\n        // Setup publisher for the motor driver \n        pub_left_motor_value_ = nh_.advertise&lt;std_msgs::Int32&gt;(\"motor_left\", 1);\n        pub_right_motor_value_ = nh_.advertise&lt;std_msgs::Int32&gt;(\"motor_right\", 1);\n\n        // Setup subscriber for the wheel encoders\n        sub_left_encoder_ticks_ = nh_.subscribe(\"ticks_left\", 1, &amp;DiffBotHWInterface::leftEncoderTicksCallback, this);\n        sub_right_encoder_ticks_ = nh_.subscribe(\"ticks_right\", 1, &amp;DiffBotHWInterface::rightEncoderTicksCallback, this);\n\n        // Initialize the hardware interface\n        init(nh_, nh_);\n    }\n\n\n    bool DiffBotHWInterface::init(ros::NodeHandle &amp;root_nh, ros::NodeHandle &amp;robot_hw_nh)\n    {\n        ROS_INFO(\"Initializing DiffBot Hardware Interface ...\");\n        num_joints_ = joint_names_.size();\n        ROS_INFO(\"Number of joints: %d\", (int)num_joints_);\n        std::array&lt;std::string, NUM_JOINTS&gt; motor_names = {\"left_motor\", \"right_motor\"};\n        for (unsigned int i = 0; i &lt; num_joints_; i++)\n        {\n            // Create a JointStateHandle for each joint and register them with the \n            // JointStateInterface.\n            hardware_interface::JointStateHandle joint_state_handle(joint_names_[i],\n                                                                    &amp;joint_positions_[i], \n                                                                    &amp;joint_velocities_[i],\n                                                                    &amp;joint_efforts_[i]);\n            joint_state_interface_.registerHandle(joint_state_handle);\n\n            // Create a JointHandle (read and write) for each controllable joint\n            // using the read-only joint handles within the JointStateInterface and \n            // register them with the JointVelocityInterface.\n            hardware_interface::JointHandle joint_handle(joint_state_handle, &amp;joint_velocity_commands_[i]);\n            velocity_joint_interface_.registerHandle(joint_handle);\n\n            // Initialize joint states with zero values\n            joint_positions_[i] = 0.0;\n            joint_velocities_[i] = 0.0;\n            joint_efforts_[i] = 0.0; // unused with diff_drive_controller\n\n            joint_velocity_commands_[i] = 0.0;\n\n            // Initialize the pid controllers for the motors using the robot namespace\n            std::string pid_namespace = \"pid/\" + motor_names[i];\n            ROS_INFO_STREAM(\"pid namespace: \" &lt;&lt; pid_namespace);\n            ros::NodeHandle nh(root_nh, pid_namespace);\n            // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what.\n            pids_[i].init(nh, 0.0, 10.0, 1.0, 1.0, 0.0, 0.0, false, -max_velocity_, max_velocity_);\n            pids_[i].setOutputLimits(-max_velocity_, max_velocity_);\n        }\n\n        // Register the JointStateInterface containing the read only joints\n        // with this robot's hardware_interface::RobotHW.\n        registerInterface(&amp;joint_state_interface_);\n\n        // Register the JointVelocityInterface containing the read/write joints\n        // with this robot's hardware_interface::RobotHW.\n        registerInterface(&amp;velocity_joint_interface_);\n\n        ROS_INFO(\"... Done Initializing DiffBot Hardware Interface\");\n        return true;\n    }\n\n    // The read method is part of the control loop cycle (read, update, write) and is used to \n    // populate the robot state from the robot's hardware resources (joints, sensors, actuators). \n    // This method should be called before controller_manager::ControllerManager::update() and write.\n    void DiffBotHWInterface::read(const ros::Time&amp; time, const ros::Duration&amp; period)\n    {\n        ros::Duration elapsed_time = period;\n\n        // Read from robot hw (motor encoders)\n        // Fill joint_state_* members with read values\n        double wheel_angles[2];\n        double wheel_angle_deltas[2];\n        for (std::size_t i = 0; i &lt; num_joints_; ++i)\n        {\n            wheel_angles[i] = ticksToAngle(encoder_ticks_[i]);\n            //double wheel_angle_normalized = normalizeAngle(wheel_angle);\n            wheel_angle_deltas[i] = wheel_angles[i] - joint_positions_[i];\n\n            joint_positions_[i] += wheel_angle_deltas[i];\n            joint_velocities_[i] = wheel_angle_deltas[i] / period.toSec();\n            joint_efforts_[i] = 0.0; // unused with diff_drive_controller\n        }\n    }\n\n    // The write method is part of the control loop cycle (read, update, write) and is used to \n    // send out commands to the robot's hardware resources (joints, actuators). \n    // This method should be called after read and controller_manager::ControllerManager::update.\n    void DiffBotHWInterface::write(const ros::Time&amp; time, const ros::Duration&amp; period)\n    {\n        ros::Duration elapsed_time = period;\n        // Write to robot hw\n        // joint velocity commands from ros_control's RobotHW are in rad/s\n        // Convert the velocity command to a percentage value for the motor\n        // This maps the velocity to a percentage value which is used to apply\n        // a percentage of the highest possible battery voltage to each motor.\n        std_msgs::Int32 left_motor;\n        std_msgs::Int32 right_motor;\n\n        double output_left = pids_[0](joint_velocities_[0], joint_velocity_commands_[0], period);\n        double output_right = pids_[1](joint_velocities_[1], joint_velocity_commands_[1], period);\n\n        left_motor.data = output_left / max_velocity_ * 100.0;\n        right_motor.data = output_right / max_velocity_ * 100.0;\n\n    // Publish the PID computed motor commands to the left and right motors\n    pub_left_motor_value_.publish(left_motor);\n        pub_right_motor_value_.publish(right_motor);\n    }\n\n    // Process updates from encoders using a subscriber\n    void DiffBotHWInterface::leftEncoderTicksCallback(const std_msgs::Int32::ConstPtr&amp; msg)\n    {\n        encoder_ticks_[0] = msg-&gt;data;\n        ROS_DEBUG_STREAM_THROTTLE(1, \"Left encoder ticks: \" &lt;&lt; msg-&gt;data);\n    }\n\n    void DiffBotHWInterface::rightEncoderTicksCallback(const std_msgs::Int32::ConstPtr&amp; msg)\n    {\n        encoder_ticks_[1] = msg-&gt;data;\n        ROS_DEBUG_STREAM_THROTTLE(1, \"Right encoder ticks: \" &lt;&lt; msg-&gt;data);\n    }\n\n    double DiffBotHWInterface::ticksToAngle(const int &amp;ticks) const\n    {\n        // Convert number of encoder ticks to angle in radians\n        double angle = (double)ticks * (2.0*M_PI / 542.0);\n        ROS_DEBUG_STREAM_THROTTLE(1, ticks &lt;&lt; \" ticks correspond to an angle of \" &lt;&lt; angle);\n    return angle;\n    }\n\n};\n</code></pre> <p>The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot,  and to command it. When the controller manager runs, the controllers will read from the <code>pos</code>, <code>vel</code> and <code>eff</code> variables of the custom robot hardware interface, and the controller will write the desired command into the <code>cmd</code> variable. It's mandatory to make sure the <code>pos</code>, <code>vel</code> and <code>eff</code> variables always have the latest joint state available, and to make sure that whatever is written into the <code>cmd</code> variable gets executed by the robot. This can be done by implementing <code>hardware_interface::RobotHW::read()</code> and a <code>hardware_interface::RobotHW::write()</code> methods.</p> <p>The <code>write()</code> method also contains the output interface to the motor driver. In this case it is publishing <code>/diffbot/motor_left</code> and <code>/diffbot/motor_right</code> topics, which are subscribed by the grove_i2c motor_driver python node that is running on the SBC.</p> <p>The main node that will be executed uses the <code>controller_manager</code> to operate the so called control loop.  In the case of  DiffBot a simple example looks like the following,  refer to the <code>diffbot_base.cpp</code> for the complete implementation:</p> <pre><code>#include &lt;ros/ros.h&gt;\n#include &lt;diffbot_base/diffbot_hw_interface.h&gt;\n#include &lt;controller_manager/controller_manager.h&gt;\n\nint main(int argc, char **argv)\n{\n    // Initialize the ROS node\n    ros::init(argc, argv, \"diffbot_hw_interface\");\n    ros::NodeHandle nh;\n\n    // Create an instance of your robot so that this instance knows about all \n    // the resources that are available.\n    diffbot_base::DiffBotHWInterface diffBot(nh);\n\n    // Create an instance of the controller manager and pass it the robot, \n    // so that it can handle its resources.\n    controller_manager::ControllerManager cm(&amp;diffBot);\n\n    // Setup a separate thread that will be used to service ROS callbacks.\n    // NOTE: We run the ROS loop in a separate thread as external calls such\n    // as service callbacks to load controllers can block the (main) control loop\n    ros::AsyncSpinner spinner(1);\n    spinner.start();\n\n    // Setup for the control loop.\n    ros::Time prev_time = ros::Time::now();\n    ros::Rate rate(10.0); // 10 Hz rate\n\n    // Blocks until shutdown signal recieved\n    while (ros::ok())\n    {\n        // Basic bookkeeping to get the system time in order to compute the control period.\n        const ros::Time     time   = ros::Time::now();\n        const ros::Duration period = time - prev_time;\n\n        // Execution of the actual control loop.\n        diffBot.read(time, period);\n        // If needed, its possible to define transmissions in software by calling the \n        // transmission_interface::ActuatorToJointPositionInterface::propagate()\n        // after reading the joint states.\n        cm.update(time, period);\n        // In case of software transmissions, use \n        // transmission_interface::JointToActuatorEffortHandle::propagate()\n        // to convert from the joint space to the actuator space.\n        diffBot.write(time, period);\n\n        // All these steps keep getting repeated with the specified rate.\n        rate.sleep();\n    }\n    return 0;\n}\n</code></pre> <p>As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following:</p> <ul> <li>Read joint states from the real robot hardware</li> <li>Update the <code>diff_drive_controller</code> with read values and compute the joint velocities using the target <code>cmd_vel</code></li> <li>Write the computed values </li> </ul> <p>You may be wondering why the read values aren't returned from the <code>diffbot.read()</code> method and nothing is passed to the <code>diffbot.write()</code>. This is because the <code>RobotHW::init()</code> method, shown in the first code snippet, is used to register the actuated joint names (described in the <code>diffbot_description</code>) to the <code>joint_position</code>, <code>joint_velocity</code> and <code>joint_effort</code> member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values  without conflicting with other controllers, is the <code>hardware_interface::JointStateInterface</code>.  ROS Control uses the <code>hardware_interface::VelocityJointInterface</code> (part of the <code>joint_command_interface.h</code>)  that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators.</p> <p>When the controller manager runs, the controllers will read from the <code>joint_position</code>, <code>joint_velocity</code> and <code>joint_effort</code> variables of the custom robot hardware interface, and the controller will write the desired command into the <code>joint_velocity_command</code> variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the <code>diff_drive_controller</code>) variables always have the latest joint state available, and to make sure that whatever is written into the <code>joint_velocity_command</code> variable gets executed by the robot. As mentioned this can be done by implementing <code>hardware_interface::RobotHW::read()</code> and a <code>hardware_interface::RobotHW::write()</code> methods.</p> <p>In the control loop the overriden <code>hardware_interface::RobotHW::read()</code> method of DiffBot is used to read the joint states. The <code>diff_drive_controller</code> works with a VelocityInterface which is why the <code>joint_position</code>, defined in rad, and <code>joint_velocity</code>, defined in rad/s, are calculated from the encoder ticks.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#pid-controller","title":"PID Controller","text":"<p>Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders  and the target velocity computed by the <code>diff_drive_controller</code> for a specific wheel joint.  The <code>diff_drive_controller</code> doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the <code>diff_drive_controller</code> just uses the <code>twist_msg</code> on the <code>cmd_vel</code> topic for example from the <code>rqt_robot_steering</code> and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account.  See the code of <code>diff_drive_controller</code> where the <code>joint_command_velocity</code> is calculated. </p> <p>This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so:</p> <p>The PID used here inherits from the ROS Control <code>control_toolbox::Pid</code> that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a <code>cfg</code> folder. </p> <p>For more details on ROS dynamic reconfigure see the official tutorials.</p> <p>With the use of the PID controller the robot is able to drive straight:</p> <p>In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. </p> <p>Using six DG01D-E motors the following values were recorded (sorted by increasing voltage):</p> Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 <p>In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. </p> <p>To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above.</p> <p>Note</p> <p>Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel.</p> <p>A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line.  Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape.  Measuring a value below 10 cm is considered precise for these motors.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#cmakeliststxt","title":"CMakeLists.txt","text":"<p>The <code>diffbot_hw_interface</code> target library of the <code>diffbot_base</code> package depends on the custom <code>diffbot_msgs</code>. To have them built first, add the following to the <code>CMakeLists.txt</code> of the <code>diffbot_base</code> package:</p> <pre><code>add_dependencies(diffbot_hw_interface diffbot_msgs_generate_messages_cpp)\n</code></pre> <p>Note</p> <p>This makes sure message headers of this package are generated before being used.  If you use messages from other packages inside your catkin workspace,  you need to add dependencies to their respective generation targets as well, because catkin builds all projects in parallel.<sup>1</sup></p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#launch-file","title":"Launch File","text":"<p>To run a single controller_manager, the one from the <code>diffbot_base</code> package defined inside <code>difbot_base.cpp</code> use the  launch file from <code>diffbot_base/launch/diffbot.launch</code>:</p> <pre><code>&lt;!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --&gt;\n&lt;launch&gt;\n    &lt;!-- Load DiffBot model --&gt;\n    &lt;param name=\"robot_description\"\n       command=\"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\"/&gt;\n\n    &lt;node name=\"diffbot_base\" pkg=\"diffbot_base\" type=\"diffbot_base\"/&gt;\n\n    &lt;!-- Load controller config to the parameter server --&gt;\n    &lt;rosparam command=\"load\" \n              file=\"$(find diffbot_control)/config/diffbot_control.yaml\"/&gt;\n\n    &lt;!-- Load base config to the parameter server --&gt;\n    &lt;rosparam command=\"load\" \n              file=\"$(find diffbot_base)/config/base.yaml\"/&gt;\n\n    &lt;!-- Load the controllers --&gt;\n    &lt;node name=\"controller_spawner\" pkg=\"controller_manager\" type=\"spawner\" respawn=\"false\"\n        output=\"screen\" ns=\"diffbot\" args=\"joint_state_controller\n                                            mobile_base_controller\"/&gt;\n&lt;/launch&gt;\n</code></pre> <p>This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node <code>diffbot_base</code>. It creates the hardware interface and instantiates a new controller manager in the <code>diffbot_base.cpp</code>. Finally the <code>spawner</code> from the <code>controller_manager</code> package is used to initialize and start the controllers defined in the <code>diffbot_control/config/diffbot_control.yaml</code>. This step of the launch file is required to get the controllers initialized and started. Another way would be to use <code>controller_manager::ControllerManager::loadControllers()</code> inside the <code>diffbot_base.cpp</code>.</p> <p>Additionally the launch file loads additional parameters, stored in the <code>diffbot_base/config/base.yaml</code> on the parameter server. These parameters are hardware related and used to tune the driving behaviour:</p> <pre><code># Hardware related parameters\n# will be loaded onto the parameter server\n# See the diffbot.launch\ndiffbot:\n  encoder_resolution: 542\n  gain: 1.0\n  trim: 0.0\n  motor_constant: 27.0\n  pwm_limit: 1.0\n  debug:\n    hardware_interface: false\n    base_controller: true\n</code></pre> <p>After launching this launch file on DiffBot's single board computer (e.g. Raspberry Pi or Jetson Nano) with </p> <pre><code>roslaunch diffbot_base diffbot.launch\n</code></pre> <p>the following parameters are stored on the parameter server:</p> <pre><code>$ rosparam list\n/diffbot/hardware_interface/joints\n/diffbot/joint_state_controller/extra_joints\n/diffbot/joint_state_controller/publish_rate\n/diffbot/joint_state_controller/type\n/diffbot/mobile_base_controller/base_frame_id\n/diffbot/mobile_base_controller/left_wheel\n/diffbot/mobile_base_controller/pose_covariance_diagonal\n/diffbot/mobile_base_controller/publish_rate\n/diffbot/mobile_base_controller/right_wheel\n/diffbot/mobile_base_controller/twist_covariance_diagonal\n/diffbot/mobile_base_controller/type\n/diffbot/mobile_base_controller/wheel_radius\n/diffbot/mobile_base_controller/wheel_separation\n/robot_description\n/rosdistro\n/roslaunch/uris/host_tensorbook__46157\n/roslaunch/uris/host_ubuntu__33729\n/rosversion\n/run_id\n</code></pre>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#additional-requirements","title":"Additional Requirements","text":"<p>Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the <code>diffbot_bringup</code> package for this setup.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#simulation","title":"Simulation","text":"<p>To have a simulation showing DiffBot, the second step is to use the <code>diffbot_gazebo/launch/diffbot_base.launch</code> on the work pc:</p> <pre><code>$ roslaunch diffbot_gazebo diffbot_base.launch\n</code></pre> <p>This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too:</p> ROS Control with Gazebo Overview. <p>After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the <code>gazebo_ros_control</code> plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the <code>diffbot_base/launch/controllers.launch</code> should be used. This launch file is just loading and starting all controllers again. Note that using the <code>spawner</code> from the <code>controller_manager</code> package, like in the <code>diffbot_base/launch/diffbot.launch</code> results in an error. (TODO this needs some more testing).</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/high-level/#references","title":"References","text":"<ol> <li> <p>ROS Tutorials: Writing Publisher Subscriber, Building your nodes \u21a9</p> </li> </ol>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/low-level/","title":"DiffBot Base Package - Low Level Approach","text":"<p>Having explained the two components of the base controller in DiffBot Base, the low-level firmware is implemented first, followed by the high-level hardware, detailed in the next section.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/low-level/#implementing-the-low-level-base-controller-for-remo","title":"Implementing the low-level base controller for Remo","text":"<p>The low-level base controller is implemented on the Teensy microcontroller using PlatformIO. The programming language in PlatformIO is the same as Arduino (based on Wiring) and it is available as a plugin for the Visual Studio Code editor. On the development PC, we can flash the robot firmware to the board with this plugin. We will get the firmware code from the <code>diffbot_base</code> ROS package, located in the <code>scripts/base_controller</code> subfolder. Opening this folder in Visual Studio Code will recognize it as a PlatformIO workspace because it contains the <code>platformio.ini</code> file. This file defines the required dependencies and makes it straightforward to flash the firmware to the Teensy board after compilation. Inside this file, the used libraries are listed:</p> <pre><code>lib_deps = frankjoshua/Rosserial Arduino Library@^0.9.1\n           adafruit/Adafruit Motor Shield V2 Library@^1.0.11\n           Wire\n</code></pre> <p>As you can see, the firmware depends on <code>rosserial</code>, the <code>Adafruit Motor Shield V2</code> library, and <code>Wire</code>, an I2C library. PlatformIO allows using custom libraries defined in the local <code>./lib</code> folder, which are developed in this section.</p> <p>The firmware is used to read from encoders and ultrasonic and IMU sensors, and receive wheel velocity commands from the high-level <code>hardware_interface::RobotHW</code> class, discussed in the next section. The following code snippets are part of the low-level base controller\u2019s <code>main.cpp</code> file and show the used libraries, found in <code>diffbot_base/scripts/base_controller</code>, in the <code>lib</code> and <code>src</code> folders. <code>src</code> contains <code>main.cpp</code> consisting of the <code>setup()</code> and <code>loop()</code> functions, common to every Arduino sketch and starts off by including the following headers:</p> <pre><code>#include &lt;ros.h&gt;\n#include \"diffbot_base_config.h\"\n</code></pre> <p>Besides the ros header file, it includes definitions specific to Remo, which are defined in the <code>diffbot_base_config.h</code> header. It contains constant parameter values such as the following:</p> <ul> <li>Encoder pins: Defines to which pins on the Teensy microcontroller the Hall   effect sensors are connected.</li> <li>Motor I2C address and pins: The Adafruit motor driver can drive four DC   motors. Due to cable management, motor terminals <code>M3</code> and <code>M4</code> are used for   the left and right motors, respectively.</li> <li>PID: The tuned constants for both PID controllers of <code>base_controller</code>.</li> <li>PWM_MAX and PWM_MIN: The minimum and maximum possible PWM values that can be   sent to the motor driver.</li> <li>Update rates: Defines how often functions of <code>base_controller</code> are executed.   For example, the control portion of the low-level base controller code reads   encoder values and writes motor commands at a specific rate.</li> </ul> <p>After including Remo-specific definitions, next follows the custom libraries in the <code>lib</code> folder:</p> <pre><code>```cpp\n#include \"base_controller.h\"\n#include \"adafruit_feather_wing/adafruit_feather_wing.h\"\n```\n</code></pre> <p>These include directives and the libraries that get included with them are introduced next:</p> <ul> <li><code>base_controller</code>: Defines the <code>BaseController</code> template class, defined in the   <code>base_controller.h</code> header, and acts as the main class to manage the two   motors, including each motor\u2019s encoder, and communicate with the high-level   hardware interface.</li> <li><code>motor_controller_intf</code>: This library is indirectly included with   <code>adafruit_feather_wing.h</code> and defines an abstract base class, named   <code>MotorControllerIntf</code>. It is a generic interface used to operate a single   motor using arbitrary motor drivers. It is meant to be implemented by other   specific motor controller subclasses and therefore avoids changing code in   classes that know the <code>MotorControllerIntf</code> interface and call its   <code>setSpeed(int value)</code> method, such as done by <code>BaseController</code>. The only   requirement for this to work is for a subclass to inherit from this   <code>MotorControllerIntf</code> interface and implement the <code>setSpeed(int value)</code> class   method.</li> <li><code>adafruit_feather_wing</code>: This library, in the <code>motor_controllers</code> folder,   implements the <code>MotorControllerIntf</code> abstract interface class and defines a   concrete motor controller. For Remo, the motor controller is defined in the   <code>AdafruitMotorController</code> class. This class has access to the motor driver   board and serves to operate the speed of a single motor, which is why two   instances are created in the <code>main.cpp</code> file.</li> <li> <p><code>encoder</code>: This library is used in the <code>BaseController</code> class and is based on   <code>Encoder.h</code> from https://www.pjrc.com/teensy/td_libs_Encoder.html that allows   reading encoder tick counts from quadrature encoders, like the DG01D-E motors   consist of. The encoder library also provides a method <code>jointState()</code> to   directly obtain the joint state, which is returned by this method in the   <code>JointState</code> struct, that consists of the measured angular position (rad) and   angular velocity (rad/s) of the wheel joints:</p> <pre><code>diffbot::JointState diffbot::Encoder::jointState() {\n    long encoder_ticks = encoder.read();\n    ros::Time current_time = nh_.now();\n    ros::Duration dt = current_time - prev_update_time_;\n    double dts = dt.toSec();\n    double delta_ticks = encoder_ticks - prev_encoder_ticks_;\n    double delta_angle = ticksToAngle(delta_ticks);\n    joint_state_.angular_position_ += delta_angle;\n    joint_state_.angular_velocity_ = delta_angle / dts;\n    prev_update_time_ = current_time;\n    prev_encoder_ticks_ = encoder_ticks;\n    return joint_state_;\n}\n</code></pre> </li> <li> <p><code>pid</code>: Defines a PID controller to compute PWM signals based on the velocity   error between measured and commanded angular wheel joint velocities. For more   infos about PIDs and the tuning sections refer to PID Controllers</p> </li> </ul> <p>With these libraries, we look at the <code>main.cpp</code> file. Inside it exists only a few global variables to keep the code organized and make it possible to test the individual components that get included. The main code is explained next:</p> <ol> <li> <p>First, we define the global ROS node handle, which is referenced in other    classes, such as BaseController, where it is needed to publish, subscribe, or    get the current time, using <code>ros::NodeHandle::now()</code>, to keep track of the    update rates:</p> <pre><code>ros::NodeHandle nh;\n</code></pre> </li> <li> <p>For convenience and to keep the code organized, we declare that we want to    use the <code>diffbot</code> namespace, where the libraries of the base controller are    declared:</p> <pre><code>using namespace diffbot;\n</code></pre> </li> <li> <p>Next, we define two concrete motor controllers of type    <code>AdafruitMotorController</code> found in the <code>motor_controllers</code> library:</p> <pre><code>AdafruitMotorController motor_controller_right = AdafruitMotorController(3);\nAdafruitMotorController motor_controller_left = AdafruitMotorController(4);\n</code></pre> <p>This class inherits from the abstract base class <code>MotorControllerIntf</code>, explained above. It knows how to connect to the Adafruit motor driver using its open source <code>Adafruit_MotorShield</code> library (https://learn.adafruit.com/adafruit-stepper-dc-motor-featherwing/library-reference) and how to get a C++ pointer to one of its DC motors (<code>getMotor(motor_num)</code>). Depending on the integer input value to <code>AdafruitMotorController::setSpeed(int value)</code>, the DC motor is commanded to rotate in a certain direction and at a specified speed. For Remo, the range is between \u2013255 and 255, specified by the <code>PWM_MAX</code> and <code>PWM_MIN</code> identifiers.</p> </li> <li> <p>The next class that is defined globally inside main is <code>BaseController</code>,    which incorporates most of the main logic of this low-level base controller:</p> <pre><code>BaseController&lt;AdafruitMotorController, Adafruit_MotorShield&gt; base_controller(nh, &amp;motor_controller_left, &amp;motor_controller_right);\n</code></pre> <p>As you can see, it is a template class that accepts different kinds of motor controllers (<code>TMotorController</code>, which equals <code>AdafruitMotorController</code> in the case of Remo) that operate on different motor drivers (<code>TMotorDriver</code>, which equals <code>Adafruit_MotorShield</code>), using the <code>MotorControllerIntf</code> interface as explained previously. The <code>BaseController</code> constructor takes a reference to the globally defined ROS node handle and the two motor controllers to let it set the commanded speeds computed through two separate PID controllers, one for each wheel.</p> <p>In addition to setting up pointers to the motor controllers, the <code>BaseController</code> class initializes two instances of type <code>diffbot::Encoder</code>. Its measured joint state, returned from <code>diffbot::Encoder::jointState()</code>, is used together with the commanded wheel joint velocities in the <code>diffbot::PID</code> controllers to compute the velocity error and output an appropriate PWM signal for the motors.</p> </li> </ol> <p>After defining the global instances, the firmware\u2019s <code>setup()</code> function is discussed next. The low-level BaseController class communicates with the high-level interface <code>DiffBotHWInterface</code> using ROS publishers and subscribers. These are set up in the <code>Basecontroller::setup()</code> method, which is called in the <code>setup()</code> function of <code>main.cpp</code>. In addition to that, the <code>BaseController::init()</code> method is here to read parameters stored on the ROS parameter server, such as the wheel radius and distance between the wheels. Beside initializing <code>BaseController</code>, the communication frequency of the motor driver is configured:</p> <pre><code>void setup() {\n  base_controller.setup();\n  base_controller.init();\n  motor_controller_left.begin();\n  motor_controller_right.begin();\n}\n</code></pre> <p>The <code>begin(uint16_t freq)</code> method of the motor controllers has to be called explicitly in the main <code>setup()</code> function because <code>MotorControllerIntf</code> doesn't provide a <code>begin()</code> or <code>setup()</code> method in its interface. This is a design choice that, when added, would make the <code>MotorControllerIntf</code> less generic.</p> <p>After the <code>setup()</code> function follows the <code>loop()</code> function, to read from sensors and write to actuators, which happens at specific rates, defined in the <code>diffbot_base_config.h</code> header. The bookkeeping of when these read/write functionalities occurred is kept in the <code>BaseController</code> class inside its <code>lastUpdateRates</code> struct. Reading from the encoders and writing motor commands happens in the same code block as the control rate:</p> <pre><code>void loop() {\nros::Duration command_dt = nh.now() - base_controller.lastUpdateTime().control;\nif (command_dt.toSec() &gt;= ros::Duration(1.0 / base_controller.publishRate().control_, 0).toSec()) {\n  base_controller.read();\n  base_controller.write();\n  base_controller.lastUpdateTime().control = nh.now();\n}\n</code></pre> <p>The following steps in this code block happen continuously at the control rate:</p> <ol> <li>Encoder sensor values are read through the <code>BaseController::read()</code> method    and the data is published inside this method for the high-level    <code>DiffbotHWInterface</code> class, on the <code>measured_joint_states</code> topic of message    type <code>sensor_msgs::JointState</code>.</li> <li>The <code>BaseController</code> class subscribes to <code>DiffBotHWInterface</code> from which it    receives the commanded wheel joint velocities (topic <code>wheel_cmd_velocities</code>,    type <code>diffbot_msgs::WheelsCmdStamped</code>) inside the    <code>BaseController::commandCallback(const diffbot_msgs::WheelsCmdStamped&amp;)</code>    callback method. In <code>BaseController::read()</code>, the PID is called to compute    the motor PWM signals from the velocity error and the motor speeds are set    with the two motor controllers.</li> <li>To keep calling this method at the desired control rate, the    <code>lastUpdateTime().control</code> variable is updated with the current time.</li> </ol> <p>After the control loop update block, if an IMU is used, its data could be read at the imu rate and published for a node that fuses the data with the encoder odometry to obtain more precise odometry. Finally, in the main <code>loop()</code>, all the callbacks waiting in the ROS callback queue are processed with a call to <code>nh.spinOnce()</code>.</p> <p>MThis describes the low-level base controller. For more details and the complete library code, please refer to the <code>diffbot_base/scripts/base_controller</code> package. </p> <p>In the following section, the <code>diffbot::DiffBotHWInterface</code> class is described.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/low-level/#ros-control-high-level-hardware-interface-for-a-differential-drive-robot","title":"ROS Control high-level hardware interface for a differential drive robot","text":"<p>The ros_control meta package contains the hardware interface class <code>hardware_interface::RobotHW</code>, which needs to be implemented to leverage many available controllers from the <code>ros_controllers</code> meta package. First, we\u2019ll look at the <code>diffbot_base</code> node that instantiates and uses the hardware interface:</p> <ol> <li> <p>The <code>diffbot_base</code> node includes the <code>diffbot_hw_interface.h</code> header, as well    as the <code>controller_manager</code>, defined in <code>controller_manager.h</code>, to create the    control loop (read, update, write):</p> <pre><code>#include &lt;ros/ros.h&gt;\n#include &lt;diffbot_base/diffbot_hw_interface.h&gt;\n#include &lt;controller_manager/controller_manager.h&gt;\n</code></pre> </li> <li> <p>Inside the main function of this <code>diffbot_base</code> node, we define the ROS node    handle, the hardware interface (<code>diffbot_base::DiffBotHWInterface</code>), and pass    it to the <code>controller_manager</code>, so that it has access to its resources:</p> <pre><code>ros::NodeHandle nh;\ndiffbot_base::DiffBotHWInterface diffBot(nh);\ncontroller_manager::ControllerManager cm(&amp;diffBot);\n</code></pre> </li> <li> <p>Next, set up a separate thread that will be used to service ROS callbacks.    This runs the ROS loop in a separate thread as service callbacks can block    the control loop:</p> <pre><code>ros::AsyncSpinner spinner(1);\nspinner.start();\n</code></pre> </li> <li> <p>Then define at which rate the control loop of the high-level hardware    interface should run. For Remo, we choose 10 Hz:</p> <pre><code>ros::Time prev_time = ros::Time::now();\nros::Rate rate(10.0); rate.sleep(); // 10 Hz rate\n</code></pre> </li> <li> <p>Inside the blocking while loop of the <code>diffbot_base</code> node, we do basic    bookkeeping to get the system time to compute the control period:</p> <pre><code>while (ros::ok()) {\n  const ros::Time time = ros::Time::now();\n  const ros::Duration period = time - prev_time;\n  prev_time = time;\n  ...\n</code></pre> </li> <li> <p>Next, we execute the control loop steps: read, update, and write. The    <code>read()</code> method is here to get sensor values, while <code>write()</code> writes commands    that were computed by <code>diff_drive_controller</code> during the <code>update()</code> step:</p> <pre><code>  ...\n  diffBot.read(time, period);\n  cm.update(time, period);\n  diffBot.write(time, period);\n  ...\n</code></pre> </li> <li> <p>These steps keep getting repeated with the specified rate using    <code>rate.sleep()</code>.</p> </li> </ol> <p>After having defined the code that runs the main control loop of the <code>diffbot_base</code> node, we\u2019ll take a look at the implementation of <code>diffbot::DiffBotHWInterface</code>, which is a child class of <code>hardware_interface::RobotHW</code>. With it, we register the hardware and implement the <code>read()</code> and <code>write()</code> methods, used above in the control loop.</p> <p>The constructor of the <code>diffbot::DiffBotHWInterface</code> class is used to get parameters from the parameter server, such as the <code>diff_drive_controller</code> configuration from the <code>diffbot_control</code> package. Inside the constructor, the wheel command publisher and measured joint state subscriber are initialized. Another publisher is <code>pub_reset_encoders_</code>, which is used in the <code>isReceivingMeasuredJointStates</code> method to reset the encoder ticks to zero after receiving measured joint states from the low-level base controller.</p> <p>After constructing <code>DiffBotHWInterface</code>, we create instances of <code>JointStateHandles</code> classes (used only for reading) and <code>JointHandle</code> classes (used for read, and write access) for each controllable joint and register them with the <code>JointStateInterface</code> and <code>VelocityJointInterface</code> interfaces, respectively. This enables the <code>controller_manager</code> to manage access for joint resources of multiple controllers. Remo uses <code>DiffDriveController</code> and <code>JointStateController</code>:</p> <pre><code>for (unsigned int i = 0; i &lt; num_joints_; i++) {\n    hardware_interface::JointStateHandle joint_state_handle(joint_names_[i], &amp;joint_positions_[i], &amp;joint_velocities_[i], &amp;joint_efforts_[i]);\n    joint_state_interface_.registerHandle(joint_state_handle);\n\n    hardware_interface::JointHandle joint_handle(joint_state_handle, &amp;joint_velocity_commands_[i]);\n    velocity_joint_interface_.registerHandle(joint_handle); \n}\n</code></pre> <p>The last step that is needed to initialize the hardware resources is to register the <code>JointStateInterface</code> and the <code>VelocityJointInterface</code> interfaces with the robot hardware interface itself, thereby grouping the interfaces together to represent Remo robot in software:</p> <pre><code>registerInterface(&amp;joint_state_interface_);\nregisterInterface(&amp;velocity_joint_interface_);\n</code></pre> <p>Now that the hardware joint resources are registered and the controller manager knows about them, it\u2019s possible to call the <code>read()</code> and <code>write()</code> methods of the hardware interface. The controller manager update happens in between the read and write steps. Remo subscribes to the <code>measured_joint_states</code> topic, published by the low-level base controller. The received messages on this topic are stored in the <code>measured_joint_states_ array</code> of type <code>diffbot_base::JointState</code> using the <code>measuredJointStateCallback</code> method, and are relevant in the <code>read()</code> method:</p> <ol> <li> <p>The <code>read()</code> method is here to update the measured joint values with the current sensor readings from the encoders \u2013 angular positions (rad) and velocities (rad/s):</p> <pre><code>void DiffBotHWInterface::read() {\n    for (std::size_t i = 0; i &lt; num_joints_; ++i) {\n    joint_positions[i] = measured_joint_states[i].angular_position;\n    joint_velocity[i] = measured_joint_states[i].angular_velocity;\n}\n</code></pre> </li> <li> <p>The final step of the control loop is to call the <code>write()</code> method of the <code>DiffBotHWInterface</code> class to publish the angular wheel velocity commands of each joint, computed by <code>diff_drive_controller</code>:</p> <pre><code>void DiffBotHWInterface::write() {\n    diffbot_msgs::WheelsCmdStamped wheel_cmd_msg;\n    for (int i = 0; i &lt; NUM_JOINTS; ++i) {\n        wheel_cmd_msg.wheels_cmd.angular_velocities.joint.push_back(joint_velocity_commands_[i]); \n    }\n    pub_wheel_cmd_velocities_.publish(wheel_cmd_msg); \n}\n</code></pre> </li> </ol> <p>In this method, it would be possible to correct for steering offsets due to model imperfections and slight differences in the wheel radii. See gain / trim model.</p> <p>This concludes the important parts of the <code>DiffBotHWInterface</code> class and enables Remo to satisfy the requirements to work with the ROS Navigation Stack. In the next section, we\u2019ll look at how to bring up the robot hardware and how the started nodes interact with each other.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","ros-control","control","controllers","diff_drive_controller"]},{"location":"packages/diffbot_base/pid/","title":"PID Controllers","text":"<p>DiffBot uses two PID controllers, one for each motor. Depending on which software version you use the two PID controllers are located in different locations in the source code. Either the two PIDs are in the</p> <ul> <li>hardware interface: high-level approach until version 0.0.2, or</li> <li>base controller firmware: low-level approach starting from version 1.0.0</li> </ul> <p>The current approach (starting from version 1.0.0) operates the PIDs in the low-level base controller firmware.</p> <p>Regardless of the location (high or low-level), each PID is passed the error between velocity measured by the encoders and the target velocity computed by the <code>diff_drive_controller</code> for a specific wheel joint. The <code>diff_drive_controller</code> doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually applied on the real robot hardware and that the joint states are always up to date. This means that the <code>diff_drive_controller</code> just uses the <code>twist_msg</code> on the <code>cmd_vel</code> topic for example from the <code>rqt_robot_steering</code> and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account.</p> <p>Note</p> <p>See the code of <code>diff_drive_controller</code> where the <code>joint_command_velocity</code> is calculated. </p> <p>For this reason a PID controller can help to avoid situations such as the following where the robot moves not straigth although it's commanded to do so:</p> <p>Note</p> <p>The video runs version 0.0.2, operating the PIDs in the high-level hardware interface.</p> <p>The PIDs used in the video inherits from the ROS Control <code>control_toolbox::Pid</code> that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain F can help to reach the setpoint faster. In the high-level approach this feed forward gain is present as a dynamic reconfigure parameter, defined in the in the <code>pid.yml</code> configuration file in the <code>cfg</code> folder of this package.</p> <p>For more details on ROS dynamic reconfigure see the official tutorials.</p> <p>With the use of the PID controller the robot is able to drive straight:</p> <p>In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. </p> <p>Using six DG01D-E motors the following values were recorded (sorted by increasing voltage):</p> Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 <p>Note</p> <p>In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly.</p> <p>To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above.</p> <p>Note</p> <p>Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel.</p> <p>A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors.</p> <p>The video shows the real DiffBot robot as well as running the <code>gazebo_ros_control</code> plugin with <code>diff_drive_controller</code> from ROS Control. The robot is commanded to drive straight on a 2 meter test track. Five runs show different offsets after 2 meter, which can be cause by:</p> <ul> <li>Orientation of the start pose</li> <li>Inaccurate motors start to spin at different voltage levels</li> <li>Low encoder resolution (geared output resolution at wheel)</li> <li>Wheel slip or alignment of the left/right and caster wheel</li> <li>Weight distribution</li> <li>Manufacturing tolerances</li> <li>Deformable material of the wheels generating different surfaces of contact with the ground depending on their assembly</li> </ul>"},{"location":"packages/diffbot_base/pid/#gain-trim-model","title":"Gain / Trim Model","text":"<p>In case the robot constantly drives towards one side (e.g., to the left), this issue might be overcome by tuning each PID individually. An alternative solution can be gain and trim parameters, as implemented in the <code>write()</code> method of the high-level hardware interface.</p> <pre><code>double motor_constant_right_inv = (gain_ + trim_) / motor_constant_;\ndouble motor_constant_left_inv = (gain_ - trim_) / motor_constant_;\n\njoint_velocity_commands_[0] = joint_velocity_commands_[0] * motor_constant_left_inv;\njoint_velocity_commands_[1] = joint_velocity_commands_[1] * motor_constant_right_inv;\n</code></pre> <p>The idea is that the trim coefficient \\(t\\) is a \"quick and dirty\" way to account for various nuisances of the physical robot, including differences in motors, wheels, and the asymmetry of the robot. It's simple and easily interpretable, too.</p> <p>The two PIDs work to ensure that the wheels spin at the desired speed, but not that the robot drives the desired way though. There are other factors such as slightly different wheel raddi \\(R\\) which lead to different rotational velocities \\(\\dot{\\phi}_{l/r}\\) (translational velocity \\(v_{l/r} = \\dot{\\phi}_{l/r} \\cdot R\\)) and therefore different traveled distances for each wheel (left and right) \\(d_{l/r}\\):</p> \\[ \\begin{aligned} d_{l/r} &amp;= v_{l/r} \\cdot \\Delta t \\\\ d_{l/r} &amp;= \\frac{\\dot{\\phi}_{l/r}}{R} \\cdot \\Delta t \\end{aligned} \\] <p>These mechanical variations \"can't be seen\" by the encoders alone. For example assume you want the robot to go straight, you would set the two desired rotational wheel velocities \\(\\dot{\\phi}_{l/r}\\)'s to be equal - but still the robot would go left if, e.g., \\(R_r &gt; R_l\\), because, for the same wheel rotation (ticks), the right wheel has traveled a bigger distance.</p> <p>So the question is: what is the control objective?</p> <p>When the control objective is to follow a desired reference position (angular or lateral), it is more meaningful to measure that as error driving the PID controller. But nothing stops us from having several PID controllers in cascade sequence. One for the, e.g., lateral position, that computes the desired \\((v_0, \\omega)\\) of the robot, and two more in parallel for the wheel velocities. Still, there would be a trim coefficient to compensate for differences in traveled distances of the two wheels.</p> <p>\\(d_{l/r}\\) are the distances traveled by the left and right wheels, respectively, in a \\(\\Delta t\\):</p> \\[ d_{l/r} = R_{l/r} \\cdot \\Delta \\phi_{l/r} \\] <p>where \\(\\Delta \\phi_{l/r}\\) are the rotations of each wheel (more accurately, of each motor), and \\(R_{l/r}\\) is the radius of each wheel. If the radii are exactly the same, then \\(R_l = R_r = R\\) and the approach using only two PID controllers would work because controlling \\(\\dot{\\phi_{l/r}} \\approx \\frac{\\Delta \\phi_{l/r,k}}{\\Delta t_k}\\) is equivalent to controlling \\(\\Delta \\phi_{l/r,k}\\), as the time interval we can assume known.</p> <p>In practice, \\(R_l = R_r = R\\) does not hold because of manufacturing tolerances, deformable material of the wheels generating different surfaces of contact with the ground depending on their assembly and robot mass distribution, etc.</p> <p>The trim parameter \\(t\\) we define as something that adjusts the relationship between the commands sent to the motors (imagine voltage \\(V\\)) and the angular speed of each wheel (\\(\\dot{\\phi}\\)):</p> \\[ \\begin{aligned} V_l = k(1-t) \\dot \\phi_l \\\\ V_r = k(1+t) \\dot \\phi_r \\end{aligned} \\] <p>Note that it's the same value of \\(t\\) for the two motors (\\(k\\), just assume to be a given constant). The way we tweak it is to find the value such that the robot goes straight, that is, \\(d_l = d_r\\) (over some distance), when we command it to go straight, that is, we send the same voltages to the two wheels (\\(V_l = V_r\\)).</p> \\[ \\begin{aligned} V_l &amp;= V_r \\\\ k(1-t) \\dot\\phi_l &amp;= k(1+t) \\dot\\phi_r \\end{aligned} \\] <p>From the two equations (\\(V_l\\), \\(V_r\\)), by imposing the assumptions above and using the definition of \\(d_{l/r}\\), we can derive \\(t = \\frac{R_r - R_l}{R_r + R_l}\\), which basically shows how the trim is there to (empirically) compensate for the difference in the wheel radii.</p> <p>source</p> <p>The trim / gain approach was customized for Duckiebots. The broader context is system identification: a good resource for further reading could be Russ Tedrake's underactuated robotics course.</p> <p>After the robot can drive on the straight line without deviating from it too far the final error can be overcome with addtional sensors, such as IMUs, cameras and LiDARs, and then fuse those sensor data to follow a goal location more accurately by constantly adjusting wheel velocities. An even simpler example are IR sensors to follow the line and adjust the wheel velocities according to stay near the line. </p>"},{"location":"packages/diffbot_base/pid/#pid-tuning","title":"PID Tuning","text":"<p>To tune the PID controller, the following video provides a helpful overview of the process. It gives some insights which behavior each PID parameter influences for a vehicle trying to follow a lane, thereby reducing the cross track error. However, the same process can be followed to reduce the error \\(e(t)\\) being the difference between the desired (target) angular wheel velocity \\(\\dot \\phi_{desired,l/r}\\) and the one measured \\(\\dot \\phi_{measured,l/r}\\) by the encoders:</p> \\[ e_{l/r} = \\dot{\\phi}_{desired,l/r} - \\dot{\\phi}_{measured,l/r} \\] <p>In case of oscillations (wheel spin direction keeps changing), might happen because of a too high proportional gain \\(K_P\\) or a too low derivative gain \\(K_D\\). Changing gains should be done one at a time because all are related by influencing the overall calculated output command \\(u(t)\\) via the following equation</p> \\[ u(t) = K_\\text{p} e(t) + K_\\text{i} \\int_0^t e(\\tau) \\,\\mathrm{d}\\tau + K_\\text{d} \\frac{\\mathrm{d}e(t)}{\\mathrm{d}t} \\] <p>One option to start the tuning process is by setting these values:</p> <p>P = something small until the motors start to spin and no major oscillations are observable I = 0 D = 0</p> <p>To overcome oscillations increase the D gain first but usually it will be a value below P (might be one or more magnitudes lower). The derivative term tracks how fast the velocity changes over time. A higher D gain can dampen oscillations and therefore lead to more stable behavior. The D term best estimate of the future trend of the error \\(e(t)\\). If the derivative gain is too low then the system is called underdamped and will pull too quickly towards the desired velocity, which can easily result in overshooting the target value. On the other hand if the D gain is too high the system is known to be overdamped and it will take a long time to reach the desired velocity. A properlly chosen D gain will allow to reach the desired velocity quickly with reduced overshoot and therefore a near zero error rate (known as critically damped).</p> <p>Term I accounts for past values of the error and integrates them over time. This term should be adjust last and increased slightly to tune the behavior further.</p> <p>In code the values to tune these PID gains are found in the <code>base_controller_config.h</code>:</p> <pre><code>#define K_P 0.6 // P constant\n#define K_I 0.3 // I constant\n#define K_D 0.5 // D constant\n</code></pre>"},{"location":"packages/diffbot_base/pid/#related-issues-to-pids","title":"Related Issues to PIDs","text":"<ul> <li>https://github.com/ros-mobile-robots/diffbot/issues/13</li> <li>https://github.com/ros-mobile-robots/diffbot/issues/35</li> <li>Duckietown EDX MOOC Forum question</li> </ul>"},{"location":"packages/imu/driver-bno055/","title":"ROS Driver for Adafruit IMU Bosch BNO055","text":"<p>The Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is based on the BNO055 from Bosch. This is system in package solution which features a MEMS accelerometer, magnetometer and gyroscope and putting them on a single die with a high speed ARM Cortex-M0 based processor to digest all the sensor data, abstract the sensor fusion and real time requirements away, and spit out data you can use in quaternions, Euler angles or vectors.</p> <p> </p> Adafruit Bosch BNO055 IMU"},{"location":"packages/imu/driver-bno055/#high-level-driver-i2c","title":"High level Driver (I2C)","text":"<p>When connecting the IMU directly via i2c to the single board computer (SBC), which is refered to as the high level approach, we can make use of <code>ros-imu-bno055</code> ROS package.</p>"},{"location":"packages/imu/driver-bno055/#low-level-driver","title":"Low Level Driver","text":"<p>In case we connect the Bosch IMU to the low level microcontroller board (e.g. Teensy or Arduino) there is Adafruit BNO055 library which can be used.</p>"},{"location":"packages/imu/driver-bno055/#pinout","title":"Pinout","text":"<p>The following sections are found at learn.adafruit.com/adafruit-bno055-absolute-orientation-sensor/pinouts. For the robot the power pins as well as the I2C pins are important.</p>"},{"location":"packages/imu/driver-bno055/#power-pins","title":"Power Pins","text":"<ul> <li>VIN: 3.3-5.0V power supply input</li> <li>3VO: 3.3V output from the on-board linear voltage regulator, you can grab up to about 50mA as necessary</li> <li>GND: The common/GND pin for power and logic</li> </ul>"},{"location":"packages/imu/driver-bno055/#i2c-pins","title":"I2C Pins","text":"<ul> <li>SCL - I2C clock pin, connect to your microcontrollers I2C clock line. This pin can be used with 3V or 5V logic, and there's a 10K pullup on this pin.</li> <li>SDA - I2C data pin, connect to your microcontrollers I2C data line. This pin can be used with 3V or 5V logic, and there's a 10K pullup on this pin.</li> </ul>"},{"location":"packages/imu/driver-bno055/#other-pins","title":"Other Pins","text":"<ul> <li>RST: Hardware reset pin. Set this pin low then high to cause a reset on the sensor. This pin is 5V safe.</li> <li>INT: The HW interrupt output pin, which can be configured to generate an interrupt signal when certain events occur like movement detected by the accelerometer, etc. (not currently supported in the Adafruit library, but the chip and HW is capable of generating this signal). The voltage level out is 3V</li> <li>ADR: Set this pin high to change the default I2C address for the BNO055 if you need to connect two ICs on the same I2C bus. The default address is 0x28. If this pin is connected to 3V, the address will be 0x29</li> <li>PS0 and PS1: These pins can be used to change the mode of the device (it can also do HID-I2C and UART) and also are provided in case Bosch provides a firmware update at some point for the ARM Cortex M0 MCU inside the sensor. They should normally be left unconnected.</li> </ul>"},{"location":"packages/imu/driver-bno055/#wiring","title":"Wiring","text":"<p>You can easily wire this breakout to a microcontroller or single board computer (SBC).  The following sections will show both options using Teensy as a microcontroller and Raspberry Pi 4 B as SBC. For another kind of microcontroller or SBC, just make sure it has I2C capability and port the code.</p>"},{"location":"packages/imu/driver-bno055/#single-board-computer","title":"Single Board Computer","text":"<p>The following sections show how to wire and use the BNO055 with the Raspberry Pi 4 B SBC.</p>"},{"location":"packages/imu/driver-bno055/#wiring_1","title":"Wiring","text":"<p>To connect the assembled BNO055 breakout to a Raspberry Pi 4 B, follow the wiring diagram.</p> <p>TODO Fritzing wiring diagram I2C to hub or one of the I2C pins (SCL and SDA) of the Raspberry Pi  Power (Vin) to 3.3V or 5V of the Raspberry Pi GND to GND</p> I2C Pinout on Raspberry Pi 4 B. <ul> <li>Connect Vin (red wire) to the power supply, 3-5V is fine.</li> <li>Connect the SCL (yellow wire) pin to the I2C clock SCL pin on your SBC or an I2C hub that is connected to the SBC.</li> <li>Connect the SDA (blue wire) pin to the I2C data SDA pin on your SBC or an I2C hub that is connected to the SBC.</li> </ul>"},{"location":"packages/imu/driver-bno055/#software","title":"Software","text":"<p>Refer to https://learn.adafruit.com/adafruit-bno055-absolute-orientation-sensor/python-circuitpython as well as to the CircuitPython Guide</p> <ol> <li> <p>Make sure you have python3-venv installed:</p> <pre><code>sudo apt install python3.8-venv\n</code></pre> </li> <li> <p>Create a virtual environment:</p> <pre><code>$ python3 -m venv venv\n</code></pre> </li> <li> <p>Activate the venv</p> <pre><code>$ source venv/bin/activate\n(venv) fjp@remo:~/test_imu$\n</code></pre> </li> <li> <p>Install the CircuitPython driver for the BNO055:</p> <pre><code>pip install adafruit-circuitpython-bno055\n</code></pre> </li> <li> <p>Create a simple test program or copy the example programs:</p> <pre><code>$ vim imu.py\n</code></pre> <p>and copy the following content:</p> <pre><code>import adafruit_bno055\n\n# This driver takes an instantiated and active I2C object as an argument to its constructor.\n# The way to create an I2C object depends on the board you are using.\n# For boards with labeled SCL and SDA pins, you can:\nimport board\n\ni2c = board.I2C()\n\n# Once you have the I2C object, you can create the sensor object:\nsensor = adafruit_bno055.BNO055_I2C(i2c)\n\n# And then you can start reading the measurements:\nprint(sensor.temperature)\nprint(sensor.euler)\nprint(sensor.gravity)\n</code></pre> </li> <li> <p>TODO i2c permissions to execute with non-admin user or use sudo</p> </li> <li> <p>Execute the python script:</p> <pre><code>python imu.py\n</code></pre> <p>This should result in an output similar to:</p> <pre><code>(venv) root@remo:/home/fjp/test_imu# python imu.py\n29\n(8.0, 0.0, 0.0)\n(0.0, 0.0, 0.0)\n</code></pre> </li> </ol>"},{"location":"packages/imu/driver-bno055/#microcontroller","title":"Microcontroller","text":"<p>The following sections show how to wire and use the BNO055 with the Teensy microcontroller board using  the PlatformIO plugin of Visual Studio Code.</p> <p>Info</p> <p>Instructions to connect the BNO055 with an Arduino microcontroller are found at  learn.adafruit.com/adafruit-bno055-absolute-orientation-sensor/arduino-code.</p>"},{"location":"packages/imu/driver-bno055/#wiring_2","title":"Wiring","text":"<p>To connect the assembled BNO055 breakout to a Raspberry Pi 4 B, follow the wiring diagram.</p> <p>TODO Fritzing wiring diagram</p> <ul> <li>Connect Vin (red wire) to the power supply, 3-5V is fine. Use the same voltage that the microcontroller logic is based off of. For most Arduinos, that is 5V</li> <li>Connect GND (black wire) to common power/data ground</li> <li>Connect the SCL (yellow wire) pin to the I2C clock SCL pin on your microcontoller.</li> <li>Connect the SDA (blue wire) pin to the I2C data SDA pin on your microcontoller.</li> </ul>"},{"location":"packages/imu/driver-bno055/#software_1","title":"Software","text":"<p>The Adafruit_BNO055 driver supports reading raw sensor data, or you can use the Adafruit Unified Sensor system to retrieve orientation data in a standard data format.</p> <p>In the Arduino library manager you can install both, the Adafruit_BNO055 driver and the Adafruit Unified Sensor.</p> <p>TODO</p> <p>Add PlatformIO instructions</p>"},{"location":"packages/imu/robot_localization/","title":"Robot Localization","text":""},{"location":"packages/imu/robot_localization/#robot-localization-package","title":"Robot Localization Package","text":"<p>The <code>robot_localization</code> ROS package is used to fuse the odometry and intertial measurement (IMU) data. The package can fuse an abitrary number of sensors, such as GPS, multiple IMUs and odometry sensors. However, for DiffBot and Remo only one IMU and the odometry data from the two wheel encoders is used.</p> <ul> <li>ROS Wiki</li> <li>Wiki Documentation</li> <li>cra-ros-pkg/robot_localization GitHub repository</li> </ul>"},{"location":"processing_units/","title":"Processing Units","text":"<p>Our robot consists of components which are connected as shown in the block diagram below:</p> <p></p> <p>The Teensy 3.2 microcontroller board is connected to the encoder and optional IMU sensor as well as the motor driver actuator. It communicates to the Raspberry Pi 4 B via USB over the <code>rosserial</code> protocol. The motor driver and the optional IMU exchange data over I2C with the microcontroller. The RPLIDAR has a serial-to-USB converter and is therefore connected to one of the USB ports of the SBC. The motor encoder sensors are interfaced through the GPIO pins of the microcontroller. The following shows the connection diagram of the components:</p> <p></p>"},{"location":"processing_units/#overview-of-ros-nodes-and-topics-for-the-remo-robot","title":"Overview of ROS nodes and topics for the Remo robot","text":"<p>The following launch file will bring up the hardware nodes, load the robot description onto the parameter server, start <code>diff_drive_controller</code>, and begin to publish the transformations using <code>tf</code>. Run this launch file on the robot's SBC:</p> <pre><code>roslaunch diffbot_bringup bringup.launch model:=remo\n</code></pre> <p>On the development PC, you can use the teleop node to steer the robot. To do this, run the following:</p> <pre><code>roslaunch diffbot_bringup keyboard_teleop.launch\n</code></pre> <p>Issuing the rosnode list command shows the following list of started nodes:</p> <pre><code>/diffbot/controller_spawner\n/diffbot/diffbot_base\n/diffbot/robot_state_publisher\n/diffbot/rosserial_base_controller\n/diffbot_teleop_keyboard\n/rosout\n</code></pre> <p></p>"},{"location":"processing_units/git-setup/","title":"Git Setup","text":""},{"location":"processing_units/git-setup/#git-setup","title":"git setup","text":"<p>Install git on Ubuntu via the following command:</p> <pre><code>fjp@ubuntu:~$ sudo apt install git\n</code></pre> <p>Set your username and email address that you use on github (when using github to host your repository):</p> <pre><code>$ git config --global user.name \"github_username\"\n$ git config --global user.email \"your.name@provider.com\"\n</code></pre> <p>To store your password credentials when pushing and pulling to the remote repository use the following commands:</p> <pre><code>fjp@ubuntu:~/git/2wd-robot$ git config --global credential.helper store\nfjp@ubuntu:~/git/2wd-robot$ git push\nUsername for 'https://github.com': fjp\nPassword for 'https://fjp@github.com': \nEverything up-to-date\nfjp@ubuntu:~/git/2wd-robot$ git push\nEverything up-to-date\n</code></pre>"},{"location":"processing_units/hardware-interfaces/","title":"Autonomous Differential Drive Mobile Robot - Hardware Interfaces","text":"<p>The hardware interfaces provide an interface between the components (sensors and actuators) of the 2WD robot and its processing units, the Raspberry Pi 4 B (or the Nvidia Jetson Nano) and the microcontroller (in this case the Teensy 4.0).</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/hardware-interfaces/#usb","title":"USB","text":"<p>The Universial Serial Bus (USB) connections are required to connect the Single Board Computer (SBC) with the microcontroller. Using this connection, it is possible to communicate via <code>rosserial</code>.</p> <p>Another USB connector is used for the RPLidar laser scanner. </p> <p>Info</p> <p>See the section USB Devices below to setup the required permissions and allow the communication between this interface.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/hardware-interfaces/#single-board-computer-gpio","title":"Single Board Computer GPIO","text":"<p>Currently, one GPIO pin is used to connect the ultrasonic ranger.</p> <p>The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11.</p> <p>Info</p> <p>In case you are using LM393 speed sensors, instead of the encoders of the DG01D-E, the LM393 use a single digital GPIO pin each.  These pins could be directly connected to the Raspberry Pi GPIOs and setup using software interrupts with the RPi.GPIO library. Alternatively they could be also connected to the pins of the microcontroller, e.g. Teensy. For this build the </p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/hardware-interfaces/#microcontroller-digital-pins","title":"Microcontroller Digital Pins","text":"<p>Four digital pins on the Teensy microcontroller are in use for the two quadrature encoders of the DG01D-E.</p> <p>Info</p> <p>See the <code>diffbot_base</code> package for the running software script to read the encoder ticks.</p>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/hardware-interfaces/#single-board-computer-i2c-connection","title":"Single Board Computer I2C Connection","text":"<p>The I2C connections on the Raspberry Pi 4 B are used for multiple components such as the motor driver and the oled display.</p> I2C Pinout on Raspberry Pi 4 B. <p>Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. </p> <p>To do so, we will use the tool <code>i2cdetect</code> which requires that we install a tool on Ubuntu called <code>i2c-tools</code>:</p> <pre><code>fjp@remo:~$ i2cdetect\n\nCommand 'i2cdetect' not found, but can be installed with:\n\nsudo apt install i2c-tools\n\nfjp@remo:~$ sudo apt install i2c-tools\n</code></pre> <p>This <code>i2cdetect</code> tool is  a  userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates  the number or name of the I2C bus to be scanned,  and should correspond to one of the busses listed by <code>i2cdetect -l</code>. See also <code>info i2cdetect</code> for the manual page.</p> <p>To test if the i2c ports are working we use the following commands:</p> <pre><code>$ i2cdetect -y 0\nError: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory\n$ i2cdetect -y 1\nError: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory\n</code></pre> <p>The ports are not setup correctly yet, which is why we need to enable the following two lines in the <code>/boot/firmware/config.txt</code> file:</p> <pre><code>dtparam=i2c0=on\ndtparam=i2c1=on\n</code></pre> <p>After rebooting the Raspberry Pi and entering the command again the following output will appear:</p> <pre><code>$ i2cdetect -y 0\nError: Could not open file `/dev/i2c-0': Permission denied\nRun as root?\n</code></pre> <p>Running as root using <code>sudo</code> will work (please read on, there is a better way):</p> <pre><code>$ sudo i2cdetect -y 0\n[sudo] password for fjp:\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- -- \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: -- -- -- -- -- -- -- --\n</code></pre> <p>As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following:</p> <pre><code>ls -l /dev/i2c-0 \ncrw-rw---- 1 root i2c 89, 0 Apr  1  2020 /dev/i2c-0\n</code></pre> <p>we see that the <code>/dev/i2c-0</code> device belongs to user <code>root</code> and <code>i2c</code> user group.  To get access without <code>sudo</code> we can add other users, requiering access to the <code>i2c</code> group with:</p> <pre><code>sudo adduser fjp i2c\nAdding user `fjp' to group `i2c' ...\nAdding user fjp to group i2c\nDone.\n</code></pre> <p>After logging out and back in again the access will be granted and following output will come up:</p> <pre><code>$ i2cdetect -y 0\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- -- \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: -- -- -- -- -- -- -- -- \n</code></pre> <p>It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0.</p> Alternative setup using raspi-config  On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool:  <pre>fjp@ubuntu:~/git/2wd-robot$ sudo raspi-config</pre> <pre>Raspberry Pi 4 Model B Rev 1.1\n\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                                                \u2502\n\u2502      1 Change User Password Change password for the current user                               \u2502\n\u2502      2 Network Options      Configure network settings                                         \u2502\n\u2502      3 Boot Options         Configure options for start-up                                     \u2502\n\u2502      4 Localisation Options Set up language and regional settings to match your location       \u2502\n\u2502      5 Interfacing Options  Configure connections to peripherals                               \u2502\n\u2502      6 Overclock            Configure overclocking for your Pi                                 \u2502\n\u2502      7 Advanced Options     Configure advanced settings                                        \u2502\n\u2502      8 Update               Update this tool to the latest version                             \u2502\n\u2502      9 About raspi-config   Information about this configuration tool                          \u2502\n\u2502                                                                                                \u2502\n\u2502                                                                                                \u2502\n\u2502                                                                                                \u2502\n\u2502                           &lt;Select&gt;                           &lt;Finish&gt;                          \u2502\n\u2502                                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>  Select the i2c option:  <pre>\n\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                                                \u2502\n\u2502        P1 Camera      Enable/Disable connection to the Raspberry Pi Camera                     \u2502\n\u2502        P2 SSH         Enable/Disable remote command line access to your Pi using SSH           \u2502\n\u2502        P3 VNC         Enable/Disable graphical remote access to your Pi using RealVNC          \u2502\n\u2502        P4 SPI         Enable/Disable automatic loading of SPI kernel module                    \u2502\n\u2502        P5 I2C         Enable/Disable automatic loading of I2C kernel module                    \u2502\n\u2502        P6 Serial      Enable/Disable shell and kernel messages on the serial connection        \u2502\n\u2502        P7 1-Wire      Enable/Disable one-wire interface                                        \u2502\n\u2502        P8 Remote GPIO Enable/Disable remote access to GPIO pins                                \u2502\n\u2502                                                                                                \u2502\n\u2502                                                                                                \u2502\n\u2502                                                                                                \u2502\n\u2502                                                                                                \u2502\n\u2502                           &lt;Select&gt;                           &lt;Back&gt;                            \u2502\n\u2502                                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n</pre>  And enable the interface:  <pre>\n\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502                                                          \u2502 \n                   \u2502 Would you like the ARM I2C interface to be enabled?      \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502               &lt;Yes&gt;                  &lt;No&gt;                \u2502 \n                   \u2502                                                          \u2502 \n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n                                                                                \n\n</pre>  Confirm the activation and restart the RPi:  <pre>\n\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502                                                          \u2502 \n                   \u2502 The ARM I2C interface is enabled                         \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                                                          \u2502 \n                   \u2502                          &lt;Ok&gt;                            \u2502 \n                   \u2502                                                          \u2502 \n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n                                                                                \n\n</pre>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/hardware-interfaces/#usb-devices","title":"USB Devices","text":"<p>Similar to accessing <code>i2c</code> devices, a non root user can use usb connections by adding it to the the <code>dialout</code> group:</p> <pre><code>sudo adduser fjp dialout\nAdding user `fjp' to group `dialout' ...\nAdding user fjp to group dialout\nDone.\n</code></pre>","tags":["2wd","differential drive","robot","ros","noetic","raspberry","pi","autonomous","ubuntu","focal","package","gazebo","simulation","hardware_interfacem","hardware","interface","low-level","microcontroller","dialout","usb","gpio"]},{"location":"processing_units/jetson-nano-setup/","title":"Jetson Nano Setup","text":"<p>These are the instructions to setup the official Ubuntu 18.04 image from Nvidia on a Jetson Nano.</p> <p>Note</p> <p>In case you don't have a Jetson Nano, it is also possible to create the robot using a Raspberry Pi 4 B. See the related section in the documentation.</p>"},{"location":"processing_units/jetson-nano-setup/#installing-the-jetpack-sdk","title":"Installing the JetPack SDK","text":"<p>The JetPack SDK is a tool that installs the software tools and operating system for a Jetson Development Kit.</p> <p>Note</p> <p>JetPack SDK includes the latest Jetson Linux Driver Package (L4T) with Linux operating system and CUDA-X accelerated libraries and APIs for Deep Learning, Computer Vision, Accelerated Computing and Multimedia. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics and Isaac for robotics.<sup>1</sup></p> <p>Start by downloading the 6 GB SD card image for the Jetson Nano Developer Kit  from the Nvidia Developer website (direct download link). Alternatively you can get the Jetpack from the Jetson Download Center. </p> <p>Note</p> <p>In case you want to use the NVIDIA SDK Manager you need to be part of the NVIDIA Developer Program.  You can use this manager to configure the Jetpack image before flashing it.</p> <p>Next use a tool to flash the image onto the SD card. One option is to use balenaEtcher that is available on Ubuntu, Mac and Windows.</p> <p>Info</p> <p>Flashing in this context, means the transfer of software data, also refered to as Firmware,  from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a  device where the Firmware is stored.</p>"},{"location":"processing_units/jetson-nano-setup/#power-supply","title":"Power Supply","text":"<p>There are four ways you can power the Jetson Nano.</p> <ol> <li>Provide 2 Amps at 5 Volts to the micro USB connector</li> <li>4 Amps at 5 Volts to the barrel jack connector.</li> <li>5 Volts on the GPIO headers, where each of the two 5 Volt pins can handle up to 3 A. The Nano has two of these 5 Volt pins, which means you could consume 6 A total at 5 V by having the two pins connected to a power supply that is capable of delivering 6 A at 5 V, corresponding to 30 Watts.</li> <li>Power Over Ethernet (POE)</li> </ol> <p>In its default power consumption state ('mode 0') the Nano itself uses 10 Watts, which is 2 A at 5 V. This, however, is only for the Jetson Nano compute module and doesn't include the peripherals. To provide power to peripherals, for example over the USB connectors, you need to operate the Nano itself in 5 Watts mode ('mode 1' or MAXN). This way there are 5 Watts left for peripherals via the micro USB connector, that provides 2 A at 5 V.</p> <p>The following table<sup>2</sup> shows the power modes for the Jetson Nano predefined by NVIDIA and the associated caps on use of the module's resources.</p> Property MAXN<sup>3</sup> 5W Power Budget 10 Watts 5 Watts Mode ID 0 1 Online CPU 4 2 CPU Maximal Frequency (MHz) 1479 918 GPU TPC 1 1 GPU Maximal Frequency (MHz) 921.6 640 Memory Maximal Frequency (MHz) 1600 1600 SOC clocks maximal frequency (MHz) All modes adsp 844.8 csi 750 se 627.2 ape 499.2 nvdec 716.8 tsec 408 host1x 408 nvenc 716.8 tsecb 627.2 isp 793.6 nvjpg 627.2 vi 793.6 display 665.6 pcie 500 vic03 627.2 <p>Note</p> <p>In practice, when running from the micro-USB connector, we should be running in 5V mode to power the rest of the sensors, like the laser scanner. The drawback is that this will slow down the computations on the Jetson Nano. See the section below on how to do that.</p> <p>To develop and test code that requires high power, it is convenient to use the second option.  To connect the 4 Amps @ 5 Volts barrel jack connector to the Jetson Nano a Jumper is required.</p> <p>Note</p> <p>Make sure to use a 5 V 4 Amps switching power supply.  For example the Mean Well GST25E05-P1J or the AC/DC Desktop Adapter 5 V from Adafruit. DC barrel jack 5.5 mm OD / 2.1 mm ID / 9.5 mm length, center pin positive.</p> <p>Connect the jumper on J48. J48 is located between the Barrel Jack connector and the Camera connector.  This jumper tells the Nano to use the Barrel Jack instead of the micro-USB port.  Then plug the power supply into the Barrel Jack, and the Nano boots.</p> <p>TODO add image of Jumper location</p> <p>Note</p> <p>There are two variants of the Jetson Nano. The older A02 and the revised B01. Depending on the variant the location of the jumper is slightly different. - For an A02 carrier board (pre-2020) J48 is the solo header next to the camera port. - For a B01 carrier board (2020+, has two camera ports) J48 is a solo header behind the barrel jack and the HDMI port.</p>"},{"location":"processing_units/jetson-nano-setup/#prepare-ubuntu","title":"Prepare Ubuntu","text":"<p>After flashing the image to the sd card insert it to the Jetson Nano, hook it up to a monitor via HDMI and power it up by pluggin in the micro USB or even better barrel jack connector (don't forget the jumper on J48). The follow the instructions on the screen to setup Ubuntu 18.04 Bionic Beaver. For this you will need to accept the Nvidia End User License Agreement, set the desired language, keyboard, time zone, login credentials, APP Partition size (choose max possible),  and delete unused bootloader that is done automatically with the new QSPI image (MaxSPI) of Jetpack 4.5. This QSPI update will take about two mins. Finally select the NVPModel mode explained above. First we go with the default MAXN 10 Watts mode ('mode 0'). We will changed this later at runtime - when powering the robot over the power bank - using the nvpmodel GUI or nvpmodel command line utility.  Refer to the NVIDIA Jetson Linux Developer Guide for further information. </p> <p>Once finished, follow the next steps to install ROS Melodic.</p> <p>Note</p> <p>To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run <code>ubuntu-setup.sh</code>. But to learn more, you should follow the instructions on the following pages.</p>"},{"location":"processing_units/jetson-nano-setup/#resources","title":"Resources","text":"<ul> <li>JetPack SDK</li> <li>Jetson Nano \u2013 Use More Power! - Jetson Hacks</li> </ul> <ol> <li> <p>https://developer.nvidia.com/embedded/jetpack \u21a9</p> </li> <li> <p>Nvidia Jetson Nano: Supported Modes and Power Efficiency \u21a9</p> </li> <li> <p>The default mode is MAXN (power budget 10 watts, mode ID 0).\u00a0\u21a9</p> </li> </ol>"},{"location":"processing_units/ros-network-setup/","title":"ROS Network Setup","text":""},{"location":"processing_units/ros-network-setup/#ros-network-setup","title":"ROS Network Setup","text":"<p>ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B.</p> <p>For detailed instructions see ROS Network Setup and the  ROS Environment Variables.</p> <p>The setup between the work machine that handles compute heavy tasks and DiffBot is as follows:</p> <p>TODO image</p> <p>On DiffBot we configure the <code>ROS_MASTER_URI</code> to be the IP address of the work machine.</p> <pre><code>export ROS_MASTER_URI=http://192.168.0.9:11311/\n</code></pre> <p>To test run the master on the work machine using the <code>roscore</code> command. Then run the the <code>listener</code> from the <code>roscpp_tutorials</code> package in another terminal:</p> <pre><code>fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener\n</code></pre> <p>Then switch to a terminal on DiffBot's Raspberry Pi and run the <code>talker</code> from <code>roscpp_tutorials</code>:</p> <pre><code>fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker \n[ INFO] [1602018325.633133449]: hello world 0\n[ INFO] [1602018325.733137152]: hello world 1\n[ INFO] [1602018325.833112540]: hello world 2\n[ INFO] [1602018325.933114483]: hello world 3\n[ INFO] [1602018326.033114093]: hello world 4\n[ INFO] [1602018326.133112684]: hello world 5\n[ INFO] [1602018326.233112183]: hello world 6\n[ INFO] [1602018326.333113126]: hello world 7\n[ INFO] [1602018326.433113680]: hello world 8\n[ INFO] [1602018326.533113031]: hello world 9\n[ INFO] [1602018326.633110140]: hello world 10\n[ INFO] [1602018326.733108954]: hello world 11\n[ INFO] [1602018326.833113267]: hello world 12\n[ INFO] [1602018326.933164505]: hello world 13\n[ INFO] [1602018327.033119135]: hello world 14\n[ INFO] [1602018327.133113559]: hello world 15\n[ INFO] [1602018327.233111003]: hello world 16\n[ INFO] [1602018327.333110705]: hello world 17\n[ INFO] [1602018327.433126425]: hello world 18\n[ INFO] [1602018327.533111498]: hello world 19\n[ INFO] [1602018327.633107978]: hello world 20\n[ INFO] [1602018327.733110736]: hello world 21\n[ INFO] [1602018327.833107605]: hello world 22\n[ INFO] [1602018327.933111659]: hello world 23\n[ INFO] [1602018328.033108065]: hello world 24\n[ INFO] [1602018328.133110379]: hello world 25\n[ INFO] [1602018328.233150191]: hello world 26\n[ INFO] [1602018328.333135986]: hello world 27\n[ INFO] [1602018328.433153558]: hello world 28\n[ INFO] [1602018328.533154557]: hello world 29\n[ INFO] [1602018328.633151667]: hello world 30\n[ INFO] [1602018328.733128777]: hello world 31\n[ INFO] [1602018328.833170108]: hello world 32\n[ INFO] [1602018328.933172402]: hello world 33\n</code></pre> <p>Looking back at the terminal on the work machine you should see the output:</p> <pre><code>fjp.github.io git:(master) rosrun roscpp_tutorials listener\n[ INFO] [1602018328.330070016]: I heard: [hello world 27]\n[ INFO] [1602018328.430244670]: I heard: [hello world 28]\n[ INFO] [1602018328.530173113]: I heard: [hello world 29]\n[ INFO] [1602018328.630251690]: I heard: [hello world 30]\n[ INFO] [1602018328.730334064]: I heard: [hello world 31]\n[ INFO] [1602018328.830346566]: I heard: [hello world 32]\n[ INFO] [1602018328.930009032]: I heard: [hello world 33]\n</code></pre> <p>Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.</p>"},{"location":"processing_units/ros-setup/","title":"ROS Installation","text":"<p>The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. To install ROS follow the installation instructions.</p> <p>Info</p> <p>In the 1.4 Installation step you have to choose how much of ROS you want to install. For the development pc you can go with the <code>sudo apt install ros-noetic-desktop-full</code> command. For the robot install the <code>ros-noetic-robot</code> Ubuntu package. Other system dependencies will be installed with the <code>rosdep</code> command, explained in the following section.</p> <p>Another program that is required to run ROS nodes written with the <code>rospy</code> client library is <code>python-is-python3</code>. Install it with:</p> <pre><code>sudo apt install python-is-python3\n</code></pre>"},{"location":"processing_units/ros-setup/#dependencies","title":"Dependencies","text":"<p>After having git cloned one or more ROS packages, such as <code>diffbot</code>,  it is necessary to install system dependencies of the packages in the catkin workspace. For this, ROS provides the <code>rosdep</code> tool.  To install all system dependencies of the packages in your catkin workspace make use of the  following command (source):</p> <pre><code>rosdep install --from-paths src --ignore-src -r -y\n</code></pre> <p>This will go through each package's <code>package.xml</code> file and install the listed dependencies that are currently not installed on your system.</p>"},{"location":"processing_units/ros-setup/#build-tool-catkin_tools","title":"Build Tool: <code>catkin_tools</code>","text":"<p>To work with ROS we will use <code>catkin_tools</code>  instead of <code>catkin_make</code>. <code>catkin_tools</code> provide commands such as <code>catkin build</code> which we will use instead of <code>catkin_make</code> because the <code>catkin_tools</code> are more actively developed than <code>catkin_make</code> ref.</p> <p>Note</p> <p>It is recommended to use <code>catkin_tools</code> instead of the default <code>catkin</code> when building ROS workspaces.  <code>catkin_tools</code> provides a number of benefits over regular <code>catkin_make</code> and will be used in the documentation.  All packages can be built using <code>catkin_make</code> however: use <code>catkin_make</code> in place of <code>catkin build</code> where appropriate.</p> <p>Bug</p> <p>The current way to install <code>catkin-tools</code> in the documentation  from the Ubuntu package repository doesn't work. Follow the steps below instead for now.</p> <p>Success</p> <p>As of now the correct way to install <code>catkin-tools</code> is to use the following command: <pre><code>sudo apt-get install python3-osrf-pycommon python3-catkin-tools\n</code></pre> For your reference, you can read more about it in this open issue.</p> <p>After sucessfully installing <code>catkin_tools</code> we can create and initialize a workspace (called <code>ros</code> for this project) with the commands listed in the build_tools documentation:</p> <p>Note</p> <p>Note that we already <code>source</code>d the <code>setup.bash</code> while following the ROS installation instructions.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src    # Make a new workspace and source space\nfjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros              # Navigate to the workspace root\nfjp@ubuntu:~/git/2wd-robot/ros$ catkin init                         # Initialize it with a hidden marker file\nInitializing catkin workspace in `/home/fjp/git/2wd-robot/ros`.\n----------------------------------------------------------------\nProfile:                     default\nExtending:             [env] /opt/ros/melodic\nWorkspace:                   /home/fjp/git/2wd-robot/ros\n----------------------------------------------------------------\nBuild Space:       [missing] /home/fjp/git/2wd-robot/ros/build\nDevel Space:       [missing] /home/fjp/git/2wd-robot/ros/devel\nInstall Space:      [unused] /home/fjp/git/2wd-robot/ros/install\nLog Space:         [missing] /home/fjp/git/2wd-robot/ros/logs\nSource Space:       [exists] /home/fjp/git/2wd-robot/ros/src\nDESTDIR:            [unused] None\n----------------------------------------------------------------\nDevel Space Layout:          linked\nInstall Space Layout:        None\n----------------------------------------------------------------\nAdditional CMake Args:       None\nAdditional Make Args:        None\nAdditional catkin Make Args: None\nInternal Make Job Server:    True\nCache Job Environments:      False\n----------------------------------------------------------------\nWhitelisted Packages:        None\nBlacklisted Packages:        None\n----------------------------------------------------------------\nWorkspace configuration appears valid.\n----------------------------------------------------------------\n</code></pre>"},{"location":"processing_units/ros-setup/#command-overview-of-catkin_tools","title":"Command Overview of <code>catkin_tools</code>","text":"<p>To create packages, which will be covered in the next posts in more depth, we will use <code>catkin create pkg PKG_NAME</code>.</p> <p>Building the workspace is done with <code>catkin build</code>.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ catkin build\n----------------------------------------------------------------\nProfile:                     default\nExtending:             [env] /opt/ros/melodic\nWorkspace:                   /home/fjp/git/2wd-robot/ros\n----------------------------------------------------------------\nBuild Space:        [exists] /home/fjp/git/2wd-robot/ros/build\nDevel Space:        [exists] /home/fjp/git/2wd-robot/ros/devel\nInstall Space:      [unused] /home/fjp/git/2wd-robot/ros/install\nLog Space:         [missing] /home/fjp/git/2wd-robot/ros/logs\nSource Space:       [exists] /home/fjp/git/2wd-robot/ros/src\nDESTDIR:            [unused] None\n----------------------------------------------------------------\nDevel Space Layout:          linked\nInstall Space Layout:        None\n----------------------------------------------------------------\nAdditional CMake Args:       None\nAdditional Make Args:        None\nAdditional catkin Make Args: None\nInternal Make Job Server:    True\nCache Job Environments:      False\n----------------------------------------------------------------\nWhitelisted Packages:        None\nBlacklisted Packages:        None\n----------------------------------------------------------------\nWorkspace configuration appears valid.\n\nNOTE: Forcing CMake to run for each package.\n----------------------------------------------------------------\n[build] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src'\n[build] No packages to be built.\n[build] Package table is up to date.\nStarting  &gt;&gt;&gt; catkin_tools_prebuild\nFinished  &lt;&lt;&lt; catkin_tools_prebuild                [ 10.0 seconds ]\n[build] Summary: All 1 packages succeeded!\n[build]   Ignored:   None.\n[build]   Warnings:  None.\n[build]   Abandoned: None.\n[build]   Failed:    None.\n[build] Runtime: 10.1 seconds total.\n</code></pre>"},{"location":"processing_units/ros-setup/#environment-setup","title":"Environment Setup","text":"<p>Finally the newly built packages have to be loaded in the environment using <code>source</code>.</p> <pre><code>fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment\n</code></pre> <p>Tip</p> <p>To avoid tediously typing the above <code>source</code> command, it is convenient to create an alias in your <code>~/.bashrc</code> or <code>~/.zshrc</code> similar to the following: <pre><code>alias s='source devel/setup.bash'\n</code></pre> or using the absolute path <pre><code>alias sa='source ~/git/2wd-robot/ros/devel/setup.bash'\n</code></pre> It is recommended to use the correct setup script for the shell you use (<code>bash</code>, <code>zsh</code>, etc.). In case you are unsure, you can check with the <code>echo $SHELL</code> command which will most likely output <code>/bin/bash</code>.</p> <p>Info</p> <p>Instead of <code>source</code> it is possible to use the <code>.</code> command instead. Don't confuse it though with the current directory, which is also represented as <code>.</code>.</p>"},{"location":"processing_units/ros-setup/#resources","title":"Resources","text":"<p>Although the catkin tutorial uses <code>catkin_make</code> it provides a helpful guide to create a workspace</p>"},{"location":"processing_units/rpi-setup/","title":"Raspberry Pi Setup","text":"<p>These are the instructions to setup a custom Ubuntu 20.04 Focal Fossa on Raspberry Pi 4 B.</p> <p>Note</p> <p>In case you don't have a Raspberry Pi 4 B, it is also possible to create the robot using a Jetson Nano from Nvidia. See the related section in the documentation.</p>"},{"location":"processing_units/rpi-setup/#obtain-ubuntu-2004-mate-image-for-raspberry-pi","title":"Obtain Ubuntu 20.04 Mate Image for Raspberry Pi","text":"<p>To install the long term supported (LTS) Ubuntu 20.04 on the Raspberry Pi 4B we make use of arm64 version  of Ubuntu Mate. </p> <p>Download the latest release of the image and flash it to an empty sd card. To do this follow the instructions on the  Raspberry Pi documentation or  balenaEtcher.  Another way is to use the Raspberry Pi Imager explained here.</p> <p>Info</p> <p>Flashing in this context, means the transfer of software data, also refered to as Firmware,  from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a  device where the Firmware is stored.</p>"},{"location":"processing_units/rpi-setup/#wifi-issues","title":"Wifi Issues","text":"<p>So far there are no known issues using WiFi with Ubuntu Mate 20.04 arm64 on the Raspberry Pi 4B.</p> Possible issues with other images    If you are not in the US it is possible that you encounter connection problems when connected to a 5Ghz Wifi network.   If you are in a different country than the US you need to update your regulatory country. 5Ghz needs this to know the right bands to use.  This can be changed by editing the value of `REGDOMAIN` in the file `/etc/default/crda` ([Central Regulatory Domain Agent](https://wireless.wiki.kernel.org/en/developers/regulatory/crda)) to the code for your country [ref](https://github.com/TheRemote/Ubuntu-Server-raspi4-unofficial/issues/98).  <p>There might be some wifi issues where the connection is lost after some time. This might be a firmware issue reported here. Although the wifi power save management is turned off by default, make sure to also do this for external wifi usb dongles by editing the following file</p> <pre><code>sudo vim /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf\n</code></pre> <p>It contains:</p> <pre><code>[connection]\nwifi.powersave = 3\n</code></pre> <p>Set this to <code>2</code> to turn power management off. </p> <p>Possible values for the <code>wifi.powersave</code> field are:</p> <pre><code>NM_SETTING_WIRELESS_POWERSAVE_DEFAULT (0): use the default value\nNM_SETTING_WIRELESS_POWERSAVE_IGNORE  (1): don't touch existing setting\nNM_SETTING_WIRELESS_POWERSAVE_DISABLE (2): disable powersave\nNM_SETTING_WIRELESS_POWERSAVE_ENABLE  (3): enable powersave\n</code></pre> <p>(Informal source on GitHub for these values.)</p> <p>You can check if it is turned off with <code>iwconfig</code>:</p> <pre><code>wlan0     IEEE 802.11  ESSID:\"wifiname\"  \n          Mode:Managed  Frequency:5.18 GHz  Access Point: E0:28:6D:48:33:5D   \n          Bit Rate=433.3 Mb/s   Tx-Power=31 dBm   \n          Retry short limit:7   RTS thr:off   Fragment thr:off\n          Power Management:off\n          Link Quality=70/70  Signal level=-27 dBm  \n          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0\n          Tx excessive retries:1  Invalid misc:0   Missed beacon:0\n\nlo        no wireless extensions.\n\nwlan1     IEEE 802.11AC  ESSID:\"wifiname\"  Nickname:\"WIFI@RTL8821CU\"\n          Mode:Managed  Frequency:5.18 GHz  Access Point: E0:28:6D:48:33:5D   \n          Bit Rate:434 Mb/s   Sensitivity:0/0  \n          Retry:off   RTS thr:off   Fragment thr:off\n          Power Management:off\n          Link Quality=64/100  Signal level=-26 dBm  Noise level=0 dBm\n          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0\n          Tx excessive retries:0  Invalid misc:0   Missed beacon:0\n\neth0      no wireless extensions.\n</code></pre> <p>This is the output when there is one external usb WiFi dongle connected to the Raspberry Pi 4 B and no ethernet cable.</p>"},{"location":"processing_units/rpi-setup/#instructions-to-install-wifi-drivers","title":"Instructions to install Wifi drivers","text":"<p>In case you use a Realtek USB Wifi dongle it might not be directly supported by the linux kernel. To install the correct driver, you first have to figure out the driver id. When plugging in the USB Wifi dongle and running <code>dmesg</code> afterwards shoud output something similar to:</p> <pre><code>[ 1430.931258] usb 1-1.3: new high-speed USB device number 13 using xhci_hcd\n[ 1431.031913] usb 1-1.3: New USB device found, idVendor=0bda, idProduct=c811, bcdDevice= 2.00\n[ 1431.031925] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[ 1431.031932] usb 1-1.3: Product: 802.11ac NIC\n[ 1431.031939] usb 1-1.3: Manufacturer: Realtek\n</code></pre> <p>With the <code>idVendor=0bda</code> and <code>idProduct=c811</code> you can search for the correct driver. In this case it is the</p> <pre><code>RTL8821cu   USB     0bda:c811   Realtek     WEP WPA WPA2\n</code></pre> <p>For it the instructions in the <code>https://github.com/brektrou/rtl8821CU</code> repository can be used to install the driver.</p> <p>Warning</p> <p>These instructions are specifically for the Realtek (id c811) USB wifi dongle. In case you use another USB dongle it might work directly or there exists a ubuntu package. In general try to use Google for instructions on how to install.  The following is only an example on what might be needed to get the driver  working.</p> <p>After cloning this package, the Makefile needs to be adapted to work for the Raspberry Pi 4B:</p> <pre><code>###################### Platform Related #######################\nCONFIG_PLATFORM_I386_PC = n\nCONFIG_PLATFORM_ARM_RPI = n\nCONFIG_PLATFORM_ARM64_RPI = y\n</code></pre> <p>Then follow the instructions from the README.md:</p> <pre><code># For AArch64\nsudo cp /lib/modules/$(uname -r)/build/arch/arm64/Makefile /lib/modules/$(uname -r)/build/arch/arm64/Makefile.$(date +%Y%m%d%H%M)\nsudo sed -i 's/-mgeneral-regs-only//' /lib/modules/$(uname -r)/build/arch/arm64/Makefile\n</code></pre> <p>and finally make and install the driver:</p> <pre><code>make\nsudo make\n</code></pre> <p>After a reboot the USB Wifi dongle should be detected and two wifi adapters should show up - the internal wifi module on the RPi and the USB dongle.</p> <p>Sources</p> <ul> <li>Disable power management in Stretch</li> <li>How to turn off Wireless power management permanently</li> <li>RPi4: WiFi client crashes often (brcmf_fw_crashed: Firmware has halted or crashed)</li> <li>WLAN Karten Realtek (german)</li> </ul>"},{"location":"processing_units/rpi-setup/#prepare-ubuntu","title":"Prepare Ubuntu","text":"<p>After flashing the image to the sd card insert it to the Pi, hook it up to a monitor via HDMI and power it up by pluggin in the USB-C connector. Then you should follow the installation instructions on the screen.</p> <p>Once finished, follow the next steps to install ROS Noetic.</p> <p>Note</p> <p>To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run <code>ubuntu-setup.sh</code>. But to learn more, you should follow the instructions on the following pages.</p>"},{"location":"processing_units/teensy-mcu/","title":"Microcontroller","text":""},{"location":"processing_units/teensy-mcu/#teensy-setup","title":"Teensy Setup","text":"<p>The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the <code>/diffbot/ticks_left</code> and <code>/diffbot/ticks_right</code> ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi.</p> <p>To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed:</p> <pre><code>sudo apt install ros-noetic-rosserial\n</code></pre> <p>To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website. Here the instructions to setup Teensyduino in Linux are listed:</p> <ol> <li>Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d.    <code>sudo cp 49-teensy.rules /etc/udev/rules.d/</code></li> <li>Download and extract one of Arduino's Linux packages.    Note: Arduino from Linux distro packages is not supported.</li> <li>Download the corresponding Teensyduino installer.</li> <li>Run the installer by adding execute permission and then execute it.    <code>chmod 755 TeensyduinoInstall.linux64</code> <code>./TeensyduinoInstall.linux64</code></li> </ol> <p>The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board.</p> <p>Note</p> <p>Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories.</p> <p>The following video shows installation process,  more instructions to setup the Arduino IDE can be found in the ROS wiki.</p> <p>To check if the connection to the Teensy board works use these commands on the Raspberry Pi:</p> <pre><code>$ lsusb\nBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial\nBus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre> <p>And to see on which serial port it is connected use:</p> <pre><code>$ ls /dev | grep ttyACM0\nttyACM0\n</code></pre> <p>If the output is empty it might be the case that the board is connected to another port like <code>ttyUSB0</code>.  </p>"},{"location":"processing_units/teensy-mcu/#encoder-program","title":"Encoder Program","text":"<p>When installing Teensyduino new example programs are provided. One of them is to test Encoders.  The code for the motor encoders uses it as basis together with a pubsub example from rosserial:</p> <p>TODO link to code encoders.ino</p> <p>After the program is flashed to the Teensy board it can be tested with the following procedure:</p> <ol> <li>Start a ROS master by executing <code>roscore</code> in a new terminal.</li> <li>Create a rosserial node using <code>rosserial_python</code> package:</li> </ol> <pre><code>$ rosrun rosserial_python serial_node.py _port:=/dev/ttyACM0 _baud:=115200\n[INFO] [1602784903.659869]: ROS Serial Python Node\n[INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud\n[INFO] [1602784905.809722]: Requesting topics...\n[INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes\n[INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32]\n[INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32]\n[INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes\n[INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty]\n</code></pre> <p>In case of the following error, probably the wrong program is flashed to the Teensy board:</p> <pre><code>[ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino\n</code></pre> <p>Note</p> <p>Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board.</p> <p>Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout.</p> Teensy 4.0 Pins. <p>The bread board view of Fritzing shows the connection schematic and is shown for both models in the following:</p> DiffBotRemo <p></p> <p></p> <p>With one motor encoder connected to pins 5, 6, echo the <code>/encoder_ticks</code> topic:</p> <pre><code>rostopic echo /encoder_ticks\n</code></pre> <p>Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array:</p> <pre><code>---\nheader: \n  seq: 190323\n  stamp: \n    secs: 0\n    nsecs:         0\n  frame_id: ''\nencoders: [0, 0]\n---\nheader: \n  seq: 190324\n  stamp: \n    secs: 0\n    nsecs:         0\n  frame_id: ''\nencoders: [230, 0]\n---\nheader: \n  seq: 190325\n  stamp: \n    secs: 0\n    nsecs:         0\n  frame_id: ''\nencoders: [350, 0]\n---\nheader: \n  seq: 190326\n  stamp: \n    secs: 0\n    nsecs:         0\n  frame_id: ''\nencoders: [480, 0]\n---\nheader: \n  seq: 190327\n  stamp: \n    secs: 0\n    nsecs:         0\n  frame_id: ''\nencoders: [540, 0]\n</code></pre> <p>The found value 540 for a full turn of the wheel is important for the hardware interface.</p>"},{"location":"processing_units/teensy-mcu/#base-controller","title":"Base Controller","text":"<p>If you are working with Remo the recommende way is to use <code>base_controller</code> from <code>diffbot_base/scripts</code> instead of the <code>encoders.ino</code>. Build instructions using Visual Studio Code including the PlatformIO plugin are shown in the following video:</p> <p>When using <code>base_controller</code> you should use the low level PID controllers running on the MCU.</p>"},{"location":"theory/","title":"Theory behind Robotics","text":"<p>The following pages will provide an overview of the following important topics of robotics: For more details be sure to check out the Self Driving cars with Duckietown edX MOOC. Also don't forget to read through the resources listed on the main page of this documentation and consult your robotics engineering text books of choice.</p> <ul> <li> <p>Modeling and Control</p> <ul> <li>Introduction to control systems</li> <li>Representations and models</li> <li>PID control</li> </ul> </li> <li> <p>Robot Vision</p> <ul> <li>Introduction to projective geometry</li> <li>Camera modeling and calibration</li> <li>Image processing</li> </ul> </li> <li> <p>Object Detection</p> <ul> <li>Convolutional neural networks</li> <li>One and two stage object detection</li> </ul> </li> <li> <p>State Estimation and Localization</p> <ul> <li>Bayes filtering framework</li> <li>Parameterized methods (Kalman filter)</li> <li>Sampling-based methods (Particle and histogram filter)</li> </ul> </li> <li> <p>Planning I</p> <ul> <li>Planning formalization</li> <li>Graphs</li> </ul> </li> <li> <p>Planning II</p> <ul> <li>Probabilistic roadmaps</li> <li>Sampling-based planning</li> </ul> </li> <li> <p>Learning by Reinforcement</p> <ul> <li>Markov decision processes</li> <li>Value functions</li> <li>Policy gradients</li> <li>Domain randomization</li> </ul> </li> <li> <p>Learning by Imitation</p> <ul> <li>Behaviour cloning</li> <li>Online imitation learning</li> <li>Safety and uncertainty</li> </ul> </li> </ul>"},{"location":"theory/#resources","title":"Resources","text":"<ul> <li>Self Driving cars with Duckietown edX MOOC</li> <li>Uni Freiburg SLAM Course, Robot Mapping - WS 2013/14</li> </ul>"},{"location":"theory/actuation/","title":"Actuation","text":""},{"location":"theory/actuation/#actuators","title":"Actuators","text":""},{"location":"theory/actuation/#motor","title":"Motor","text":""},{"location":"theory/actuation/#encoder","title":"Encoder","text":""},{"location":"theory/actuation/#gearbox","title":"Gearbox","text":"<p>Gearboxes can be used to fine-tune the performance characteristics of a motion axis.  They're most commonly considered torque multipliers but actually serve several other functions,  including speed matching, inertia reduction, and resolution increaseref.</p>"},{"location":"theory/motion-and-odometry/","title":"Motion and Odometry","text":""},{"location":"theory/motion-and-odometry/#robotic-motion-and-odometry","title":"Robotic Motion and Odometry","text":"<p>The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics. The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E. </p> <p>This section reviews the basic concepts of distance, time, velocity and acceleration.  The physics of motion can be described using calculus, but a computer cannot deal with continuous functions;  instead, discrete approximations must be used.</p> <p>Odometry, the fundamental algorithm for computing robotic motion.  An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and  the change direction from the velocity of the wheels in a short period of time.  Unfortunately, odometry is subject to serious errors.  It is important to understand that errors in direction are much more significant than errors in distance.</p> <p>See the following video explaining odometry:</p> <p>In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to  the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used,  which measure the actual number of revolutions of the wheels.</p> <p>The following video from Sparkfun gives an overview of Encoders</p>"},{"location":"theory/motion-and-odometry/#distance-velocity-and-time","title":"Distance, Velocity and Time","text":"<p>In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\), the distance \\(s\\) it moves is \\(s = v \\cdot t\\). When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity</p> <ul> <li>No two electrical or mechanical components are ever precisely identical.</li> <li>The environment affects the velocity of a robot because of different friction of the surface</li> <li>External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity.</li> </ul> <p>Note</p> <p>Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different.</p>"},{"location":"theory/motion-and-odometry/#acceleration-as-change-in-velocity","title":"Acceleration as Change in Velocity","text":"<p>To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually.  Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by:</p> \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] <p>Acceleration is defined as the change in velocity over a period of time</p> \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\]"},{"location":"theory/motion-and-odometry/#references","title":"References","text":"<ul> <li>Kinematics equations for Differential Drive and Articulated Steering</li> </ul>"},{"location":"theory/architecture/docker/","title":"Docker","text":""},{"location":"theory/architecture/docker/#docker-containers","title":"Docker Containers","text":"<p>A container includes an application and its dependencies.  It's goal is to easily ship (deploy) and handle applications.</p> <p>Like real world shipping containers, Docker containers wrap up an application in a filesystem containing everything the application needs to run:</p> <ul> <li>source code</li> <li>runtime libraries</li> <li>system tools</li> <li>configruation files</li> </ul> <p>The result is that a containerzied application will run identically on any host.  And there are no incompatibilieties of any kind.</p>"},{"location":"theory/architecture/docker/#why-containerization","title":"Why Containerization","text":"<p>In the traditional operating systems like Linux Ubuntu a package manager install apps. These apps run on shared runtime libraries. Therfore, applications are coupled, because they share the same dependencies. This can can lead to compatibility issues, when, for example, two or more applications requires a different version of the same shared library.</p> <p>When using containerization, each application running in a container comes with its own needed set of libraries. This way, each application container is isolated and can be updated independently.</p>"},{"location":"theory/architecture/docker/#difference-between-containerization-and-virtual-machines","title":"Difference between Containerization and Virtual Machines","text":"<p>Containers are different to virtual machines. In a virtual machine environment there is a host system that runs a hypervisor (e.g. VMWare). The hypervisor divides the physical  hardware resources among the virtual machines. Each virtual machine runs its own operating system.</p> <p>The disadvantage of this is that there is a considerable overhead and processes cannot communicate  over different virtual machines.</p> <p>On the other hand, containerization has minimal overhead and it allow us to deploy multiple applications so that they can communicate with each other.</p>"},{"location":"theory/localization/markov-and-gaussian-localization/","title":"Markov and Gaussian Localization","text":""},{"location":"theory/localization/markov-and-gaussian-localization/#mobile-robot-localization-markov-and-gaussian","title":"Mobile Robot Localization: Markov and Gaussian","text":"<p>Mobile robot localilzation is the problem of determining the pose of a robot relative to a given map of the environment. It is also refered to as position estimation.</p>"},{"location":"theory/localization/markov-and-gaussian-localization/#markov-localization","title":"Markov Localization","text":"<p>Probabilistic localization algorithms are variants of the Bayes filter. Markov localization is just a different name for the Bayes filter applied to the mobile robot localization problem.</p> <pre>\n    \\begin{algorithm}\n    \\caption{Markov localization}\n    \\begin{algorithmic}\n    \\PROCEDURE{MarkovLocalization}{$bel(x_{t-1}), u_t, z_t, m$}\n        \\FORALL{$x_t$}\n            \\STATE $\\bar{bel}(x_t) = \\int p(x_t | u_t, x_{t-1}, m) bel(x_{t-1}) \\,d x_{t-1}$\n            \\STATE $bel(x_t) = \\eta p(z_t | x_t, m) \\bar{bel}(x_t)$\n        \\ENDFOR\n        \\RETURN $bel(x_t)$\n    \\ENDPROCEDURE\n    \\end{algorithmic}\n    \\end{algorithm}\n</pre>"},{"location":"theory/modeling-control/control-systems-introduction/","title":"Introduction to Control Systems","text":"<p>Quote</p> <p>Control systems is the science of making machines behave the way we want them to behave as opposed to how they would naturally behave.</p> <p>Systems have input and output signals and a behavior that evolves over time. Systems can be broken down into components, which are something that we think we understand. In physical systems, inputs are generated by actuators. For example, DC motors or LEDs. Outputs are measured by sensors, for example, cameras or wheel encoders. These sensors produce observations.</p> graph RL   A[Actuators] -- Inputs --&gt; P[Process and Environment];   DIR(\"&lt;img src='https://iconscout.com/ms-icon-310x310.png' width='30' /&gt;\")   P --&gt;|Outputs| S[Sensors];   S -- Observations --&gt; id1[\"&lt;img src='' /&gt;\"];   style id1 fill:#ffff,stroke-width:0px <p>In the case of a mobile robot, the system is the real physical robot and its environment around it. This system receives inputs from the robot's actuators, which results in an observable output by the robot's sensors. For example a change in the camera viewport of the environment or a change in encoder ticks because of rotating wheels.</p> <p>Controlling a system means to design a logical component that will output, at every instant in time, the commands for the plant actuators so that the output of our system follow a given plan.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#need-for-control-systems","title":"Need for Control Systems","text":"<p>The main objectives of the controller are the following:</p> <ol> <li>Stability</li> <li>Performance</li> <li>Robustness</li> </ol>"},{"location":"theory/modeling-control/control-systems-introduction/#stability","title":"Stability","text":"<p>Mathematically, stability can be formalized as Bounded Input Bounded Output.  The output of a system will be bounded for every input to the system that is bounded. </p> <p>TODO</p> <p>Add stability image</p> <p>In other words, if finite energy is provided to the system, then finite energy should exit the system.</p> <p>TODO</p> <p>Add stability I/O image</p> <p>According to the definition of stability provided (BIBO stability),  a system goes unstable when an output of the system diverges (always continues growing in time),  when provided with a finite energy input. When defining the output as \"distance from the center of the lane\",  driving outside of the road would be an example of instability.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#performance","title":"Performance","text":"<p>Beside being stable a system needs also to perform well. Performance can be measured in several ways:</p> <ul> <li>Time before reaching the target state: How quickly does the system converge to the plan</li> <li>Tracking error: how closely a reference signal is followed or how precisely does the system converge.</li> <li>Maximum acceptable error at any point in time</li> <li>Disturbance rejection: the ability to compensate for external stimuli.    How well can the system recover from unexpected external stimuli, such as the noise in the measurements   or disturbances, like a sudden gust of wind or hitting a bump on the road.</li> <li>Noise attenuation: the ability to minimize the effect of high-frequency signals added to the measurements or inputs.</li> </ul> <p>These are all good examples of performance criteria for controller design.</p> <p>TODO</p> <p>Add performance image with direct path to target and max acceptable error</p>"},{"location":"theory/modeling-control/control-systems-introduction/#robustness","title":"Robustness","text":"<p>Robustness, is the ability of the controller to provide stability and performance even in the presence of uncertainty in the mathematical model of the plant.</p> <p>TODO</p> <p>Add model uncertainty image</p> <p>Model uncertainty can mean that the wheels of the robot are slightly of different sizes or the disposition of mass is different than what's assumed. Also the parameters parameters of the system might just change over time because of wear and tear. A robust controller would handle these cases well despite the uncertainty. Robustness and performance are trade offs and striking the right balance is a challenge that really depends on the application.</p> <p>Model uncertainty is defined as a \"bounded variation\" of the parameters describing the controlled system's model.  Choosing completely different sensors (e.g., a camera instead of the planned pressure sensor for altitude control of a hot air balloon)  might induce a completely different structure in the plant's model, which wouldn't not therefore be a \"bounded\" variation.</p> <p>\"Bounded variation\" in the system behavior of a mobile robot could count as:</p> <ul> <li>Wear and tear of components over time.</li> <li>Slight imperfections in the assembly.</li> </ul>"},{"location":"theory/modeling-control/control-systems-introduction/#controller-structure","title":"Controller Structure","text":"<p>TODO</p> <p>Add stability, performance, robustness images</p> <p>With the mentioned objectives in mind, stability, performance, and robustness, it is important to think about the structure of the controller.</p> <ul> <li>Open Loop Control</li> <li>Close Loop Control</li> </ul> <p>The simplest approach is for the user to send a predetermined sequence of commands directly to the actuators, which is called open loop control.</p> <p>TODO</p> <p>Add open loop image</p> <p>Open Loop Pros</p> <ul> <li>it is stable</li> <li>it is convenient</li> <li>works when the model of the system is good</li> </ul> <p>Open Loop Cons</p> <ul> <li>everything must be planned in advance</li> </ul> <p>If our understanding of the platform response is good, we will obtain the desired outcome. If it is not good or if something unexpected happens during the execution, then the end result will diverge from the plan. In open loop control, information flows only in one direction, from the controller to the plant.</p> <p>To enable the controller to take into account the actual execution,  it is possible to close the loop by feeding the sensor measurements back to the controller, creating a feedback loop.</p> <p>TODO</p> <p>Add closed loop image</p> <p>Closed Feedback Loop Pros</p> <ul> <li>it adapts to circumstances</li> </ul> <p>Closed Feedback Loop Cons</p> <ul> <li>it is less convenient</li> <li>it can destabalize a stable system</li> </ul> <p>Feedback control is very powerful because it allows the whole system to adapt to circumstances as they are unfolding and apply corrections on the fly. The measurements themselves, though, generally need to be processed before being fed back to the controller. This is required because the raw measurements might include a lot of redundant data straight out of the sensors.</p> <p>How to structure this agent is up to the designer, to us.</p> <p>TODO</p> <p>Add closed loop empty agent image</p> <p>We could use different approaches, for example, a deep neural network trained from real data or a simulation to translate the images from the camera directly into commands to the wheels to keep the robot following a desired plan.</p> <p>Traditionally, agents are designed with three main components, perception, planning, and control: to see, to plan, and to act.</p> <p>TODO</p> <p>Add closed loop designed agent image</p>"},{"location":"theory/modeling-control/control-systems-introduction/#perception","title":"Perception","text":"<p>Perception</p> graph LR   id1[\"&lt;img src='' /&gt;\"] -- observations --&gt; P[perception]   subgraph agent/controller   P --&gt; |estimate/belief|PL[planner] --&gt; C[controller]   P --&gt; |estimate/belief|C   end   C -- commands --&gt; id2[\"&lt;img src='' /&gt;\"]   style id1 fill:#ffff,stroke-width:0px   style id2 fill:#ffff,stroke-width:0px <p>TODO</p> <p>Add closed loop empty agent image: perception highlighted</p> <p>The perception block is responsible for transforming the data from the sensors into actionable information for the robot. This passage is sometimes called filtering or estimation. These estimates of relevant quantities, for example, the position or the orientation of the robot's reference frame with respect to the world and the other objects in it, represent what is the robot's belief of his current state. This belief might be more or less corresponding to the truth depending on the quality of the measurements and the perception solution used.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#planner","title":"Planner","text":"<p>TODO</p> <p>Add closed loop empty agent image: planner highlighted</p> <p>The planner instead provides a reference path for the robot to follow. For example, between two navigation points in a more general task of reaching a goal position while avoiding obstacles on the road. The planner receives the state estimate as input so it can adjust the nominal plan on the fly.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#controller","title":"Controller","text":"<p>TODO</p> <p>Add closed loop empty agent image: controller highlighted</p> <p>The plan and the state estimate are finally fed into the actual controller, which uses them to compute a decision applying a certain logic. For a mobile robot, the decision of the controller will be a sequence of commands which will be sent to the motors, finally closing the loop.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#summary","title":"Summary","text":"<ul> <li>Systems are input/output relations we can formalize</li> <li>Control is about making the system's output follow a given plan</li> <li>The control objectives are stability, performance and robustness</li> <li>The traditional feedback control architecture includes perception   planning and control steps</li> </ul> <p>Designing a controller allows to have systems behave in a desired way, rather than following their natural dynamics. Although we can measure quantities of interest and drive the system through actuators, not all systems are controllable. Some systems simply can't be controlled.</p> <p>Systems have their natural dynamics, fundamentally dictated by the laws of physics. More often than not, just \"letting the system go\" will not meet the user's requirements. Control systems leverage our abilities to measure quantities of interest and to actuate (or influence the physical world through devices), to drive the system where we want, and how we want.</p> <p>In practice a controller consists of lines of code or one or more mechanical devices. A controller is typically a logic, that outputs decisions. These decisions are translated to the real world through actuators. It is actually possible to create control logics with analog devices too.</p> <p>Stability is the first design objective for most controllers because unstable systems are potentially unsafe. An unstable system might lead to overshooting driving behaviour, not following a reference path and even driving outside of the road without heading back. There are very few instances in which sending a system unstable might be desirable (exceptions being, e.g., acrobatic flight or selected military/destructive applications). An unstable system will behave unpredictably, potentially with catastrophic results.</p>"},{"location":"theory/modeling-control/control-systems-introduction/#references","title":"References","text":"<ul> <li>Self Driving Cars with Duckietown</li> </ul>"},{"location":"theory/modeling-control/modeling-differential-drive-robot/","title":"Modeling of a differential drive robot","text":"<p>Mathematical models are powerful tools because they allow us to predict the future.</p> \\[ \\begin{align}     \\dot{x}_t &amp;= f(x_t,{\\color{orange}{u_t}}) \\\\     {\\color{green}{y}_t} &amp;= g(x_t,{\\color{orange}{u_t}}) \\end{align} \\] <p>Models map between inputs and outputs of systems and can be derived from first principles or learned from data.</p> <p>TODO</p> <p>Add robot input output image</p> <p>We use models to quantify some essential variable that is useful to accomplish a task,  not to provide a faithful description of the exact reality of all the physical processes going on.</p> <p>The Diffbot is a differential drive robot, where the motion of each wheel is controlled by one DC motor.</p> <p>TODO</p> <p>Add image of robot and two dc motors as input</p> <p>DC motors receive voltages \\({\\color{orange}V_{l/r,t}}\\) as inputs and produce torques on the motor drive axis that spins the wheels, and leads to angular velocity \\(\\dot{\\phi}_{l/r,t}\\) of the motor shaft and wheel. The movement of the wheels will produce an evolution of the robots pose \\({\\color{green}q_t}\\) over time, which is what we want to quantify.</p>"},{"location":"theory/modeling-control/modeling-differential-drive-robot/#forward-and-inverse-kinematics","title":"Forward and Inverse Kinematics","text":"<p>Through these models, we can answer two questions.</p> <ol> <li>(Forward Kinematics) Given a sequence of commands to the wheels \\(\\dot{\\phi}_{1}, \\dot{\\phi}_{2}, \\cdots, \\dot{\\phi}_{t}\\), how will the robot move?</li> <li>(Inverse Kinematics) If we want the robot to move in a certain way, given a desired movement \\((q_1, q_2, \\cdots, q_t)\\), what commands should we send to the wheels?</li> </ol>"},{"location":"theory/modeling-control/modeling-differential-drive-robot/#notations","title":"Notations","text":"<p>We know that the pose of a robot is the position and the orientation of the body frame with respect to the world frame. We define the robot body frame so that the origin, \\(A\\), is in the mid-axle point.</p> <ul> <li>World Frame: \\(\\{{\\color{blue}x^{w}}, {\\color{blue}y^{w}}\\}\\)</li> <li>Body (robot) frame: \\(\\{{\\color{orange}x^{r}}, {\\color{orange}y^{r}}\\}\\)</li> </ul> <p>TODO</p> <p>Add image of robot including orange reference frame and blue world reference frame.</p> <p>Assumption 1: robot is symmetric along longitudinal axis (\\(x^r\\)) and we take it as the x direction of the robot frame.</p> <ul> <li>Equidsitand wheels (axle length = \\(2L\\)). Both wheels will be at a distance \\(L\\) from point \\(A\\).</li> <li>Identical wheels with diameter \\(R\\) ($R_l = R_r = R)</li> <li>Center of mass of the robot will lie on x-axis \\(x^r\\) at distance \\(c\\) from \\(A\\)</li> </ul> <p>Assumption 2: robot chassis is rigid body.</p> <ul> <li>Distance between any two points of the robot does not change in time.</li> <li>in particular \\(\\dot{c} = 0\\), whre \\((\\dot{\\star}) = \\frac{d(\\star)}{dt}\\).</li> </ul>"},{"location":"theory/modeling-control/odometry/","title":"Odometry","text":"<p>As robots move in the world to reach an objective or avoid an obstacle, it is important for them to know where they are.</p> <p>Through odometry, robots can update their pose in time, as long as they know where they started from.</p> <p>Odometry comes from the Greek words \u1f41\u03b4\u1f79\u03c2 [odos] (route) and \u03bc\u1f73\u03c4\u03c1\u03bf\u03bd [metron] (measurement), which literally mean: \"measurement of the route\".</p> <p>The odometry problem can be formulated as:  given an initial pose \\(q\\) of \\(t_0\\) at some initial time, find the pose at any future time \\(t_0 + \\Delta t\\).</p> <p>Given:</p> <p>\\(q(t_0) = q_{t_0} = q_0 = \\begin{bmatrix}x_0 &amp; y_0 &amp; \\theta_0 \\end{bmatrix}^T\\)</p> <p>Find:</p> <p>\\(q_{t_0 + \\Delta t}, \\forall \\Delta t &gt; 0\\)</p> <p>When \\(\\Delta t\\) is small enough to consider the angular speed of the wheels constant,  the pose update can be approximated as a simple sum.</p> <p>\\(q_{t} = \\begin{bmatrix}x_t &amp; y_t &amp; \\theta_t \\end{bmatrix}^T\\)</p> <p>\\(q_{t_{k+1}} = q_{t_k} + \\dot{q}_{t_k}(t_{k+1} - t_k)\\)</p> <p>The process can then be applied iteratively to find the pose at any time,  and at each iteration using the previous estimate as an initial condition.</p> <p>TODO</p> <p>Add odometry update image in world frame</p> <p>The essence of odometry is to use the measurements of the distance traveled by each wheel in a certain time interval and use them to derive the linear and  the angular displacement of the robot in time through a motion model.</p> <p>TODO</p> <p>Add odometry update image with \\(\\Delta x\\) and \\(\\Delta y\\) displacements</p> <p>For example, if our robot starts at time 0 with pose (0, 0, 0),  and drives straight for 1 second at the speed of 1 meter per second,  the odometery will tell us that the final pose will be (1 meter, 0, 0).</p> <p>Recall, the kinematics model, that maps the wheel velocities to the variation of the pose in time.</p> \\[ q_{t_{k+1}} \\approx q_{t_k} + {\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\] <p>Kinematic model</p> \\[ {\\color{orange}{\\dot{q}_{t}}} = \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} =  \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) &amp; 0 \\\\ sin(\\theta_t) &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}1 &amp; 1 \\\\ \\frac{1}{ \\color{green}{L} } &amp; -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} \\] <p>This model allows us to perform the pose update once we determine its parameters, which are the wheel radii, which we assume identical, and the distance between the wheels, or the baseline.</p> <p>Parameters</p> <ul> <li>\\({\\color{green}{R}}\\): wheel radius</li> <li>\\(2 {\\color{green}{L}}\\): baseline (distance between wheels)</li> </ul> <p>What's also needed is to measure the wheel angular velocities.</p> <p>Measurements</p> <ul> <li>\\(\\color{red}{\\dot{\\phi}_{l,t}}\\): wheel angular speeds</li> </ul>"},{"location":"theory/modeling-control/odometry/#wheel-encoders","title":"Wheel Encoders","text":"<p>To measure the wheel angular velocities, we can use wheel encoders. Although there are various implementations, the operation principle of wheel encoders is simple.</p> <p>The outer rim of the motor's rotor has some evenly spaced, fixed markers. Optical encoders, for example, have a perforated disk where the empty space are the markers.</p> <p>TODO</p> <p>Add encoder wheel image robotc.net</p> <ul> <li>One pulse every \\({\\color{red}{\\alpha}} = \\frac{2\\pi}{N_{tot}}\\) radians</li> </ul> <p>Every time any of these markers transitions through some reference position on the stator,  the marker is sensed, and a signal is produced.</p> <p>For example, in optical encoders, a light sensor will pick up the light from a source  every time an empty space in the disk comes about.</p> <p>Knowing how many markers there are in a whole circumference,  we can derive how much each wheel rotated by just counting the pulses in each of the k-th time interval.</p> <ul> <li>Wheel rotation (in \\(\\Delta t_k\\)): \\({\\color{orange}{\\Delta \\phi_k}} = N_k \\cdot {\\color{red}{\\alpha}}\\)</li> </ul> <p>By dividing the total rotation by delta t, we can then measure the average wheel angular speed in that time frame.</p> <ul> <li>Angular speed: \\(\\dot{\\phi}_{t_k} \\approx \\frac{ {\\color{orange}{\\Delta \\phi_k}} }{\\Delta t_k}\\)</li> </ul> <p>Expanding the kinematics model expressions, we gain insight on the pose update process.</p> \\[ \\begin{align} &amp;&amp;{\\color{orange}{\\dot{q}_{t}}} &amp;= \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} =  \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) &amp; 0 \\\\ sin(\\theta_t) &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}1 &amp; 1 \\\\ \\frac{1}{ \\color{green}{L} } &amp; -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} {\\color{red}{\\longleftarrow}} {\\color{red}{\\dot{\\phi}_{t_k}}} \\approx \\frac{ \\Delta \\phi_k }{\\Delta t_k} \\\\ &amp;&amp;{\\color{orange}{\\downarrow}} \\\\ &amp;&amp; {\\color{green}{q_{t_{k+1}}}} \\approx q_{t_k} + &amp;{\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\\\ &amp;&amp;{\\color{green}{\\downarrow}} \\\\ &amp;&amp;{\\color{green}{x_{t_{k+1}}}} \\approx x_{t_k} + &amp; \\frac{R}{2 \\cancel{\\Delta t_k}} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\cos(\\theta_{t_k}) \\cancel{\\Delta t_k} \\\\ &amp;&amp;{\\color{green}{y_{t_{k+1}}}} \\approx y_{t_k} + &amp; \\frac{R}{2} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\sin(\\theta_{t_k}) \\\\ &amp;&amp;{\\color{green}{\\theta_{t_{k+1}}}} \\approx \\theta_{t_k} + &amp; \\frac{R}{2L} \\left(\\Delta \\phi_{r,t} - \\Delta \\phi_{l,t} \\right) \\\\ \\end{align} \\] <p>Notice how the time intervals cancel out, so we don't need to actually compute the angular speed of each wheel,  but just the total rotation.</p> <p>The first step in solving the odometry is transforming the wheel rotations into traveled distances for each wheel. We count the pulses from the encoders, derive the rotation of each wheel,  and then multiply by the radius of each wheel.</p> \\[ \\begin{align} x_{t_{k+1}} &amp;\\approx x_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &amp;\\approx y_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &amp;\\approx \\theta_{t_k} + \\frac{ {\\color{orange}{R}} }{2L} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} - {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\\\ \\end{align} \\] <p>Wheel travelled distance: \\(\\color{orange}{d_{l/r}} = R \\cdot \\Delta \\phi_{r/l}\\)</p> <p>TODO</p> <p>Add robot image with \\(d_l\\) \\(d_r\\) travelled distance</p> <p>The second step is to transform the wheel displacements into the linear and the angular displacements of the robot reference frame,  as we have seen in the modeling section.</p> \\[ \\begin{align} x_{t_{k+1}} &amp;\\approx x_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &amp;\\approx y_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &amp;\\approx \\theta_{t_k} + {\\color{red}{ \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} }} \\\\ \\end{align} \\] <p>TODO</p> <p>Add robot image with \\(d_l\\) \\(d_r\\) travelled distance and \\(\\Delta \\theta\\)</p> <p>The final step is to represent the displacement in the world frame and add the increments to the previous estimates.</p> \\[ \\begin{align} x_{t_{k+1}} &amp;\\approx x_{t_k} + {\\color{orange}{ d_{A,t_k} \\cos(\\theta_{t_k}) }} \\\\ y_{t_{k+1}} &amp;\\approx y_{t_k} + {\\color{orange}{ d_{A,t_k} \\sin(\\theta_{t_k}) }} \\\\ \\theta_{t_{k+1}} &amp;\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] <p>TODO</p> <p>Add robot image showing increment update in world frame</p>"},{"location":"theory/modeling-control/odometry/#summary-of-odometry-equations","title":"Summary of Odometry Equations","text":"\\[ \\begin{align} x_{t_{k+1}} &amp;\\approx x_{t_k} + d_{A,t_k} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &amp;\\approx y_{t_k} + d_{A,t_k} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &amp;\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] \\[ \\begin{align} d_{A,t_k} &amp;= \\frac{ d_{r,t_k} + d_{l,t_k} }{2} \\\\ \\Delta \\theta_{t_k} &amp;= \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} \\\\ d_{r/l,t_k} &amp;= 2\\pi R \\frac{N_k}{N_{tot}} \\end{align} \\]"},{"location":"theory/modeling-control/odometry/#challenges-in-odometry","title":"Challenges in Odometry","text":"<p>There are practical challenges in odometry.</p>"},{"location":"theory/modeling-control/odometry/#dead-reconing","title":"\"Dead reconing\"","text":"<p>The first practical challenge stems from using this dead reckoning approach,  which is the official name of always adding an increment to a previous estimate in order to obtain a new one.</p> \\[ \\begin{align} x_{t_{k+1}} &amp;\\approx x_{t_k} + \\Delta x_{t_k} \\\\ y_{t_{k+1}} &amp;\\approx y_{t_k} +  \\Delta y_{t_k} \\\\ \\theta_{t_{k+1}} &amp;\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] <p>TODO</p> <p>Reuse odometry update image in world frame</p> <p>While it might work well for short distances, over time, errors like the discrete time approximation will accumulate,  making the estimate drift from reality.</p>"},{"location":"theory/modeling-control/odometry/#kinematic-model","title":"Kinematic Model","text":"<p>Second, we're using a mathematical model, that of the kinematics of a differential drive robot, to translate the actual measurements to the pose of the robot.</p> \\[ \\dot{q}_{t} =  \\frac{R}{2} \\begin{bmatrix}cos(\\theta_t) &amp; 0 \\\\ sin(\\theta_t) &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}1 &amp; 1 \\\\ \\frac{1}{L} &amp; -\\frac{1}{L} \\end{bmatrix} \\begin{bmatrix}\\dot{\\phi}_{r,t} \\\\ \\dot{\\phi}_{l,t} \\end{bmatrix} \\] <p>You might recall that we previously said that all models are wrong, although some are useful. This wisdom is ever more true when the assumptions of the model are not respected.</p>"},{"location":"theory/modeling-control/odometry/#wheel-slip","title":"Wheel Slip","text":"<p>In particular, we impose the condition of no slippage of the wheels.</p> <p>TODO</p> <p>Add wheel slip images/animation</p> <p>When the wheels slip, it means that the motor will be spinning, the encoders will be producing measurements, but the robot will not be moving the same distance as we are assuming. This will induce errors in the odometry, and they will compound over time.</p>"},{"location":"theory/modeling-control/odometry/#odometry-calibration","title":"Odometry Calibration","text":"<p>Finally, we need to use some actual numerical values for the parameters of the model: the wheel radii - which, by the way, are assumed to be identical, but will they really be - and the robot baseline.</p> \\[ \\begin{align} \\Delta \\theta_{t_k} &amp;= \\frac{ d_{r,t_k} - d_{l,t_k} }{2 {\\color{red}{L}} } \\\\ d_{r/l,t_k} &amp;= 2\\pi {\\color{red}{R}} \\frac{N_k}{N_{tot}} \\end{align} \\] <p>Accurately measuring these parameters is very important. Even small imperfections will induce systematic errors in the odometry that, again, will compound over time.</p> <p>Note that although nominally identical, no two real-world physical robots will ever be the same due to manufacturing, assembly, or handling differences.</p> <p>To find the values of the parameters of the model that best fit our robot, we will need to perform an odometry calibration procedure.</p>"},{"location":"theory/modeling-control/odometry/#summary","title":"Summary","text":"<ul> <li> <p>Wheel encoders can be used to update the robot's pose in time:</p> <ol> <li>Measure the motor's angular displacements \\(\\Delta \\phi\\) in \\(\\Delta t\\)</li> <li>Use the kinematics mdoel to find the robot's \\(\\Delta x\\), \\(\\Delta y\\), \\(\\Delta \\theta\\)</li> <li>Update the pose by adding the calculated increments</li> </ol> </li> <li> <p>Subject to dfit in time due to accumulation of numerical, slipping/skidding and calibration impercision errors.</p> </li> </ul>"},{"location":"theory/modeling-control/robot-representations/","title":"Robot Representations","text":"<p>Representations of the robot and its environment are fundamental to the capabilities that make a vehicle autonomous.</p> <p>To sense, to plan, and to act. Different tasks might require different representations. For example, navigating the city or avoiding a pedestrian on the road.</p>"},{"location":"theory/modeling-control/robot-representations/#state","title":"State","text":"<p>To quantify representations, we use states. The state \\(x_t\\) of a robot and of the world has the following properties</p> <ol> <li>\\(x_t \\in X\\) exists independently of us and the algorithms that we choose to determine it.</li> <li> <p>The state evolves over time,</p> \\[ \\underbrace{x_0, x_1, \\dots,}_{\\color{orange}Past} \\quad \\overbrace{x_t}^{\\color{green}Present} \\quad \\underbrace{x_{t+1}, x_{t+n}, \\dots}_{\\color{red}Future} \\] </li> <li> <p>the robot will need to estimate the \\(\\color{green}present\\) and \\(\\color{red}future\\) state on the fly,   so it should be efficiently computable. </p> \\[ {\\color{red}x_{t+1}} =  f(\\color{green}x_t, \\color{orange}x_{t-1}, \\dots, x_0; \\color{green}u_t, \\dots, \\color{orange}u_0) \\] </li> </ol> <p>Good choices of state are such that given the present information, the future state is independent of the past. This is called Markov property,  and it's very desirable because it allows the robot not having to keep track of all the information gathered in the past.</p> <p>The state is typically observed from the sensor measurements,  but taking the whole history of the measurements as choice of a state is inefficient,  because measurements contain redundant information and increase over time,  so they require more and more computation and memory to process.</p>"},{"location":"theory/modeling-control/robot-representations/#robot-pose","title":"Robot Pose","text":"<p>A sufficient and efficient representation of a mobile robot is the pose \\(q_t\\).</p> <p>Pose Definition</p> <p>\\(q_t\\): position and orientation of the \\({\\color{orange}\\text{robot}}\\) (\\({\\color{orange}\\text{body}}\\)) frame relative to a \\({\\color{red}\\text{world}}\\) (\\({\\color{red}\\text{fixed}}\\)) reference frame.</p> <p>That is, the position and the orientation of the robot in space. The pose may also include the linear and the angular velocities. The environment of a mobile robot can be seen as a 2D world, but pose can be generalized to 3D as well.</p> <p>TODO</p> <p>Add reference frame image</p>"},{"location":"theory/modeling-control/robot-representations/#reference-frames-mathbbr2","title":"Reference Frames (\\(\\mathbb{R}^2\\))","text":"<p>To formalize a robot's pose, we need to introduce reference systems. We take a world frame with origin in W and a robot, or body, frame,  which moves with the robot and has origin in point A at position (x,y) in the world frame.</p> <ul> <li>\\({\\color{red}\\text{World frame }}(x^w, y^w)\\), origin in \\(W\\)</li> <li>\\({\\color{orange}\\text{Robot frame }}(x^r, y^r)\\), origin in \\(A\\) orientation \\(\\theta\\) with \\(x^w\\)</li> </ul> <p>TODO</p> <p>Add reference frames image</p> <p>It is important to express the coordinates of any point with respect to the robot and the world frames, which in the general case are rotated and translated one with respect to the other.</p>"},{"location":"theory/modeling-control/robot-representations/#moving-between-reference-frames","title":"Moving between Reference Frames","text":"<p>Let's look at the math on how to move between frames,  starting from the simpler case of translations.</p> <p>TODO</p> <p>Add points frames image</p>"},{"location":"theory/modeling-control/robot-representations/#translations","title":"Translations","text":"<p>Take two reference frames and assume that they are purely translated with respect to each other by \\(x_A\\) and \\(y_A\\).</p>"},{"location":"theory/object-detection/introduction-to-neural-networks/","title":"Introduction to neural networks","text":""},{"location":"theory/object-detection/introduction-to-neural-networks/#introduction-to-neural-networks","title":"Introduction to Neural Networks","text":"<p>Convolutional neural networks can learn features through supervised learning and can be categorized into three different types:</p> <ol> <li>Classification </li> <li>Object detection</li> <li>Segmentation</li> </ol> <p>The underlying basis of all these neural networks is the convolutional layer.</p> <p>A classification CNN takes in an input image and outputs a distribution of class scores. This is done by feeding the input image through convolutional layers, which is the backbone of a CNN. These layers are used to filter the input image, and the filters are also known as convolution kernels. A kernel has a certain (small) size and one or more channels, specifying its depth. Each filter extracts different features from an image, such as edges or colors.  The output of a given convolutional layer is a set of feature maps (also called activation maps), which are filtered versions of an original input image. As a CNN trains it updates the weights that define the image filters in this convolutional layer using back propagation.</p>"},{"location":"theory/object-detection/introduction-to-neural-networks/#activation-function","title":"Activation Function","text":"<p>Activation functions, such as ReLU's, are typically placed after a convolutional layer to slightly  transform the output so that it's more efficient to perform backpropagation and effectively train the network.</p>"},{"location":"theory/object-detection/introduction-to-neural-networks/#references","title":"References","text":"<ul> <li>Self-Driving Cars with Duckietown</li> <li>https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</li> <li>Gradient Descent</li> <li>Yes you should understand backprop by Andrej Karpathy</li> <li>Hacker's guide to Neural Networks</li> <li>CS231n taught by Andrej Karpathy</li> </ul>"},{"location":"theory/preliminaries/linear-algebra/","title":"Linear Algebra","text":""},{"location":"theory/preliminaries/linear-algebra/#linear-algebra-basics","title":"Linear Algebra Basics","text":"<ul> <li>Vectors, matrices</li> <li>Matrix vector multiplication</li> <li>Jacobians</li> </ul>"},{"location":"theory/preliminaries/probability/","title":"Probability","text":""},{"location":"theory/preliminaries/probability/#probability-basics","title":"Probability Basics","text":""},{"location":"theory/preliminaries/probability/#random-variables","title":"Random Variables","text":""},{"location":"theory/preliminaries/probability/#probability-density-function","title":"Probability Density Function","text":""},{"location":"theory/preliminaries/probability/#normal-distribution-and-gaussian-function","title":"Normal Distribution and Gaussian Function","text":""},{"location":"theory/preliminaries/probability/#multivariate-distribution","title":"Multivariate Distribution","text":""},{"location":"theory/preliminaries/probability/#joint-distribution","title":"Joint Distribution","text":"<p>If we have two different RVs representing two different events \\(X\\) and \\(Y\\),  then we represent the probability of two distinct events \\(x \\in \\mathcal{X}\\) and \\(y \\in \\mathcal{Y}\\) both happening, which we will denote as follows:</p> \\[ p(X=x AND Y=y) = p(x,y) \\] <p>The function \\(p(x,y)\\) is called joint distribution.</p>"},{"location":"theory/preliminaries/probability/#independence","title":"Independence","text":""},{"location":"theory/preliminaries/probability/#conditional-probability","title":"Conditional Probability","text":""},{"location":"theory/preliminaries/probability/#theorem-of-total-probability-and-marginal-distribution","title":"Theorem of Total Probability and Marginal Distribution","text":""},{"location":"theory/preliminaries/probability/#bayes-rule","title":"Bayes Rule","text":""},{"location":"theory/preliminaries/probability/#conditional-independence","title":"Conditional Independence","text":""},{"location":"theory/preliminaries/probability/#moments-of-rvs","title":"Moments of RVs","text":""},{"location":"theory/preliminaries/probability/#entropy","title":"Entropy","text":""},{"location":"theory/preliminaries/probability/#resources","title":"Resources","text":"<ul> <li>Duckietown Preliminaries</li> <li>Probabilistic Robotics, Thrun, Burgard, Fox</li> <li>Papoulis: Probability, Random Variables, Stochastic Processes</li> </ul>"},{"location":"theory/preliminaries/programming/","title":"Programming","text":""},{"location":"theory/preliminaries/programming/#programming","title":"Programming","text":""},{"location":"theory/robot-vision/camera-calibration/","title":"Camera Calibration","text":""},{"location":"theory/robot-vision/camera-calibration/#camera-calibration","title":"Camera Calibration","text":"<p>As mentioned previously, the pin-hole camera model describes a light-proof box with a small aperture that allows a limited amount of light reflecting off of objects in the scene to pass through and strike the sensor plane. The result is an idealized, yet surprisingly useful approximation of the cameras commonly used on a variety of robots.</p> <p>The model describes how points in the world are mapped to image-space coordinates. This is done, by first transorming points given in world coordinates into the camera's reference frame, and then projecting the points onto the image plane.  In order to use this model, we need to know its parameters, such as the focal length \\(f\\) and skew \\(s\\) of the camera (intrinsic parameters), and its pose relative to the frame of the world or the robot (extrinsic parameters). Calibration refers to the process of estimating these parameters.</p> <p>Note</p> <p>TODO missing transformation image</p> \\[ \\mathbf{x} = \\underbrace{\\begin{bmatrix}        f_x &amp; s &amp; p_x \\\\       0   &amp; f_y &amp; p_y \\\\       0   &amp; 0   &amp; 1 \\\\      \\end{bmatrix}}_{\\textbf{intrinsics} \\\\ \\text{5 DOF + lens distortion}}      \\underbrace{\\begin{bmatrix}       R | t      \\end{bmatrix}}_{\\text{\\textbf{extrinsics}} \\\\ \\text{6 DOF}}      \\mathbf{X} = P \\mathbf{X} \\] <p>Having a mathematical expression for perspective projection allows us to reason over a robot's three-dimensional world from two-dimensional images.</p> <p>Suppose an algorithm that detects lane markings in an image received from the robot's camera. In order to use these detections to keep the robot in its lane, it would be useful to understand where tehy are in relation to the robot.  As we will see, calibrating the camera allows us to transform these detections into the robot's reference frame. The result can then be used to understand the lane geometry and where the robot is relative to the lane.</p> <p>There are different approaches to calibrating a camera. The pin-hole camera model can be represendet as a product of two matrices, one defined in terms of up to five parameters intrinsic to the camera, and the other specifying the camera six degrees of freedom pose. Ignoring lens distortions, there is a total of 11 parameters that define the camera model.</p> <p>Nonlinear intrinsic parameters such as lens distortion are also important although they cannot be included in the linear camera model described by the intrinsic parameter matrix.  Many modern camera calibration algorithms estimate these intrinsic parameters as well in the form of non-linear optimisation techniques. This is done in the form of optimising the camera and distortion parameters in the form of what is generally known as bundle adjustment.<sup>1</sup></p> <p>The calibration process typically involves associating the coordinates of points in the scene with their corresponding projections onto the image plane. Each correspondence \\(\\mathbf{x}_i \\leftrightarrow \\mathbf{X}_i\\) provides two constraints on the projection model. One for each of the two image coordinates.</p> <p>With 11 degrees-of-freedom in the model, we need at least six point-pairs but have to be careful about avoiding degeneracies. For example, the three points can't all lie on the same line or the same plane.</p> <p>In practice, we use a calibration target that provides a well-defined set of 3D points that are easy to detect in the image,  often involving one or more checkerboard patterns. One option for calibrating the camera is to directly estimate the entries of the three-by-four camera matrix P that maps 3D scene points to their 2D image coordinates. While the matrix has 12 entries, there are only 11 degrees-of-freedom, since perspective projection is only defined up to scale.</p>"},{"location":"theory/robot-vision/camera-calibration/#calibration-via-direct-linear-transformation","title":"Calibration via Direct Linear Transformation","text":"<p>The direct linear transform is a 11 DOF transformation, taking the 6 DOF from the extrinsics and the 5 DOF from the intrinsics into account to describe a so called affine camera model. This is basically a camera which has a perfect lense, without distortions from, e.g. lenses. This is an approximation of the camera because all real world cameras have lens distortions.</p> <p>The camera parameters obtained through DLT can be computed using six or more control points, which are points with known coordinates in the environment. By picturing those points we can estimate what are the intrinsics of the camera and where that camera is.</p> <p>In practice there are addiontal non-linear parameters to account for lens distortions:</p> <ul> <li>Radial Distortion</li> <li>barrel distortion</li> <li>cushion distortion</li> <li>Tangential Distortion</li> </ul>"},{"location":"theory/robot-vision/camera-calibration/#calibration-as-an-optimization-problem","title":"Calibration as an Optimization Problem","text":""},{"location":"theory/robot-vision/camera-calibration/#homographies-and-homography-estimation","title":"Homographies and Homography Estimation","text":""},{"location":"theory/robot-vision/camera-calibration/#references","title":"References","text":"<ul> <li>Self-Driving Cars with Duckietown</li> <li>Mathworks Camera Calibration</li> <li>Pin-hole Camera Calibration with Matlab Toolbox Documentation</li> <li>OpenCV Camera Calibration and 3D Reconstruction</li> <li>ROS Camera Calibration</li> <li>ROS Wiki image_pipeline CameraInfo</li> <li>Z. Zhang. A Flexible New Technique for Camera Calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(11):1330-1334, 2000.</li> </ul>"},{"location":"theory/robot-vision/camera-calibration/#youtube","title":"Youtube","text":"<ul> <li>Camera Intrinsics and Extrinsics - 5 Minutes with Cyrill</li> </ul> <ol> <li> <p>Wikipedia Camera Resectioning \u21a9</p> </li> </ol>"},{"location":"theory/robot-vision/combining-camera-and-lidar/","title":"Combining Camera and Lidar","text":""},{"location":"theory/robot-vision/combining-camera-and-lidar/#combining-camera-and-lidar","title":"Combining Camera and LiDAR","text":"<p>The main problem with a single-sensor approach is its reduced reliability as each sensor has its weaknesses under certain situations. To improve the tracking process results we can make use of both, camera and LiDAR sensors, and can combine the data from these sensors.</p> <p>The first step in the fusion process will be to combine the tracked feature points within the camera images with the 3D Lidar points.  For this we use homogeneous coordinates and the transformation matrices related to cameras to geometrically project the LiDAR points  into the camera in such a way that we know the position of each 3D Lidar point on the image sensor.</p>"},{"location":"theory/robot-vision/combining-camera-and-lidar/#project-lidar-points-into-the-camera-image","title":"Project LiDAR Points into the Camera Image","text":"<p>The basic steps to project LiDAR points into the camera image are the following:</p> <ol> <li>Preparation    a. Make sure to synchronize all your LiDAR measurements to the exact recording timestamp of the camrea.    b. Typically the camera is moving, which is why we need to compensate for this motion artifact. This is because a typical laser takes some short amount of time    to scan the the entire view around it (usually 100 milliseconds). In this time the robot traveled a bit, which needs to be taken into account,    so that the LiDAR point falls into the correct location in the image plane.</li> <li>Convert LiDAR points into homogeneous coordinates.</li> <li>Map (transform) all those points onto the image plane using the intrinsic camera matrix and extrinsic camera matrix that relates the camera frame to the LiDAR frame.</li> <li> <p>Move the LiDAR points \\(x\\) and \\(y\\) back from homogeneous coordinates to the Euclidean coordinate system to get the position where the LiDAR point hits the image plane in pixel coordinates.</p> </li> <li> <p>Homogeneous coordinates</p> </li> <li>Euclidean to homogeneous</li> <li>Homogeneous to Euclidean</li> </ol>"},{"location":"theory/robot-vision/combining-camera-and-lidar/#intrinsic-parameters","title":"Intrinsic Parameters","text":""},{"location":"theory/robot-vision/combining-camera-and-lidar/#extrinsic-parameters","title":"Extrinsic Parameters","text":""},{"location":"theory/robot-vision/combining-camera-and-lidar/#references","title":"References","text":""},{"location":"theory/robot-vision/combining-camera-and-lidar/#papers","title":"Papers","text":"<ul> <li>PointPainting: Sequential Fusion for 3D Object Detection by Vora et. al., 2020</li> <li>3D LIDAR\u2013camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization by Mirzaei et. al., 2012</li> <li>LiDAR-Camera Calibration using 3D-3D Point correspondences</li> </ul>"},{"location":"theory/robot-vision/features-and-segmentation/","title":"Features and Segmentation","text":""},{"location":"theory/robot-vision/features-and-segmentation/#features","title":"Features","text":"<p>Features and feature extraction form the basis for many computer vision applications and it is an important task in computer vision to select the right features. The idea is to represent the content of an image as a piece of information, named a feature. Thus defining a set of data,  such as a set of images, as a smaller, simpler model made of a combination of visual features: a few colors and shapes. Features as measurable pieces of data in an image that help distinguish between different classes of images.</p> <p>There are two main types of features, which can be useful individually or in combination:</p> <ol> <li>Color-based and</li> <li>Shape-based</li> </ol> <p>Edges are one of the simplest shapes that you can detect;  edges often define the boundaries between objects but they may not provide enough information to find and identify small features on those objects.</p>"},{"location":"theory/robot-vision/features-and-segmentation/#types-of-features","title":"Types of Features","text":"<ol> <li>Edges: Areas with a high intensity gradient</li> <li>Corners: Intersection of two edges</li> <li>Blobs: Region based features; areas of extreme brightness or unique texture</li> </ol> <p>Corners are the most repeatable feature, which means they are easy to recognize given two or more images of the same scene. Corners match exactly and are good features, therefore we are interested in finding this type of features.</p>"},{"location":"theory/robot-vision/features-and-segmentation/#segmentation","title":"Segmentation","text":"<ul> <li>Contours in OpenCV</li> </ul>"},{"location":"theory/robot-vision/image-processing/","title":"Image Processing","text":""},{"location":"theory/robot-vision/image-processing/#image-filtering","title":"Image Filtering","text":"<p>The concept of (spacial) frequency in images is important to filter certain frequencies and thus enhance or dampen certain features of an image, such as corners or edges.</p>"},{"location":"theory/robot-vision/image-processing/#frequency-in-images","title":"Frequency in Images","text":"<p>As with 1-D signals, frequency in images describes a rate of change in pixel intensities.</p> <p>The Fourier Transform (FT) is an important image processing tool which is used to decompose an image into its frequency components.  The output of an FT represents the image in the frequency domain, while the input image is the spatial domain (x, y) equivalent.</p> <ul> <li>OpenCV Fourier Transform</li> </ul>"},{"location":"theory/robot-vision/image-processing/#linear-filtering-and-convolution","title":"Linear Filtering and Convolution","text":"<ul> <li>OpenCV Filtering Tutorial</li> </ul>"},{"location":"theory/robot-vision/image-processing/#gradient-and-sobel-filter","title":"Gradient and Sobel Filter","text":"<p>Gradients are a measure of intensity change in an image. As images are treated as functions \\(I(x,y)\\),  the gradient is the derivative of this function \\(I'(x,y)\\), describing the change in intensity \\(\\Delta I\\) at pixel locations \\(x\\) and \\(y\\).</p> <p>The Sobel filter is very commonly used in edge detection and in finding patterns in intensity in an image.  Applying a Sobel filter to an image is a way of taking (an approximation) of the derivative of the image in the \\(x\\) or \\(y\\) direction.</p> \\[ S_x = \\begin{matrix}         -1 &amp; 0 &amp; 1 \\\\         -2 &amp; 0 &amp; 2 \\\\         -1 &amp; 0 &amp; 1 \\\\       \\end{matrix} \\] \\[ S_y = \\begin{matrix}         -1 &amp; -2 &amp; -1 \\\\         0 &amp; 0 &amp; 0 \\\\         1 &amp; 2 &amp; 1 \\\\       \\end{matrix} \\] <p>Note</p> <p>TODO add image examples</p> <ul> <li>Magnitude: \\(S = \\sqrt{S_x^2 + S_y^2}\\)</li> <li>Direction: \\(\\theta = \\atan2{S_y, S_x}\\)</li> </ul> <p>For more details see also the Sobel operator on Wikipedia.</p>"},{"location":"theory/robot-vision/image-processing/#image-blurring-and-low-pass-filter","title":"Image Blurring and Low-pass Filter","text":"<p>To block noise in an image, use a filter that filters high frequencies, e.g. specle or discoloration, and let low frequency components of an image pass, such as smooth surfaces.</p> <p>Without blurring the image and therefore removing the high-frequnecy noise, a preceding high-pass filtering step would also amplify the noise. To enhance only the high-frequency edges, it is common to first apply a low-pass filter, which basically takes an average of neighbouring pixels.</p> <p>A simple filter to blur or low-pass filter an image is the normalized average filter:</p> \\[ 1/9 \\begin{matrix}      1 &amp; 1 &amp; 1 \\\\      1 &amp; 1 &amp; 1 \\\\      1 &amp; 1 &amp; 1 \\\\     \\end{matrix} \\] <p>Note</p> <p>Normalization is important to avoid brightening the image and therefore maintain the same energy. For the averiging filter the pixel values sum to one: \\(1/9 (1+1+1 + 1+1+1 + 1+1+1)=1\\)</p>"},{"location":"theory/robot-vision/image-processing/#gaussian-kernels","title":"Gaussian Kernels","text":"<p>Gaussian kernels are used to blur/smooth an image and therefore block high frequency parts of an image. Compared to an averageing filter, Gaussian kernels better preserve edges.</p> <p>The gaussian kernel can be seen as a weighted average, which gives the most weight to the center pixel.</p> \\[ 1/16 \\begin{matrix}      1 &amp; 2 &amp; 1 \\\\      2 &amp; 4 &amp; 2 \\\\      1 &amp; 2 &amp; 1 \\\\     \\end{matrix} \\] <p>As with the average filter, the values of the gaussian filter sum to one \\(1/16 (1+2+1 + 2+4+2 + 1+2+1)=1\\). </p> <p>Note</p> <p>An even better method to preserve edges are bilateral filters. It is highly effective in noise removal while keeping edges sharp. But the operation is slower compared to other filters.</p> <p>Read more about the math behind Gaussian filters on Wikipedia  and also see the OpenCV documentation of the <code>GaussianBlur</code> function.</p>"},{"location":"theory/robot-vision/image-processing/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>Convolutional neural networks consist of convolutional layers which are made up of similar filters defined previously. The only difference is that neural networks learn to create filters themselfes through gradient descent and error back propagation. The weights of a convolutional neural network that it updates during training are the values inside the filter kernels.</p> <p>In deep convolutional neural networks the individual filter kernels are stacked which increases the depth of a convolutional filter (or kernel),  similar to the channels of a color image are stacked. Using stacked filters yields more features from an input image but also increases the number of learnable weights (filter values). In traditional filters the values of filter weights were set explicitly,  but neural networks will actually learn the best filter weights as they train on a set of image data.</p>"},{"location":"theory/robot-vision/image-processing/#canny-edge-detection","title":"Canny Edge Detection","text":"<p>OpenCV provides the <code>Canny</code> edge detector, which  is a widely used edge detection algorithm that performs the following steps:</p> <ol> <li>Filters out noise using a Gaussian blur</li> <li>Finds the strength and direction of edges using Sobel filters</li> <li>Applies non-maximum suppression to isolate the strongest edges and thin them to one-pixel wide lines.</li> <li>Uses hysteresis to isolate the best edges using low and high thresholds to cut/pass intensity values.</li> </ol> <p>Canny edge detection eliminates weak edges and noise and isolates edges that are part of an object boundary. Read more about the Canny edge detection algorithm on Wikipedia.</p>"},{"location":"theory/robot-vision/image-processing/#shape-detection","title":"Shape Detection","text":"<p>So far we defined image filters for smoothing images and detecting the edges (high-frequency) components of objects in an image.  Using this knowledge about pattern recognition in images enables one to begin identifying unique shapes and then objects.</p> <p>Edges to Boundaries and Shapes To find unifying boundaries around objects to separate and locate multiple objects in a given image, the Hough transform can be used. It transforms image data from the x-y coordinate system into Hough space, where you can easily identify simple boundaries like lines and circles.</p> <p>The Hough transform is used in a variety of shape-recognition applications. A Hough transform can find the edges of driving lanes.</p> <p>See this resource on Hough transform and also the  Wikipedia article about it. An example is found in the OpenCV Hough lines tutorial</p>"},{"location":"theory/robot-vision/image-processing/#references","title":"References","text":"<ul> <li>Self-driving cars with Duckietown</li> <li>OpenCV Fourier Transform</li> <li>OpenCV Filtering tutorial</li> <li>Wikipedia Sobel operator</li> </ul>"},{"location":"theory/robot-vision/pinhole-camera-model/","title":"Pinhole Camera Model","text":""},{"location":"theory/robot-vision/pinhole-camera-model/#pinhole-camera-model","title":"Pinhole Camera Model","text":""},{"location":"theory/robot-vision/pinhole-camera-model/#pinhole-camera","title":"Pinhole Camera","text":""},{"location":"theory/robot-vision/pinhole-camera-model/#perspective-projection","title":"Perspective Projection","text":""},{"location":"theory/robot-vision/pinhole-camera-model/#resources","title":"Resources","text":"<ul> <li>Duckietown MOOC Edx</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/","title":"Tracking Image Features","text":""},{"location":"theory/robot-vision/tracking-image-features/#tracking-image-features","title":"Tracking Image Features","text":"<p>The task of tracking image features is very challenging because we want to identify and track reliable features in an image, also known as keypoints, through a sequence of images.</p>"},{"location":"theory/robot-vision/tracking-image-features/#corner-detectors","title":"Corner Detectors","text":"<p>A corner can be located by following these steps:</p> <ul> <li>Calculate the gradient for a small window of the image, using sobel-x and sobel-y operators (without applying binary thesholding).</li> <li>Use vector addition to calculate the magnitude and direction of the total gradient from these two values.</li> <li>Apply this calculation as you slide the window across the image, calculating the gradient of each window. When a big variation in the direction &amp; magnitude of the gradient has been detected - a corner has been found!</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#intensity-gradient-and-filtering","title":"Intensity Gradient and Filtering","text":""},{"location":"theory/robot-vision/tracking-image-features/#harris-corner-detector","title":"Harris Corner Detector","text":"<ul> <li>OpenCV Harris tutorial</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#overview-of-popular-keypoint-detectors","title":"Overview of Popular Keypoint Detectors","text":"<p>Depending on which type of keypoints we want to detect there are a variety of different keypoint detection algorithms. There are four basic transformation types we need to think about when selecting a suitable keypoint detector:</p> <ol> <li>Rotation</li> <li>Scale change</li> <li>Intensity change</li> <li>Affine transformation</li> </ol> <p>Applications of keypoint detection include such things as object recognition and tracking, image matching and panoramic stitching as well as robotic mapping and 3D modeling. In addition to invariance under the transformations mentioned above, detectors can be compared for their detection performance and their processing speed.</p> <p>The Harris detector along with several other \"classics\" belongs to a group of traditional detectors, which aim at maximizing detection accuracy.  In this group, computational complexity is not a primary concern. The following list shows a number of popular classic detectors :</p> <ul> <li>1988 Harris Corner Detector (Harris, Stephens)</li> <li>1996 Good Features to Track (Shi, Tomasi)</li> <li>1999 Scale Invariant Feature Transform (Lowe)</li> <li>2006 Speeded Up Robust Features (Bay, Tuytelaars, Van Gool)</li> </ul> <p>In recent years, a number of faster detectors have been developed which aim at real-time applications on smartphones and other portable devices.  The following list shows the most popular detectors belonging to this group:</p> <ul> <li>2006 Features from Accelerated Segment Test (FAST) (Rosten, Drummond)</li> <li>2010 Binary Robust Independent Elementary Features (BRIEF) (Calonder, et al.)</li> <li>2011 Oriented FAST and Rotated BRIEF (ORB) (Rublee et al.)</li> <li>2011 Binary Robust Invariant Scalable Keypoints (BRISK) (Leutenegger, Chli, Siegwart)</li> <li>2012 Fast Retina Keypoint (FREAK) (Alahi, Ortiz, Vandergheynst)</li> <li>2012 KAZE (Alcantarilla, Bartoli, Davidson)</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#features-from-accelerated-segments-test-fast","title":"Features from Accelerated Segments Test (FAST)","text":"<p>Finds keypoints by comparing the brightness levels in a given pixel area. Given a pixel \\(p\\) in an image, FAST compares the brigthness of \\(p\\) to a set of 16 surrounding pixels that are in a small circle around \\(p\\).</p> <p></p> <p>Each pixel in this circle is then sorted into three classes, depending on the brightness of the pixel \\(I_p\\) (intensity of pixel \\(p\\)):</p> <ol> <li>brighter than \\(p\\): </li> <li>darker than \\(p\\)</li> <li>similar to \\(p\\)</li> </ol> <p>So if the brithness of a pixel is \\(I_p\\), then for a given threshold \\(h\\) brither pixels will be those, whose brithness exceeds \\(I_p + h\\). Darker pixels will be those whose brithness is below \\(I_p - h\\), and similar pixels will be those whose brithness lie in-between those values. Once the pixels are classified into the three classes mentiond above,  pixel \\(p\\) is selected as a keypoint if more than eight connected pixels on the circle are either darker or brighter than \\(p\\).</p> <p>The reason FAST is so efficient, is that it takes advantage of the fact that the same result can be achieved by comparing \\(p\\) to only four equidistant pixels in the circle, instead of all 16 surrounding pixels. For example, we only have to compare \\(p\\) to pixels 1, 5, 9, and 13. In this case, \\(p\\) is selected as a keypoint if there are at least a pair of consecutive pixels that are either brighter or darker than \\(p\\). This optimization reduces the time required to search an entire image for keypoints by a factor of four.</p> <p>These keypoints are providing us with informatrion, about where in the image there is a change in intensity. Such regions usually determine an edge of some kind, which is why the keypoints found by FAST, give us information about the location of object defining edges in an image. However, one thing to note is that these keypoints only give us the location of an edge, and don't include any information about the direction of the change of intensity. So we can not distinguish between horizontal and vertical edges, for example. However, this directionality can be useful in some cases.</p> <p>Now that we know how ORB uses FAST to locates the key points in an image, we can look at how ORB uses the BRIEF algorithm to convert these keypoints into feature vectors, also known as keypoint descriptors.</p>"},{"location":"theory/robot-vision/tracking-image-features/#descriptors-feature-vectors","title":"Descriptors (Feature Vectors)","text":"<p>Descriptors also known als feature vectors provide distinctive information on the surrounding area of a keypoint. The literature differentiates between gradient-based descriptors and binary descriptors,  with the latter being a relatively new addition with the clear advantage of computational speed. </p> <ul> <li>An example of a gradient-based descriptor is the Scale Invariant Feature Transform (SIFT).</li> <li>A representative of binary descriptors is the Binary Robust Invariant Scalable Keypoints (BRISK).</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#hog-descriptors-sift-and-surf-etc","title":"HOG Descriptors (SIFT and SURF, etc.)","text":""},{"location":"theory/robot-vision/tracking-image-features/#binary-descriptors-brisk-orb-etc","title":"Binary Descriptors (BRISK, ORB, etc.)","text":""},{"location":"theory/robot-vision/tracking-image-features/#oriented-fast-and-rotated-brief-orb","title":"Oriented FAST and Rotated BRIEF (ORB)","text":"<p>ORB is a combination of FAST and BRIEF. FAST is a feature detection algorithm, while BRIEF is a feature vector creation algorithm.</p> <ol> <li>Keypoint detection using FAST:     ORB starts by detecting special regions in an image called keypoints, which is a small region in an image, that is particularly distinctive.    Such keypoints are for example corners, where the pixel values sharply change from light to dark.</li> </ol> <p>Note</p> <p>TODO image showing ORB features.</p> <ol> <li> <p>Description using BRIEF:     After locating keypoints in an image, ORB calculates a corresponding feature vector for each keypoint.    The ORB algorithm creates feature vectors that contain only ones and zeros, which is why they are called    binary feature vectors:</p> \\[ V1 = [01011100100110\\dots]\\\\ V2 = [10010100110100\\dots]\\\\ V3 = [11000100101110\\dots]\\\\ \\hdots \\] </li> </ol> <p>The sequence of ones and zeros varies, according to what a specific keypoint and its surrounding pixel area looks like. The vector represents the pattern of intensity around a keypoint. So multiple feature vectors can be used to identify a larger area and even a specific object in an image.</p> <p>ORB is not only incredible fast but it is also impervious to noise illumination, and image transformations such as rotations.</p>"},{"location":"theory/robot-vision/tracking-image-features/#references","title":"References","text":""},{"location":"theory/robot-vision/tracking-image-features/#text-books-and-papers","title":"Text Books and Papers","text":"<ul> <li>Peter Corke, Robotics, Vision, and Control. (Official website)    This book has a particular emphasis on computer vision for robotics, but as the title suggests, it goes beyond robot vision.</li> <li>Richard Szeliski, Computer Vision: Algorithms and Applications, Springer. The official website provides drafts of recent updates to the book.</li> <li>Richard Hartley and Andrew Zisserman, Multiple View Geometry,  Cambridge University Press. (Official website)</li> <li>David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Pearson. (Publisher's website)</li> <li>Evaluation of Several Feature Detectors/Extractors on Underwater Images towards vSLAM.</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#courses","title":"Courses","text":"<ul> <li>Udacity Sensor Fusion Engineer Nanodegree</li> <li>Stanford University, CS231A: Computer Vision, From 3D Reconstruction to Recognition (Course website)</li> <li>Georgia Tech, CS 6476: Computer Vision (Course website)</li> <li>MIT, 6.819/6.869: Advances in Computer Vision (Course website)</li> </ul>"},{"location":"theory/robot-vision/tracking-image-features/#online-resources","title":"Online Resources","text":"<ul> <li>OpenCV Tutorials (website)</li> <li>OpenCV Python Tutorials (website)</li> <li>Medium SIFT (Scale Invariant Feature Transform)</li> <li>Detecting lane lines on the road</li> <li>Wikipedia Features from Accelerated Segment Test</li> </ul>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/","title":"Robot Class","text":"In\u00a0[4]: Copied! <pre># import some resources\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n%matplotlib inline\n</pre> # import some resources import numpy as np import matplotlib.pyplot as plt import random %matplotlib inline In\u00a0[5]: Copied! <pre># the robot class\nclass robot:\n\n    # --------\n    # init: \n    #   creates a robot with the specified parameters and initializes \n    #   the location (self.x, self.y) to the center of the world\n    #\n    def __init__(self, world_size = 100.0, measurement_range = 30.0,\n                 motion_noise = 1.0, measurement_noise = 1.0):\n        self.measurement_noise = 0.0\n        self.world_size = world_size\n        self.measurement_range = measurement_range\n        self.x = world_size / 2.0\n        self.y = world_size / 2.0\n        self.motion_noise = motion_noise\n        self.measurement_noise = measurement_noise\n        self.landmarks = []\n        self.num_landmarks = 0\n\n\n    # returns a positive, random float\n    # returns random value between [-1.0, 1.0)\n    def rand(self):\n        return random.random() * 2.0 - 1.0\n\n\n    # --------\n    # move: attempts to move robot by dx, dy. If outside world\n    #       boundary, then the move does nothing and instead returns failure\n    #\n    def move(self, dx, dy):\n\n        x = self.x + dx + self.rand() * self.motion_noise\n        y = self.y + dy + self.rand() * self.motion_noise\n\n        if x &lt; 0.0 or x &gt; self.world_size or y &lt; 0.0 or y &gt; self.world_size:\n            return False\n        else:\n            self.x = x\n            self.y = y\n            return True\n    \n\n    # --------\n    # sense: returns x- and y- distances to landmarks within visibility range\n    #        because not all landmarks may be in this range, the list of measurements\n    #        is of variable length. Set measurement_range to -1 if you want all\n    #        landmarks to be visible at all times\n    #\n    \n    ## TODO: complete the sense function\n    def sense(self):\n        ''' This function does not take in any parameters, instead it references internal variables\n            (such as self.landamrks) to measure the distance between the robot and any landmarks\n            that the robot can see (that are within its measurement range).\n            This function returns a list of landmark indices, and the measured distances (dx, dy)\n            between the robot's position and said landmarks.\n            This function should account for measurement_noise and measurement_range.\n            One item in the returned list should be in the form: [landmark_index, dx, dy].\n            '''\n           \n        measurements = []\n        \n        ## TODO: iterate through all of the landmarks in a world\n        \n        ## TODO: For each landmark\n        ## 1. compute dx and dy, the distances between the robot and the landmark\n        ## 2. account for measurement noise by *adding* a noise component to dx and dy\n        ##    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise\n        ##    - Feel free to use the function self.rand() to help calculate this noise component\n        ##    - It may help to reference the `move` function for noise calculation\n        ## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range\n        ##    then we cannot record them; if they do fall in the range, then add them to the measurements list\n        ##    as list.append([index, dx, dy]), this format is important for data creation done later\n        \n        ## TODO: return the final, complete list of measurements\n        for index, landmark in enumerate(self.landmarks):\n            rx = self.x\n            ry = self.y\n            \n            lx = landmark[0]\n            ly = landmark[1]\n            \n            # 1. compute dx and dy, the distances between the robot and the landmark\n            # The measurements are with respect to robot's position, \n            # hence it's position has to be subtracted from the landmarks' position and not the other way around.\n            dx = lx - rx\n            dy = ly - ry\n            \n            # 2. account for measurement noise by *adding* a noise component to dx and dy\n            #    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise\n            #    - Feel free to use the function self.rand() to help calculate this noise component\n            #    - It may help to reference the `move` function for noise calculation\n            dx = dx + self.rand() * self.measurement_noise \n            dy = dy + self.rand() * self.measurement_noise\n            \n            # 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range\n            #    then we cannot record them; if they do fall in the range, then add them to the measurements list\n            #    as list.append([index, dx, dy]), this format is important for data creation done later\n            if abs(dx) &gt; self.measurement_range or abs(dy) &gt; self.measurement_range:\n                continue\n            \n            measurements.append([index, dx, dy])\n            \n        return measurements\n\n    \n    # --------\n    # make_landmarks: \n    # make random landmarks located in the world\n    #\n    def make_landmarks(self, num_landmarks):\n        self.landmarks = []\n        for i in range(num_landmarks):\n            self.landmarks.append([round(random.random() * self.world_size),\n                                   round(random.random() * self.world_size)])\n        self.num_landmarks = num_landmarks\n    \n    \n    # called when print(robot) is called; prints the robot's location\n    def __repr__(self):\n        return 'Robot: [x=%.5f y=%.5f]'  % (self.x, self.y)\n</pre> # the robot class class robot:      # --------     # init:      #   creates a robot with the specified parameters and initializes      #   the location (self.x, self.y) to the center of the world     #     def __init__(self, world_size = 100.0, measurement_range = 30.0,                  motion_noise = 1.0, measurement_noise = 1.0):         self.measurement_noise = 0.0         self.world_size = world_size         self.measurement_range = measurement_range         self.x = world_size / 2.0         self.y = world_size / 2.0         self.motion_noise = motion_noise         self.measurement_noise = measurement_noise         self.landmarks = []         self.num_landmarks = 0       # returns a positive, random float     # returns random value between [-1.0, 1.0)     def rand(self):         return random.random() * 2.0 - 1.0       # --------     # move: attempts to move robot by dx, dy. If outside world     #       boundary, then the move does nothing and instead returns failure     #     def move(self, dx, dy):          x = self.x + dx + self.rand() * self.motion_noise         y = self.y + dy + self.rand() * self.motion_noise          if x &lt; 0.0 or x &gt; self.world_size or y &lt; 0.0 or y &gt; self.world_size:             return False         else:             self.x = x             self.y = y             return True           # --------     # sense: returns x- and y- distances to landmarks within visibility range     #        because not all landmarks may be in this range, the list of measurements     #        is of variable length. Set measurement_range to -1 if you want all     #        landmarks to be visible at all times     #          ## TODO: complete the sense function     def sense(self):         ''' This function does not take in any parameters, instead it references internal variables             (such as self.landamrks) to measure the distance between the robot and any landmarks             that the robot can see (that are within its measurement range).             This function returns a list of landmark indices, and the measured distances (dx, dy)             between the robot's position and said landmarks.             This function should account for measurement_noise and measurement_range.             One item in the returned list should be in the form: [landmark_index, dx, dy].             '''                     measurements = []                  ## TODO: iterate through all of the landmarks in a world                  ## TODO: For each landmark         ## 1. compute dx and dy, the distances between the robot and the landmark         ## 2. account for measurement noise by *adding* a noise component to dx and dy         ##    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise         ##    - Feel free to use the function self.rand() to help calculate this noise component         ##    - It may help to reference the `move` function for noise calculation         ## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range         ##    then we cannot record them; if they do fall in the range, then add them to the measurements list         ##    as list.append([index, dx, dy]), this format is important for data creation done later                  ## TODO: return the final, complete list of measurements         for index, landmark in enumerate(self.landmarks):             rx = self.x             ry = self.y                          lx = landmark[0]             ly = landmark[1]                          # 1. compute dx and dy, the distances between the robot and the landmark             # The measurements are with respect to robot's position,              # hence it's position has to be subtracted from the landmarks' position and not the other way around.             dx = lx - rx             dy = ly - ry                          # 2. account for measurement noise by *adding* a noise component to dx and dy             #    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise             #    - Feel free to use the function self.rand() to help calculate this noise component             #    - It may help to reference the `move` function for noise calculation             dx = dx + self.rand() * self.measurement_noise              dy = dy + self.rand() * self.measurement_noise                          # 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range             #    then we cannot record them; if they do fall in the range, then add them to the measurements list             #    as list.append([index, dx, dy]), this format is important for data creation done later             if abs(dx) &gt; self.measurement_range or abs(dy) &gt; self.measurement_range:                 continue                          measurements.append([index, dx, dy])                      return measurements           # --------     # make_landmarks:      # make random landmarks located in the world     #     def make_landmarks(self, num_landmarks):         self.landmarks = []         for i in range(num_landmarks):             self.landmarks.append([round(random.random() * self.world_size),                                    round(random.random() * self.world_size)])         self.num_landmarks = num_landmarks               # called when print(robot) is called; prints the robot's location     def __repr__(self):         return 'Robot: [x=%.5f y=%.5f]'  % (self.x, self.y)  In\u00a0[6]: Copied! <pre>world_size         = 10.0    # size of world (square)\nmeasurement_range  = 5.0     # range at which we can sense landmarks\nmotion_noise       = 0.2      # noise in robot motion\nmeasurement_noise  = 0.2      # noise in the measurements\n\n# instantiate a robot, r\nr = robot(world_size, measurement_range, motion_noise, measurement_noise)\n\n# print out the location of r\nprint(r)\n</pre> world_size         = 10.0    # size of world (square) measurement_range  = 5.0     # range at which we can sense landmarks motion_noise       = 0.2      # noise in robot motion measurement_noise  = 0.2      # noise in the measurements  # instantiate a robot, r r = robot(world_size, measurement_range, motion_noise, measurement_noise)  # print out the location of r print(r) <pre>Robot: [x=5.00000 y=5.00000]\n</pre> In\u00a0[7]: Copied! <pre># import helper function\nfrom helpers import display_world\n\n# define figure size\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\n# call display_world and display the robot in it's grid world\nprint(r)\ndisplay_world(int(world_size), [r.x, r.y])\n</pre> # import helper function from helpers import display_world  # define figure size plt.rcParams[\"figure.figsize\"] = (5,5)  # call display_world and display the robot in it's grid world print(r) display_world(int(world_size), [r.x, r.y]) <pre>Robot: [x=5.00000 y=5.00000]\n</pre> In\u00a0[8]: Copied! <pre># choose values of dx and dy (negative works, too)\ndx = -1\ndy = -2\nr.move(dx, dy)\n\n# print out the exact location\nprint(r)\n\n# display the world after movement, not that this is the same call as before\n# the robot tracks its own movement\ndisplay_world(int(world_size), [r.x, r.y])\n</pre> # choose values of dx and dy (negative works, too) dx = -1 dy = -2 r.move(dx, dy)  # print out the exact location print(r)  # display the world after movement, not that this is the same call as before # the robot tracks its own movement display_world(int(world_size), [r.x, r.y]) <pre>Robot: [x=3.82654 y=3.10412]\n</pre> In\u00a0[9]: Copied! <pre># create any number of landmarks\nnum_landmarks = 3\nr.make_landmarks(num_landmarks)\n\n# print out our robot's exact location\nprint(r)\n\n# display the world including these landmarks\ndisplay_world(int(world_size), [r.x, r.y], r.landmarks)\n\n# print the locations of the landmarks\nprint('Landmark locations [x,y]: ', r.landmarks)\n</pre> # create any number of landmarks num_landmarks = 3 r.make_landmarks(num_landmarks)  # print out our robot's exact location print(r)  # display the world including these landmarks display_world(int(world_size), [r.x, r.y], r.landmarks)  # print the locations of the landmarks print('Landmark locations [x,y]: ', r.landmarks) <pre>Robot: [x=3.82654 y=3.10412]\n</pre> <pre>Landmark locations [x,y]:  [[3, 10], [1, 2], [1, 7]]\n</pre> In\u00a0[10]: Copied! <pre># try to sense any surrounding landmarks\nmeasurements = r.sense()\n\n# this will print out an empty list if `sense` has not been implemented\nprint(measurements)\n</pre> # try to sense any surrounding landmarks measurements = r.sense()  # this will print out an empty list if `sense` has not been implemented print(measurements) <pre>[[1, -2.9000596767394335, -0.9178807796354672], [2, -2.9474328448361815, 3.828379182602073]]\n</pre> <p>Refer back to the grid map above. Do these measurements make sense to you? Are all the landmarks captured in this list (why/why not)?</p> In\u00a0[\u00a0]: Copied! <pre>data = []\n\n# after a robot first senses, then moves (one time step)\n# that data is appended like so:\ndata.append([measurements, [dx, dy]])\n\n# for our example movement and measurement\nprint(data)\n</pre> data = []  # after a robot first senses, then moves (one time step) # that data is appended like so: data.append([measurements, [dx, dy]])  # for our example movement and measurement print(data) In\u00a0[\u00a0]: Copied! <pre># in this example, we have only created one time step (0)\ntime_step = 0\n\n# so you can access robot measurements:\nprint('Measurements: ', data[time_step][0])\n\n# and its motion for a given time step:\nprint('Motion: ', data[time_step][1])\n</pre> # in this example, we have only created one time step (0) time_step = 0  # so you can access robot measurements: print('Measurements: ', data[time_step][0])  # and its motion for a given time step: print('Motion: ', data[time_step][1])"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#robot-class","title":"Robot Class\u00b6","text":"<p>In this project, we'll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot's sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#uncertainty","title":"Uncertainty\u00b6","text":"<p>As you've learned, robot motion and sensors have some uncertainty associated with them. For example, imagine a car driving up hill and down hill; the speedometer reading will likely overestimate the speed of the car going up hill and underestimate the speed of the car going down hill because it cannot perfectly account for gravity. Similarly, we cannot perfectly predict the motion of a robot. A robot is likely to slightly overshoot or undershoot a target location.</p> <p>In this notebook, we'll look at the <code>robot</code> class that is partially given to you for the upcoming SLAM notebook. First, we'll create a robot and move it around a 2D grid world. Then, you'll be tasked with defining a <code>sense</code> function for this robot that allows it to sense landmarks in a given world! It's important that you understand how this robot moves, senses, and how it keeps track of different landmarks that it sees in a 2D grid world, so that you can work with it's movement and sensor data.</p> <p>Before we start analyzing robot motion, let's load in our resources and define the <code>robot</code> class. You can see that this class initializes the robot's position and adds measures of uncertainty for motion. You'll also see a <code>sense()</code> function which is not yet implemented, and you will learn more about that later in this notebook.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#define-a-world-and-a-robot","title":"Define a world and a robot\u00b6","text":"<p>Next, let's instantiate a robot object. As you can see in <code>__init__</code> above, the robot class takes in a number of parameters including a world size and some values that indicate the sensing and movement capabilities of the robot.</p> <p>In the next example, we define a small 10x10 square world, a measurement range that is half that of the world and small values for motion and measurement noise. These values will typically be about 10 times larger, but we just want to demonstrate this behavior on a small scale. You are also free to change these values and note what happens as your robot moves!</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#visualizing-the-world","title":"Visualizing the World\u00b6","text":"<p>In the given example, we can see/print out that the robot is in the middle of the 10x10 world at (x, y) = (5.0, 5.0), which is exactly what we expect!</p> <p>However, it's kind of hard to imagine this robot in the center of a world, without visualizing the grid itself, and so in the next cell we provide a helper visualization function, <code>display_world</code>, that will display a grid world in a plot and draw a red <code>o</code> at the location of our robot, <code>r</code>. The details of how this function works can be found in the <code>helpers.py</code> file in the home directory; you do not have to change anything in this <code>helpers.py</code> file.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#movement","title":"Movement\u00b6","text":"<p>Now you can really picture where the robot is in the world! Next, let's call the robot's <code>move</code> function. We'll ask it to move some distance <code>(dx, dy)</code> and we'll see that this motion is not perfect by the placement of our robot <code>o</code> and by the printed out position of <code>r</code>.</p> <p>Try changing the values of <code>dx</code> and <code>dy</code> and/or running this cell multiple times; see how the robot moves and how the uncertainty in robot motion accumulates over multiple movements.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#for-a-dx-1-does-the-robot-move-exactly-one-spot-to-the-right-what-about-dx-1-what-happens-if-you-try-to-move-the-robot-past-the-boundaries-of-the-world","title":"For a <code>dx</code> = 1, does the robot move exactly one spot to the right? What about <code>dx</code> = -1? What happens if you try to move the robot past the boundaries of the world?\u00b6","text":""},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#landmarks","title":"Landmarks\u00b6","text":"<p>Next, let's create landmarks, which are measurable features in the map. You can think of landmarks as things like notable buildings, or something smaller such as a tree, rock, or other feature.</p> <p>The robot class has a function <code>make_landmarks</code> which randomly generates locations for the number of specified landmarks. Try changing <code>num_landmarks</code> or running this cell multiple times to see where these landmarks appear. We have to pass these locations as a third argument to the <code>display_world</code> function and the list of landmark locations is accessed similar to how we find the robot position <code>r.landmarks</code>.</p> <p>Each landmark is displayed as a purple <code>x</code> in the grid world, and we also print out the exact <code>[x, y]</code> locations of these landmarks at the end of this cell.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#sense","title":"Sense\u00b6","text":"<p>Once we have some landmarks to sense, we need to be able to tell our robot to try to sense how far they are away from it. It will be up to you to code the <code>sense</code> function in our robot class.</p> <p>The <code>sense</code> function uses only internal class parameters and returns a list of the the measured/sensed x and y distances to the landmarks it senses within the specified <code>measurement_range</code>.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#todo-implement-the-sense-function","title":"TODO: Implement the <code>sense</code> function\u00b6","text":"<p>Follow the <code>##TODO's</code> in the class code above to complete the <code>sense</code> function for the robot class. Once you have tested out your code, please copy your complete <code>sense</code> code to the <code>robot_class.py</code> file in the home directory. By placing this complete code in the <code>robot_class</code> Python file, we will be able to refernce this class in a later notebook.</p> <p>The measurements have the format, <code>[i, dx, dy]</code> where <code>i</code> is the landmark index (0, 1, 2, ...) and <code>dx</code> and <code>dy</code> are the measured distance between the robot's location (x, y) and the landmark's location (x, y). This distance will not be perfect since our sense function has some associated <code>measurement noise</code>.</p> <p>In the example in the following cell, we have given our robot a range of <code>5.0</code> so any landmarks that are within that range of our robot's location, should appear in a list of measurements. Not all landmarks are guaranteed to be in our visibility range, so this list will be variable in length.</p> <p>Note: the robot's location is often called the pose or <code>[Pxi, Pyi]</code> and the landmark locations are often written as <code>[Lxi, Lyi]</code>. You'll see this notation in the next notebook.</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#data","title":"Data\u00b6","text":""},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#putting-it-all-together","title":"Putting it all together\u00b6","text":"<p>To perform SLAM, we'll collect a series of robot sensor measurements and motions, in that order, over a defined period of time. Then we'll use only this data to re-construct the map of the world with the robot and landmark locations. You can think of SLAM as peforming what we've done in this notebook, only backwards. Instead of defining a world and robot and creating movement and sensor data, it will be up to you to use movement and sensor measurements to reconstruct the world!</p> <p>In the next notebook, you'll see this list of movements and measurements (which you'll use to re-construct the world) listed in a structure called <code>data</code>. This is an array that holds sensor measurements and movements in a specific order, which will be useful to call upon when you have to extract this data and form constraint matrices and vectors.</p> <p><code>data</code> is constructed over a series of time steps as follows:</p>"},{"location":"theory/slam/1.%20Robot%20Moving%20and%20Sensing/#final-robot-class","title":"Final robot class\u00b6","text":"<p>Before moving on to the last notebook in this series, please make sure that you have copied your final, completed <code>sense</code> function into the <code>robot_class.py</code> file in the home directory. We will be using this file in the final implementation of slam!</p>"},{"location":"theory/slam/2.%20Omega%20and%20Xi%2C%20Constraints/","title":"Omega and Xi Constraints Notebook","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\n# define omega and xi as in the example\nomega = np.array([[1,0,0],\n                  [-1,1,0],\n                  [0,-1,1]])\n\nxi = np.array([[-3],\n               [5],\n               [3]])\n\n# calculate the inverse of omega\nomega_inv = np.linalg.inv(np.matrix(omega))\n\n# calculate the solution, mu\nmu = omega_inv*xi\n\n# print out the values of mu (x0, x1, x2)\nprint(mu)\n</pre> import numpy as np  # define omega and xi as in the example omega = np.array([[1,0,0],                   [-1,1,0],                   [0,-1,1]])  xi = np.array([[-3],                [5],                [3]])  # calculate the inverse of omega omega_inv = np.linalg.inv(np.matrix(omega))  # calculate the solution, mu mu = omega_inv*xi  # print out the values of mu (x0, x1, x2) print(mu) <pre>[[-3.]\n [ 2.]\n [ 5.]]\n</pre>"},{"location":"theory/slam/2.%20Omega%20and%20Xi%2C%20Constraints/#omega-and-xi","title":"Omega and Xi\u00b6","text":"<p>To implement Graph SLAM, a matrix and a vector (omega and xi, respectively) are introduced. The matrix is square and labelled with all the robot poses (xi) and all the landmarks (Li). Every time you make an observation, for example, as you move between two poses by some distance <code>dx</code> and can relate those two positions, you can represent this as a numerical relationship in these matrices.</p> <p>It's easiest to see how these work in an example. Below you can see a matrix representation of omega and a vector representation of xi.</p> <p></p> <p>Next, let's look at a simple example that relates 3 poses to one another.</p> <ul> <li>When you start out in the world most of these values are zeros or contain only values from the initial robot position</li> <li>In this example, you have been given constraints, which relate these poses to one another</li> <li>Constraints translate into matrix values</li> </ul> <p></p> <p>If you have ever solved linear systems of equations before, this may look familiar, and if not, let's keep going!</p>"},{"location":"theory/slam/2.%20Omega%20and%20Xi%2C%20Constraints/#solving-for-x","title":"Solving for x\u00b6","text":"<p>To \"solve\" for all these x values, we can use linear algebra; all the values of x are in the vector <code>mu</code> which can be calculated as a product of the inverse of omega times xi.</p> <p></p> <p>You can confirm this result for yourself by executing the math in the cell below.</p>"},{"location":"theory/slam/2.%20Omega%20and%20Xi%2C%20Constraints/#motion-constraints-and-landmarks","title":"Motion Constraints and Landmarks\u00b6","text":"<p>In the last example, the constraint equations, relating one pose to another were given to you. In this next example, let's look at how motion (and similarly, sensor measurements) can be used to create constraints and fill up the constraint matrices, omega and xi. Let's start with empty/zero matrices.</p> <p></p> <p>This example also includes relationships between poses and landmarks. Say we move from x0 to x1 with a displacement <code>dx</code> of 5. Then we have created a motion constraint that relates x0 to x1, and we can start to fill up these matrices.</p> <p></p> <p>In fact, the one constraint equation can be written in two ways. So, the motion constraint that relates x0 and x1 by the motion of 5 has affected the matrix, adding values for all elements that correspond to x0 and x1.</p>"},{"location":"theory/slam/2.%20Omega%20and%20Xi%2C%20Constraints/#2d-case","title":"2D case\u00b6","text":"<p>In these examples, we've been showing you change in only one dimension, the x-dimension. In the project, it will be up to you to represent x and y positional values in omega and xi. One solution could be to create an omega and xi that are 2x larger that the number of robot poses (that will be generated over a series of time steps) and the number of landmarks, so that they can hold both x and y values for poses and landmark locations. I might suggest drawing out a rough solution to graph slam as you read the instructions in the next notebook; that always helps me organize my thoughts. Good luck!</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/","title":"Project 3: Implement SLAM","text":"In\u00a0[124]: Copied! <pre>import numpy as np\nfrom helpers import make_data\n\n# your implementation of slam should work with the following inputs\n# feel free to change these input values and see how it responds!\n\n# world parameters\nnum_landmarks      = 5        # number of landmarks\nN                  = 20       # time steps\nworld_size         = 100.0    # size of world (square)\n\n# robot parameters\nmeasurement_range  = 40.0     # range at which we can sense landmarks\nmotion_noise       = 2.0      # noise in robot motion\nmeasurement_noise  = 2.0      # noise in the measurements\ndistance           = 20.0     # distance by which robot (intends to) move each iteratation \n\n\n# make_data instantiates a robot, AND generates random landmarks for a given world size and number of landmarks\ndata, robot = make_data(N, num_landmarks, world_size, measurement_range, motion_noise, measurement_noise, distance)\n</pre> import numpy as np from helpers import make_data  # your implementation of slam should work with the following inputs # feel free to change these input values and see how it responds!  # world parameters num_landmarks      = 5        # number of landmarks N                  = 20       # time steps world_size         = 100.0    # size of world (square)  # robot parameters measurement_range  = 40.0     # range at which we can sense landmarks motion_noise       = 2.0      # noise in robot motion measurement_noise  = 2.0      # noise in the measurements distance           = 20.0     # distance by which robot (intends to) move each iteratation    # make_data instantiates a robot, AND generates random landmarks for a given world size and number of landmarks data, robot = make_data(N, num_landmarks, world_size, measurement_range, motion_noise, measurement_noise, distance) <pre> \nLandmarks:  [[18, 89], [69, 87], [90, 4], [50, 26], [62, 51]]\nRobot: [x=82.09985 y=4.82303]\n</pre> In\u00a0[125]: Copied! <pre>robot.path\n</pre> robot.path Out[125]: <pre>[[50.0, 50.0],\n [47.9653139108604, 70.87161995162137],\n [46.66831802984643, 90.10390099865566],\n [64.9704446366091, 77.4106448190126],\n [80.17310381101771, 64.23715998766768],\n [94.91763548457514, 53.502443213027924],\n [77.08680761056024, 40.73509636452328],\n [59.702645485723146, 30.275962929087015],\n [41.70453017836984, 20.06209885054181],\n [25.45398739107488, 8.48256679673503],\n [30.570711182198295, 28.194910946997087],\n [36.607433722001275, 46.682605230459274],\n [42.920363188234894, 65.51438580614918],\n [48.20551574003593, 86.66781862781585],\n [29.903323916004695, 93.60054185900569],\n [39.160937351731135, 76.84657271867609],\n [50.538242710815986, 60.96459525262081],\n [60.09765529394042, 41.61714396696874],\n [70.6945585558651, 22.544039120924644],\n [82.09984637157041, 4.823028844244325]]</pre> In\u00a0[126]: Copied! <pre># print out some stats about the data\ntime_step = 0\n\nprint('Example measurements: \\n', data[time_step][0])\nprint('\\n')\nprint('Example motion: \\n', data[time_step][1])\n</pre> # print out some stats about the data time_step = 0  print('Example measurements: \\n', data[time_step][0]) print('\\n') print('Example motion: \\n', data[time_step][1]) <pre>Example measurements: \n [[0, -32.09960529315943, 39.66833246766444], [1, 19.586368844525015, 35.85549304306623], [3, 1.531243254884402, -22.343892559365603], [4, 13.90407402915669, 1.5378339984084413]]\n\n\nExample motion: \n [-0.3757560909009105, 19.99646987245876]\n</pre> <p>Try changing the value of <code>time_step</code>, you should see that the list of measurements varies based on what in the world the robot sees after it moves. As you know from the first notebook, the robot can only sense so far and with a certain amount of accuracy in the measure of distance between its location and the location of landmarks. The motion of the robot always is a vector with two values: one for x and one for y displacement. This structure will be useful to keep in mind as you traverse this data in your implementation of slam.</p> In\u00a0[127]: Copied! <pre>def initialize_constraints(N, num_landmarks, world_size):\n    ''' This function takes in a number of time steps N, number of landmarks, and a world_size,\n        and returns initialized constraint matrices, omega and xi.'''\n    \n    ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable\n    \n    ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values\n    ## for the initial x, y location of our robot\n    omega = [0]\n    \n    ## TODO: Define the constraint *vector*, xi\n    ## you can assume that the robot starts out in the middle of the world with 100% confidence\n    xi = [0]\n    \n    # initialize constraint matrices with 0's\n    # Now these are 4x4 because of 3 poses and a landmark\n    size = 2*N + 2*num_landmarks\n    omega = np.zeros((size, size))\n    xi = np.zeros((size, 1))\n    \n    # add initial pose constraint\n    omega[0][0] = 1\n    omega[1][1] = 1\n    xi[0] = world_size/2\n    xi[1] = world_size/2\n    \n    return omega, xi\n    \n</pre> def initialize_constraints(N, num_landmarks, world_size):     ''' This function takes in a number of time steps N, number of landmarks, and a world_size,         and returns initialized constraint matrices, omega and xi.'''          ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable          ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values     ## for the initial x, y location of our robot     omega = [0]          ## TODO: Define the constraint *vector*, xi     ## you can assume that the robot starts out in the middle of the world with 100% confidence     xi = [0]          # initialize constraint matrices with 0's     # Now these are 4x4 because of 3 poses and a landmark     size = 2*N + 2*num_landmarks     omega = np.zeros((size, size))     xi = np.zeros((size, 1))          # add initial pose constraint     omega[0][0] = 1     omega[1][1] = 1     xi[0] = world_size/2     xi[1] = world_size/2          return omega, xi      In\u00a0[128]: Copied! <pre>#%matplotlib notebook\n</pre> #%matplotlib notebook In\u00a0[129]: Copied! <pre># import data viz resources\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nimport seaborn as sns\n%matplotlib inline\n</pre> # import data viz resources import matplotlib.pyplot as plt from pandas import DataFrame import seaborn as sns %matplotlib inline In\u00a0[130]: Copied! <pre># define a small N and world_size (small for ease of visualization)\nN_test = 5\nnum_landmarks_test = 2\nsmall_world = 10\n\n# initialize the constraints\ninitial_omega, initial_xi = initialize_constraints(N_test, num_landmarks_test, small_world)\n</pre> # define a small N and world_size (small for ease of visualization) N_test = 5 num_landmarks_test = 2 small_world = 10  # initialize the constraints initial_omega, initial_xi = initialize_constraints(N_test, num_landmarks_test, small_world) In\u00a0[131]: Copied! <pre># define figure size\nplt.rcParams[\"figure.figsize\"] = (10,7)\n\n# display omega\nsns.heatmap(DataFrame(initial_omega), cmap='Blues', annot=True, linewidths=.5)\n</pre> # define figure size plt.rcParams[\"figure.figsize\"] = (10,7)  # display omega sns.heatmap(DataFrame(initial_omega), cmap='Blues', annot=True, linewidths=.5) Out[131]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fef941b9c88&gt;</pre> In\u00a0[132]: Copied! <pre># define  figure size\nplt.rcParams[\"figure.figsize\"] = (1,7)\n\n# display xi\nsns.heatmap(DataFrame(initial_xi), cmap='Oranges', annot=True, linewidths=.5)\n</pre> # define  figure size plt.rcParams[\"figure.figsize\"] = (1,7)  # display xi sns.heatmap(DataFrame(initial_xi), cmap='Oranges', annot=True, linewidths=.5) Out[132]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fef8fbba630&gt;</pre> In\u00a0[133]: Copied! <pre>def plot_omega(omega):\n    # define figure size\n    plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (20,14)\n\n    # display omega\n    sns.heatmap(DataFrame(omega), cmap='Blues', annot=True, linewidths=.5)\n    \ndef plot_xi(xi):\n    # define  figure size\n    plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (1,14)\n\n    # display xi\n    sns.heatmap(DataFrame(xi), cmap='Oranges', annot=True, linewidths=.5)\n</pre> def plot_omega(omega):     # define figure size     plt.figure()     plt.rcParams[\"figure.figsize\"] = (20,14)      # display omega     sns.heatmap(DataFrame(omega), cmap='Blues', annot=True, linewidths=.5)      def plot_xi(xi):     # define  figure size     plt.figure()     plt.rcParams[\"figure.figsize\"] = (1,14)      # display xi     sns.heatmap(DataFrame(xi), cmap='Oranges', annot=True, linewidths=.5) In\u00a0[134]: Copied! <pre>def construct_constraints_sense(N, num_landmarks, time_step, measurement, measurement_noise):\n    ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific measurement including its noise,\n        and returns constraint matrices, omega and xi.'''\n    \n    size = 2 * (N + num_landmarks)\n    \n    landmark_index = measurement[0]\n    dx = measurement[1]\n    dy = measurement[2]\n    \n    ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable\n    \n    ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values\n    ## for the initial x, y location of our robot\n    # initialize constraint matrices with 0's\n    omega = np.zeros((size, size))\n    \n    ## TODO: Define the constraint *vector*, xi\n    ## you can assume that the robot starts out in the middle of the world with 100% confidence\n    xi = np.zeros((size, 1))\n    \n    \n    strength = 1.0 / measurement_noise\n    \n    # incorporate sense constraint for x coordinate\n    x_idx = time_step*2\n    lx_idx = 2 * (N + landmark_index)\n    omega[x_idx][x_idx] = strength\n    omega[x_idx][lx_idx] = -strength\n    omega[lx_idx][x_idx] = -strength\n    omega[lx_idx][lx_idx] = strength\n    xi[x_idx] = -dx / measurement_noise\n    xi[lx_idx] = dx / measurement_noise\n    \n    # incorporate sense constraint for y coordinate\n    y_idx = time_step*2 + 1\n    ly_idx = 2 * (N + landmark_index) + 1\n    omega[y_idx][y_idx] = strength\n    omega[y_idx][ly_idx] = -strength\n    omega[ly_idx][y_idx] = -strength\n    omega[ly_idx][ly_idx] = strength\n    xi[y_idx] = -dy / measurement_noise\n    xi[ly_idx] = dy / measurement_noise\n    \n    return omega, xi\n</pre> def construct_constraints_sense(N, num_landmarks, time_step, measurement, measurement_noise):     ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific measurement including its noise,         and returns constraint matrices, omega and xi.'''          size = 2 * (N + num_landmarks)          landmark_index = measurement[0]     dx = measurement[1]     dy = measurement[2]          ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable          ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values     ## for the initial x, y location of our robot     # initialize constraint matrices with 0's     omega = np.zeros((size, size))          ## TODO: Define the constraint *vector*, xi     ## you can assume that the robot starts out in the middle of the world with 100% confidence     xi = np.zeros((size, 1))               strength = 1.0 / measurement_noise          # incorporate sense constraint for x coordinate     x_idx = time_step*2     lx_idx = 2 * (N + landmark_index)     omega[x_idx][x_idx] = strength     omega[x_idx][lx_idx] = -strength     omega[lx_idx][x_idx] = -strength     omega[lx_idx][lx_idx] = strength     xi[x_idx] = -dx / measurement_noise     xi[lx_idx] = dx / measurement_noise          # incorporate sense constraint for y coordinate     y_idx = time_step*2 + 1     ly_idx = 2 * (N + landmark_index) + 1     omega[y_idx][y_idx] = strength     omega[y_idx][ly_idx] = -strength     omega[ly_idx][y_idx] = -strength     omega[ly_idx][ly_idx] = strength     xi[y_idx] = -dy / measurement_noise     xi[ly_idx] = dy / measurement_noise          return omega, xi In\u00a0[135]: Copied! <pre>def construct_constraints_move(N, num_landmarks, time_step, motion, motion_noise):\n    ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific motion including its noise,\n        and returns constraint matrices, omega and xi.'''\n    \n    size = 2 * (N + num_landmarks)\n    \n    dx = motion[0]\n    dy = motion[1]\n    \n    ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable\n    \n    ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values\n    ## for the initial x, y location of our robot\n    # initialize constraint matrices with 0's\n    omega = np.zeros((size, size))\n    \n    ## TODO: Define the constraint *vector*, xi\n    ## you can assume that the robot starts out in the middle of the world with 100% confidence\n    xi = np.zeros((size, 1))\n    \n    strength = 1.0 / motion_noise\n    \n    # incorporate motion constraint for x coordinate\n    x0 = time_step*2\n    x1 = x0 + 2\n    omega[x0][x0] = strength\n    omega[x0][x1] = -strength\n    omega[x1][x0] = -strength\n    omega[x1][x1] = strength\n    xi[x0] = -dx / motion_noise\n    xi[x1] = dx / motion_noise\n    \n    # incorporate motion constraint for y coordinate\n    y0 = time_step*2 + 1\n    y1 = y0 + 2\n    omega[y0][y0] = strength\n    omega[y0][y1] = -strength\n    omega[y1][y0] = -strength\n    omega[y1][y1] = strength\n    xi[y0] = -dy / motion_noise\n    xi[y1] = dy / motion_noise\n    \n    return omega, xi\n</pre> def construct_constraints_move(N, num_landmarks, time_step, motion, motion_noise):     ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific motion including its noise,         and returns constraint matrices, omega and xi.'''          size = 2 * (N + num_landmarks)          dx = motion[0]     dy = motion[1]          ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable          ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values     ## for the initial x, y location of our robot     # initialize constraint matrices with 0's     omega = np.zeros((size, size))          ## TODO: Define the constraint *vector*, xi     ## you can assume that the robot starts out in the middle of the world with 100% confidence     xi = np.zeros((size, 1))          strength = 1.0 / motion_noise          # incorporate motion constraint for x coordinate     x0 = time_step*2     x1 = x0 + 2     omega[x0][x0] = strength     omega[x0][x1] = -strength     omega[x1][x0] = -strength     omega[x1][x1] = strength     xi[x0] = -dx / motion_noise     xi[x1] = dx / motion_noise          # incorporate motion constraint for y coordinate     y0 = time_step*2 + 1     y1 = y0 + 2     omega[y0][y0] = strength     omega[y0][y1] = -strength     omega[y1][y0] = -strength     omega[y1][y1] = strength     xi[y0] = -dy / motion_noise     xi[y1] = dy / motion_noise          return omega, xi In\u00a0[136]: Copied! <pre>## TODO: Complete the code to implement SLAM\n\n## slam takes in 6 arguments and returns mu, \n## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations\ndef slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise):\n    \n    ## TODO: Use your initilization to create constraint matrices, omega and xi\n    # initialize the constraints\n    omega, xi = initialize_constraints(N, num_landmarks, world_size)\n    \n    ## TODO: Iterate through each time step in the data\n    ## get all the motion and measurement data as you iterate\n    for time_step in range(N-1):\n        #print(time_step)\n        measurements = data[time_step][0]\n        motion = data[time_step][1]\n    ## TODO: update the constraint matrix/vector to account for all *measurements*\n    ## this should be a series of additions that take into account the measurement noise\n        for measurement in measurements:\n    \n            omega_sense, xi_sense = construct_constraints_sense(N, num_landmarks, time_step, measurement, measurement_noise)\n            omega += omega_sense\n            xi += xi_sense\n            \n        #plot_omega(omega)\n        #plot_xi(xi)\n        #return\n    ## TODO: update the constraint matrix/vector to account for all *motion* and motion noise\n        omega_move, xi_move = construct_constraints_move(N, num_landmarks, time_step, motion, motion_noise)\n        omega += omega_move\n        xi += xi_move\n        \n        #plot_omega(omega)\n        #plot_xi(xi)\n        #return\n        \n    \n    ## TODO: After iterating through all the data\n    ## Compute the best estimate of poses and landmark positions\n    ## using the formula, omega_inverse * Xi\n    mu = None\n    #plot_omega(omega)\n    #plot_xi(xi)\n    #return\n    \n    omega_inv = np.linalg.inv(np.matrix(omega))\n    mu = omega_inv*xi\n    \n    return mu # return `mu`\n</pre> ## TODO: Complete the code to implement SLAM  ## slam takes in 6 arguments and returns mu,  ## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations def slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise):          ## TODO: Use your initilization to create constraint matrices, omega and xi     # initialize the constraints     omega, xi = initialize_constraints(N, num_landmarks, world_size)          ## TODO: Iterate through each time step in the data     ## get all the motion and measurement data as you iterate     for time_step in range(N-1):         #print(time_step)         measurements = data[time_step][0]         motion = data[time_step][1]     ## TODO: update the constraint matrix/vector to account for all *measurements*     ## this should be a series of additions that take into account the measurement noise         for measurement in measurements:                  omega_sense, xi_sense = construct_constraints_sense(N, num_landmarks, time_step, measurement, measurement_noise)             omega += omega_sense             xi += xi_sense                      #plot_omega(omega)         #plot_xi(xi)         #return     ## TODO: update the constraint matrix/vector to account for all *motion* and motion noise         omega_move, xi_move = construct_constraints_move(N, num_landmarks, time_step, motion, motion_noise)         omega += omega_move         xi += xi_move                  #plot_omega(omega)         #plot_xi(xi)         #return                   ## TODO: After iterating through all the data     ## Compute the best estimate of poses and landmark positions     ## using the formula, omega_inverse * Xi     mu = None     #plot_omega(omega)     #plot_xi(xi)     #return          omega_inv = np.linalg.inv(np.matrix(omega))     mu = omega_inv*xi          return mu # return `mu`  In\u00a0[137]: Copied! <pre># a helper function that creates a list of poses and of landmarks for ease of printing\n# this only works for the suggested constraint architecture of interlaced x,y poses\ndef get_poses_landmarks(mu, N):\n    # create a list of poses\n    poses = []\n    for i in range(N):\n        poses.append((mu[2*i].item(), mu[2*i+1].item()))\n\n    # create a list of landmarks\n    landmarks = []\n    for i in range(num_landmarks):\n        landmarks.append((mu[2*(N+i)].item(), mu[2*(N+i)+1].item()))\n\n    # return completed lists\n    return poses, landmarks\n</pre> # a helper function that creates a list of poses and of landmarks for ease of printing # this only works for the suggested constraint architecture of interlaced x,y poses def get_poses_landmarks(mu, N):     # create a list of poses     poses = []     for i in range(N):         poses.append((mu[2*i].item(), mu[2*i+1].item()))      # create a list of landmarks     landmarks = []     for i in range(num_landmarks):         landmarks.append((mu[2*(N+i)].item(), mu[2*(N+i)+1].item()))      # return completed lists     return poses, landmarks  In\u00a0[138]: Copied! <pre>def print_all(poses, landmarks):\n    print('\\n')\n    print('Estimated Poses:')\n    for i in range(len(poses)):\n        print('['+', '.join('%.3f'%p for p in poses[i])+']')\n    print('\\n')\n    print('Estimated Landmarks:')\n    for i in range(len(landmarks)):\n        print('['+', '.join('%.3f'%l for l in landmarks[i])+']')\n</pre> def print_all(poses, landmarks):     print('\\n')     print('Estimated Poses:')     for i in range(len(poses)):         print('['+', '.join('%.3f'%p for p in poses[i])+']')     print('\\n')     print('Estimated Landmarks:')     for i in range(len(landmarks)):         print('['+', '.join('%.3f'%l for l in landmarks[i])+']')  In\u00a0[139]: Copied! <pre># a helper function that creates a list of poses and of landmarks for ease of printing\n# this only works for the suggested constraint architecture of interlaced x,y poses\ndef get_pose_landmarks_online(mu):\n    # create a list of poses\n    pose = (mu[0].item(), mu[1].item())\n\n    # create a list of landmarks\n    landmarks = []\n    N = 1\n    for i in range(num_landmarks):\n        landmarks.append((mu[2*(N+i)].item(), mu[2*(N+i)+1].item()))\n\n    # return completed lists\n    return pose, landmarks\n</pre> # a helper function that creates a list of poses and of landmarks for ease of printing # this only works for the suggested constraint architecture of interlaced x,y poses def get_pose_landmarks_online(mu):     # create a list of poses     pose = (mu[0].item(), mu[1].item())      # create a list of landmarks     landmarks = []     N = 1     for i in range(num_landmarks):         landmarks.append((mu[2*(N+i)].item(), mu[2*(N+i)+1].item()))      # return completed lists     return pose, landmarks In\u00a0[140]: Copied! <pre>def print_all_online(pose, landmarks):\n    print('\\n')\n    print('Estimated Pose:')\n    print('['+', '.join('%.3f'%p for p in pose)+']')\n    print('\\n')\n    print('Estimated Landmarks:')\n    for i in range(len(landmarks)):\n        print('['+', '.join('%.3f'%l for l in landmarks[i])+']')\n</pre> def print_all_online(pose, landmarks):     print('\\n')     print('Estimated Pose:')     print('['+', '.join('%.3f'%p for p in pose)+']')     print('\\n')     print('Estimated Landmarks:')     for i in range(len(landmarks)):         print('['+', '.join('%.3f'%l for l in landmarks[i])+']') In\u00a0[141]: Copied! <pre># call your implementation of slam, passing in the necessary parameters\nmu = slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise)\n\n# print out the resulting landmarks and poses\nif(mu is not None):\n    # get the lists of poses and landmarks\n    # and print them out\n    poses, landmarks = get_poses_landmarks(mu, N)\n    print_all(poses, landmarks)\n</pre> # call your implementation of slam, passing in the necessary parameters mu = slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise)  # print out the resulting landmarks and poses if(mu is not None):     # get the lists of poses and landmarks     # and print them out     poses, landmarks = get_poses_landmarks(mu, N)     print_all(poses, landmarks) <pre>\n\nEstimated Poses:\n[50.000, 50.000]\n[49.438, 70.169]\n[48.361, 89.863]\n[64.592, 77.021]\n[80.771, 64.443]\n[95.764, 54.027]\n[78.608, 42.276]\n[60.960, 30.572]\n[43.660, 20.198]\n[26.696, 9.385]\n[31.650, 28.717]\n[37.060, 47.409]\n[43.975, 66.118]\n[49.716, 86.220]\n[30.860, 93.758]\n[40.653, 77.368]\n[51.684, 60.255]\n[61.657, 41.688]\n[72.529, 23.425]\n[82.204, 5.921]\n\n\nEstimated Landmarks:\n[18.364, 89.130]\n[69.980, 86.949]\n[91.585, 5.242]\n[51.569, 26.748]\n[63.195, 51.718]\n</pre> In\u00a0[142]: Copied! <pre># import the helper function\nfrom helpers import display_world, display_world_extended\n\n# Display the final world!\n\n# define figure size\nplt.rcParams[\"figure.figsize\"] = (10,10)\n\n# check if poses has been created\nif 'poses' in locals():\n    # print out the last pose\n    print('Last pose: ', poses[-1])\n    print('True last pose: ', robot.path[-1])\n    # display the last position of the robot *and* the landmark positions\n    display_world(int(world_size), poses[-1], landmarks)\n    display_world_extended(int(world_size), poses[-1], landmarks, robot)\n</pre> # import the helper function from helpers import display_world, display_world_extended  # Display the final world!  # define figure size plt.rcParams[\"figure.figsize\"] = (10,10)  # check if poses has been created if 'poses' in locals():     # print out the last pose     print('Last pose: ', poses[-1])     print('True last pose: ', robot.path[-1])     # display the last position of the robot *and* the landmark positions     display_world(int(world_size), poses[-1], landmarks)     display_world_extended(int(world_size), poses[-1], landmarks, robot) <pre>Last pose:  (82.2038611962253, 5.9212434963337515)\nTrue last pose:  [82.09984637157041, 4.823028844244325]\n</pre> <p>Answer: (Write your answer here.)</p> <p>The true landmark positions and the final robot pose are compared to their estimated locations in the following pandas dataframes:</p> <p>Motion and Measurement Noise</p> <p>For a <code>motion_noise</code> of 2.0 the mean pose error is 0.993865921977 The mean landmark location error is 0.858795754817 with a <code>measurement_noise</code> of 2.0.</p> <p>For <code>motion_noise=0.2</code> the mean pose error decreases to 0.0882231486516. The same is true for the mean landmark location error. It decreases to 0.0773540304092 when the <code>measurement_noise</code> is set to 0.2.</p> <p>Changing N</p> <p>Increasing the time steps <code>N</code> doesn't reduce the error between true and estimated poses. The main issue is the measurement and motion noise.</p> <p>The error increases when <code>N</code>, the number of sense, move cycles is set too small, meaning that the robot is not able to explore the whole world and therefore might miss some of the landmarks.</p> In\u00a0[143]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[144]: Copied! <pre>def calculate_errors(dataframe):\n    errors = []\n    for index, row in dataframe.iterrows():\n        gt = row.iloc[0]\n        est = row.iloc[1]\n        error = np.sqrt((gt[0] - est[0])**2 + (gt[1] - est[1])**2)\n        errors.append(error)\n        \n    return errors\n</pre> def calculate_errors(dataframe):     errors = []     for index, row in dataframe.iterrows():         gt = row.iloc[0]         est = row.iloc[1]         error = np.sqrt((gt[0] - est[0])**2 + (gt[1] - est[1])**2)         errors.append(error)              return errors In\u00a0[145]: Copied! <pre>df_landmarks = pd.DataFrame(list(zip(robot.landmarks, landmarks)), \n                  columns =['Landmarks (Ground Truth)', 'Landmarks Estimated'])\n\ndf_landmarks['error'] = calculate_errors(df_landmarks)\nprint(\"Mean landmark location error: \", np.mean(df_landmarks['error']))\ndf_landmarks\n</pre> df_landmarks = pd.DataFrame(list(zip(robot.landmarks, landmarks)),                    columns =['Landmarks (Ground Truth)', 'Landmarks Estimated'])  df_landmarks['error'] = calculate_errors(df_landmarks) print(\"Mean landmark location error: \", np.mean(df_landmarks['error'])) df_landmarks <pre>Mean landmark location error:  1.30282438513\n</pre> Out[145]: Landmarks (Ground Truth) Landmarks Estimated error 0 [18, 89] (18.36405719564165, 89.13010359088983) 0.386607 1 [69, 87] (69.98045471591945, 86.9493502529124) 0.981762 2 [90, 4] (91.58546248832768, 5.2416232456900715) 2.013782 3 [50, 26] (51.56864778209609, 26.747538544008243) 1.737662 4 [62, 51] (63.19516104206713, 51.71811349575321) 1.394309 In\u00a0[146]: Copied! <pre>df_poses = pd.DataFrame(list(zip(robot.path, poses)), \n                       columns = ['Robot Pose (Ground Truth)', 'Robot Pose Estimated'])\n\n    \ndf_poses['error'] = calculate_errors(df_poses)\nprint(\"Mean pose error: \", np.mean(df_poses['error']))\ndf_poses\n</pre> df_poses = pd.DataFrame(list(zip(robot.path, poses)),                         columns = ['Robot Pose (Ground Truth)', 'Robot Pose Estimated'])       df_poses['error'] = calculate_errors(df_poses) print(\"Mean pose error: \", np.mean(df_poses['error'])) df_poses <pre>Mean pose error:  1.29529591361\n</pre> Out[146]: Robot Pose (Ground Truth) Robot Pose Estimated error 0 [50.0, 50.0] (49.99999999999985, 50.0) 1.492140e-13 1 [47.9653139109, 70.8716199516] (49.438004008780396, 70.16913093866839) 1.631658e+00 2 [46.6683180298, 90.1039009987] (48.3613484935135, 89.86262143791677) 1.710137e+00 3 [64.9704446366, 77.410644819] (64.59171970265324, 77.02139007457875) 5.430947e-01 4 [80.173103811, 64.2371599877] (80.77088843363241, 64.44260858598707) 6.321041e-01 5 [94.9176354846, 53.502443213] (95.76385984050381, 54.02656419804629) 9.953886e-01 6 [77.0868076106, 40.7350963645] (78.60795822841908, 42.276292520391976) 2.165453e+00 7 [59.7026454857, 30.2759629291] (60.96010682550587, 30.571796497157187) 1.291792e+00 8 [41.7045301784, 20.0620988505] (43.659700512526946, 20.19836786089933) 1.959913e+00 9 [25.4539873911, 8.48256679674] (26.696105438280426, 9.38491673983075) 1.535283e+00 10 [30.5707111822, 28.194910947] (31.649736401802663, 28.716669572468135) 1.198552e+00 11 [36.607433722, 46.6826052305] (37.05953922912646, 47.409313061464694) 8.558643e-01 12 [42.9203631882, 65.5143858061] (43.97456034544608, 66.11753269114453) 1.214544e+00 13 [48.20551574, 86.6678186278] (49.716087470691065, 86.22023305818547) 1.575487e+00 14 [29.903323916, 93.600541859] (30.8596240555992, 93.75812932878824) 9.691975e-01 15 [39.1609373517, 76.8465727187] (40.65306530504595, 77.36807311352446) 1.580635e+00 16 [50.5382427108, 60.9645952526] (51.68415109497511, 60.25454461488866) 1.348065e+00 17 [60.0976552939, 41.617143967] (61.6569214175812, 41.688277105275304) 1.560888e+00 18 [70.6945585559, 22.5440391209] (72.5285881581232, 23.425217092011252) 2.034733e+00 19 [82.0998463716, 4.82302884424] (82.2038611962253, 5.9212434963337515) 1.103129e+00 In\u00a0[147]: Copied! <pre># Here is the data and estimated outputs for test case 1\n\ntest_data1 = [[[[1, 19.457599255548065, 23.8387362100849], [2, -13.195807561967236, 11.708840328458608], [3, -30.0954905279171, 15.387879242505843]], [-12.2607279422326, -15.801093326936487]], [[[2, -0.4659930049620491, 28.088559771215664], [4, -17.866382374890936, -16.384904503932]], [-12.2607279422326, -15.801093326936487]], [[[4, -6.202512900833806, -1.823403210274639]], [-12.2607279422326, -15.801093326936487]], [[[4, 7.412136480918645, 15.388585962142429]], [14.008259661173426, 14.274756084260822]], [[[4, -7.526138813444998, -0.4563942429717849]], [14.008259661173426, 14.274756084260822]], [[[2, -6.299793150150058, 29.047830407717623], [4, -21.93551130411791, -13.21956810989039]], [14.008259661173426, 14.274756084260822]], [[[1, 15.796300959032276, 30.65769689694247], [2, -18.64370821983482, 17.380022987031367]], [14.008259661173426, 14.274756084260822]], [[[1, 0.40311325410337906, 14.169429532679855], [2, -35.069349468466235, 2.4945558982439957]], [14.008259661173426, 14.274756084260822]], [[[1, -16.71340983241936, -2.777000269543834]], [-11.006096015782283, 16.699276945166858]], [[[1, -3.611096830835776, -17.954019226763958]], [-19.693482634035977, 3.488085684573048]], [[[1, 18.398273354362416, -22.705102332550947]], [-19.693482634035977, 3.488085684573048]], [[[2, 2.789312482883833, -39.73720193121324]], [12.849049222879723, -15.326510824972983]], [[[1, 21.26897046581808, -10.121029799040915], [2, -11.917698965880655, -23.17711662602097], [3, -31.81167947898398, -16.7985673023331]], [12.849049222879723, -15.326510824972983]], [[[1, 10.48157743234859, 5.692957082575485], [2, -22.31488473554935, -5.389184118551409], [3, -40.81803984305378, -2.4703329790238118]], [12.849049222879723, -15.326510824972983]], [[[0, 10.591050242096598, -39.2051798967113], [1, -3.5675572049297553, 22.849456408289125], [2, -38.39251065320351, 7.288990306029511]], [12.849049222879723, -15.326510824972983]], [[[0, -3.6225556479370766, -25.58006865235512]], [-7.8874682868419965, -18.379005523261092]], [[[0, 1.9784503557879374, -6.5025974151499]], [-7.8874682868419965, -18.379005523261092]], [[[0, 10.050665232782423, 11.026385307998742]], [-17.82919359778298, 9.062000642947142]], [[[0, 26.526838150174818, -0.22563393232425621], [4, -33.70303936886652, 2.880339841013677]], [-17.82919359778298, 9.062000642947142]]]\n\n##  Test Case 1\n##\n# Estimated Pose(s):\n#     [50.000, 50.000]\n#     [37.858, 33.921]\n#     [25.905, 18.268]\n#     [13.524, 2.224]\n#     [27.912, 16.886]\n#     [42.250, 30.994]\n#     [55.992, 44.886]\n#     [70.749, 59.867]\n#     [85.371, 75.230]\n#     [73.831, 92.354]\n#     [53.406, 96.465]\n#     [34.370, 100.134]\n#     [48.346, 83.952]\n#     [60.494, 68.338]\n#     [73.648, 53.082]\n#     [86.733, 38.197]\n#     [79.983, 20.324]\n#     [72.515, 2.837]\n#     [54.993, 13.221]\n#     [37.164, 22.283]\n\n\n# Estimated Landmarks:\n#     [82.679, 13.435]\n#     [70.417, 74.203]\n#     [36.688, 61.431]\n#     [18.705, 66.136]\n#     [20.437, 16.983]\n\n\n### Uncomment the following three lines for test case 1 and compare the output to the values above ###\n\nmu_1 = slam(test_data1, 20, 5, 100.0, 2.0, 2.0)\nposes_1, landmarks_1 = get_poses_landmarks(mu_1, 20)\nprint_all(poses_1, landmarks_1)\n</pre> # Here is the data and estimated outputs for test case 1  test_data1 = [[[[1, 19.457599255548065, 23.8387362100849], [2, -13.195807561967236, 11.708840328458608], [3, -30.0954905279171, 15.387879242505843]], [-12.2607279422326, -15.801093326936487]], [[[2, -0.4659930049620491, 28.088559771215664], [4, -17.866382374890936, -16.384904503932]], [-12.2607279422326, -15.801093326936487]], [[[4, -6.202512900833806, -1.823403210274639]], [-12.2607279422326, -15.801093326936487]], [[[4, 7.412136480918645, 15.388585962142429]], [14.008259661173426, 14.274756084260822]], [[[4, -7.526138813444998, -0.4563942429717849]], [14.008259661173426, 14.274756084260822]], [[[2, -6.299793150150058, 29.047830407717623], [4, -21.93551130411791, -13.21956810989039]], [14.008259661173426, 14.274756084260822]], [[[1, 15.796300959032276, 30.65769689694247], [2, -18.64370821983482, 17.380022987031367]], [14.008259661173426, 14.274756084260822]], [[[1, 0.40311325410337906, 14.169429532679855], [2, -35.069349468466235, 2.4945558982439957]], [14.008259661173426, 14.274756084260822]], [[[1, -16.71340983241936, -2.777000269543834]], [-11.006096015782283, 16.699276945166858]], [[[1, -3.611096830835776, -17.954019226763958]], [-19.693482634035977, 3.488085684573048]], [[[1, 18.398273354362416, -22.705102332550947]], [-19.693482634035977, 3.488085684573048]], [[[2, 2.789312482883833, -39.73720193121324]], [12.849049222879723, -15.326510824972983]], [[[1, 21.26897046581808, -10.121029799040915], [2, -11.917698965880655, -23.17711662602097], [3, -31.81167947898398, -16.7985673023331]], [12.849049222879723, -15.326510824972983]], [[[1, 10.48157743234859, 5.692957082575485], [2, -22.31488473554935, -5.389184118551409], [3, -40.81803984305378, -2.4703329790238118]], [12.849049222879723, -15.326510824972983]], [[[0, 10.591050242096598, -39.2051798967113], [1, -3.5675572049297553, 22.849456408289125], [2, -38.39251065320351, 7.288990306029511]], [12.849049222879723, -15.326510824972983]], [[[0, -3.6225556479370766, -25.58006865235512]], [-7.8874682868419965, -18.379005523261092]], [[[0, 1.9784503557879374, -6.5025974151499]], [-7.8874682868419965, -18.379005523261092]], [[[0, 10.050665232782423, 11.026385307998742]], [-17.82919359778298, 9.062000642947142]], [[[0, 26.526838150174818, -0.22563393232425621], [4, -33.70303936886652, 2.880339841013677]], [-17.82919359778298, 9.062000642947142]]]  ##  Test Case 1 ## # Estimated Pose(s): #     [50.000, 50.000] #     [37.858, 33.921] #     [25.905, 18.268] #     [13.524, 2.224] #     [27.912, 16.886] #     [42.250, 30.994] #     [55.992, 44.886] #     [70.749, 59.867] #     [85.371, 75.230] #     [73.831, 92.354] #     [53.406, 96.465] #     [34.370, 100.134] #     [48.346, 83.952] #     [60.494, 68.338] #     [73.648, 53.082] #     [86.733, 38.197] #     [79.983, 20.324] #     [72.515, 2.837] #     [54.993, 13.221] #     [37.164, 22.283]   # Estimated Landmarks: #     [82.679, 13.435] #     [70.417, 74.203] #     [36.688, 61.431] #     [18.705, 66.136] #     [20.437, 16.983]   ### Uncomment the following three lines for test case 1 and compare the output to the values above ###  mu_1 = slam(test_data1, 20, 5, 100.0, 2.0, 2.0) poses_1, landmarks_1 = get_poses_landmarks(mu_1, 20) print_all(poses_1, landmarks_1) <pre>\n\nEstimated Poses:\n[50.000, 50.000]\n[37.973, 33.652]\n[26.185, 18.155]\n[13.745, 2.116]\n[28.097, 16.783]\n[42.384, 30.902]\n[55.831, 44.497]\n[70.857, 59.699]\n[85.697, 75.543]\n[74.011, 92.434]\n[53.544, 96.454]\n[34.525, 100.080]\n[48.623, 83.953]\n[60.197, 68.107]\n[73.778, 52.935]\n[87.132, 38.538]\n[80.303, 20.508]\n[72.798, 2.945]\n[55.245, 13.255]\n[37.416, 22.317]\n\n\nEstimated Landmarks:\n[82.956, 13.539]\n[70.495, 74.141]\n[36.740, 61.281]\n[18.698, 66.060]\n[20.635, 16.875]\n</pre> In\u00a0[148]: Copied! <pre># Here is the data and estimated outputs for test case 2\n\ntest_data2 = [[[[0, 26.543274387283322, -6.262538160312672], [3, 9.937396825799755, -9.128540360867689]], [18.92765331253674, -6.460955043986683]], [[[0, 7.706544739722961, -3.758467215445748], [1, 17.03954411948937, 31.705489938553438], [3, -11.61731288777497, -6.64964096716416]], [18.92765331253674, -6.460955043986683]], [[[0, -12.35130507136378, 2.585119104239249], [1, -2.563534536165313, 38.22159657838369], [3, -26.961236804740935, -0.4802312626141525]], [-11.167066095509824, 16.592065417497455]], [[[0, 1.4138633151721272, -13.912454837810632], [1, 8.087721200818589, 20.51845934354381], [3, -17.091723454402302, -16.521500551709707], [4, -7.414211721400232, 38.09191602674439]], [-11.167066095509824, 16.592065417497455]], [[[0, 12.886743222179561, -28.703968411636318], [1, 21.660953298391387, 3.4912891084614914], [3, -6.401401414569506, -32.321583037341625], [4, 5.034079343639034, 23.102207946092893]], [-11.167066095509824, 16.592065417497455]], [[[1, 31.126317672358578, -10.036784369535214], [2, -38.70878528420893, 7.4987265861424595], [4, 17.977218575473767, 6.150889254289742]], [-6.595520680493778, -18.88118393939265]], [[[1, 41.82460922922086, 7.847527392202475], [3, 15.711709540417502, -30.34633659912818]], [-6.595520680493778, -18.88118393939265]], [[[0, 40.18454208294434, -6.710999804403755], [3, 23.019508919299156, -10.12110867290604]], [-6.595520680493778, -18.88118393939265]], [[[3, 27.18579315312821, 8.067219022708391]], [-6.595520680493778, -18.88118393939265]], [[], [11.492663265706092, 16.36822198838621]], [[[3, 24.57154567653098, 13.461499960708197]], [11.492663265706092, 16.36822198838621]], [[[0, 31.61945290413707, 0.4272295085799329], [3, 16.97392299158991, -5.274596836133088]], [11.492663265706092, 16.36822198838621]], [[[0, 22.407381798735177, -18.03500068379259], [1, 29.642444125196995, 17.3794951934614], [3, 4.7969752441371645, -21.07505361639969], [4, 14.726069092569372, 32.75999422300078]], [11.492663265706092, 16.36822198838621]], [[[0, 10.705527984670137, -34.589764174299596], [1, 18.58772336795603, -0.20109708164787765], [3, -4.839806195049413, -39.92208742305105], [4, 4.18824810165454, 14.146847823548889]], [11.492663265706092, 16.36822198838621]], [[[1, 5.878492140223764, -19.955352450942357], [4, -7.059505455306587, -0.9740849280550585]], [19.628527845173146, 3.83678180657467]], [[[1, -11.150789592446378, -22.736641053247872], [4, -28.832815721158255, -3.9462962046291388]], [-19.841703647091965, 2.5113335861604362]], [[[1, 8.64427397916182, -20.286336970889053], [4, -5.036917727942285, -6.311739993868336]], [-5.946642674882207, -19.09548221169787]], [[[0, 7.151866679283043, -39.56103232616369], [1, 16.01535401373368, -3.780995345194027], [4, -3.04801331832137, 13.697362774960865]], [-5.946642674882207, -19.09548221169787]], [[[0, 12.872879480504395, -19.707592098123207], [1, 22.236710716903136, 16.331770792606406], [3, -4.841206109583004, -21.24604435851242], [4, 4.27111163223552, 32.25309748614184]], [-5.946642674882207, -19.09548221169787]]] \n\n\n##  Test Case 2\n##\n# Estimated Pose(s):\n#     [50.000, 50.000]\n#     [69.035, 45.061]\n#     [87.655, 38.971]\n#     [76.084, 55.541]\n#     [64.283, 71.684]\n#     [52.396, 87.887]\n#     [44.674, 68.948]\n#     [37.532, 49.680]\n#     [31.392, 30.893]\n#     [24.796, 12.012]\n#     [33.641, 26.440]\n#     [43.858, 43.560]\n#     [54.735, 60.659]\n#     [65.884, 77.791]\n#     [77.413, 94.554]\n#     [96.740, 98.020]\n#     [76.149, 99.586]\n#     [70.211, 80.580]\n#     [64.130, 61.270]\n#     [58.183, 42.175]\n\n\n# Estimated Landmarks:\n#     [76.777, 42.415]\n#     [85.109, 76.850]\n#     [13.687, 95.386]\n#     [59.488, 39.149]\n#     [69.283, 93.654]\n\n\n### Uncomment the following three lines for test case 2 and compare to the values above ###\n\nmu_2 = slam(test_data2, 20, 5, 100.0, 2.0, 2.0)\nposes, landmarks = get_poses_landmarks(mu_2, 20)\nprint_all(poses, landmarks)\n</pre> # Here is the data and estimated outputs for test case 2  test_data2 = [[[[0, 26.543274387283322, -6.262538160312672], [3, 9.937396825799755, -9.128540360867689]], [18.92765331253674, -6.460955043986683]], [[[0, 7.706544739722961, -3.758467215445748], [1, 17.03954411948937, 31.705489938553438], [3, -11.61731288777497, -6.64964096716416]], [18.92765331253674, -6.460955043986683]], [[[0, -12.35130507136378, 2.585119104239249], [1, -2.563534536165313, 38.22159657838369], [3, -26.961236804740935, -0.4802312626141525]], [-11.167066095509824, 16.592065417497455]], [[[0, 1.4138633151721272, -13.912454837810632], [1, 8.087721200818589, 20.51845934354381], [3, -17.091723454402302, -16.521500551709707], [4, -7.414211721400232, 38.09191602674439]], [-11.167066095509824, 16.592065417497455]], [[[0, 12.886743222179561, -28.703968411636318], [1, 21.660953298391387, 3.4912891084614914], [3, -6.401401414569506, -32.321583037341625], [4, 5.034079343639034, 23.102207946092893]], [-11.167066095509824, 16.592065417497455]], [[[1, 31.126317672358578, -10.036784369535214], [2, -38.70878528420893, 7.4987265861424595], [4, 17.977218575473767, 6.150889254289742]], [-6.595520680493778, -18.88118393939265]], [[[1, 41.82460922922086, 7.847527392202475], [3, 15.711709540417502, -30.34633659912818]], [-6.595520680493778, -18.88118393939265]], [[[0, 40.18454208294434, -6.710999804403755], [3, 23.019508919299156, -10.12110867290604]], [-6.595520680493778, -18.88118393939265]], [[[3, 27.18579315312821, 8.067219022708391]], [-6.595520680493778, -18.88118393939265]], [[], [11.492663265706092, 16.36822198838621]], [[[3, 24.57154567653098, 13.461499960708197]], [11.492663265706092, 16.36822198838621]], [[[0, 31.61945290413707, 0.4272295085799329], [3, 16.97392299158991, -5.274596836133088]], [11.492663265706092, 16.36822198838621]], [[[0, 22.407381798735177, -18.03500068379259], [1, 29.642444125196995, 17.3794951934614], [3, 4.7969752441371645, -21.07505361639969], [4, 14.726069092569372, 32.75999422300078]], [11.492663265706092, 16.36822198838621]], [[[0, 10.705527984670137, -34.589764174299596], [1, 18.58772336795603, -0.20109708164787765], [3, -4.839806195049413, -39.92208742305105], [4, 4.18824810165454, 14.146847823548889]], [11.492663265706092, 16.36822198838621]], [[[1, 5.878492140223764, -19.955352450942357], [4, -7.059505455306587, -0.9740849280550585]], [19.628527845173146, 3.83678180657467]], [[[1, -11.150789592446378, -22.736641053247872], [4, -28.832815721158255, -3.9462962046291388]], [-19.841703647091965, 2.5113335861604362]], [[[1, 8.64427397916182, -20.286336970889053], [4, -5.036917727942285, -6.311739993868336]], [-5.946642674882207, -19.09548221169787]], [[[0, 7.151866679283043, -39.56103232616369], [1, 16.01535401373368, -3.780995345194027], [4, -3.04801331832137, 13.697362774960865]], [-5.946642674882207, -19.09548221169787]], [[[0, 12.872879480504395, -19.707592098123207], [1, 22.236710716903136, 16.331770792606406], [3, -4.841206109583004, -21.24604435851242], [4, 4.27111163223552, 32.25309748614184]], [-5.946642674882207, -19.09548221169787]]]    ##  Test Case 2 ## # Estimated Pose(s): #     [50.000, 50.000] #     [69.035, 45.061] #     [87.655, 38.971] #     [76.084, 55.541] #     [64.283, 71.684] #     [52.396, 87.887] #     [44.674, 68.948] #     [37.532, 49.680] #     [31.392, 30.893] #     [24.796, 12.012] #     [33.641, 26.440] #     [43.858, 43.560] #     [54.735, 60.659] #     [65.884, 77.791] #     [77.413, 94.554] #     [96.740, 98.020] #     [76.149, 99.586] #     [70.211, 80.580] #     [64.130, 61.270] #     [58.183, 42.175]   # Estimated Landmarks: #     [76.777, 42.415] #     [85.109, 76.850] #     [13.687, 95.386] #     [59.488, 39.149] #     [69.283, 93.654]   ### Uncomment the following three lines for test case 2 and compare to the values above ###  mu_2 = slam(test_data2, 20, 5, 100.0, 2.0, 2.0) poses, landmarks = get_poses_landmarks(mu_2, 20) print_all(poses, landmarks) <pre>\n\nEstimated Poses:\n[50.000, 50.000]\n[69.181, 45.665]\n[87.743, 39.703]\n[76.270, 56.311]\n[64.317, 72.176]\n[52.257, 88.154]\n[44.059, 69.401]\n[37.002, 49.918]\n[30.924, 30.955]\n[23.508, 11.419]\n[34.180, 27.133]\n[44.155, 43.846]\n[54.806, 60.920]\n[65.698, 78.546]\n[77.468, 95.626]\n[96.802, 98.821]\n[75.957, 99.971]\n[70.200, 81.181]\n[64.054, 61.723]\n[58.107, 42.628]\n\n\nEstimated Landmarks:\n[76.779, 42.887]\n[85.065, 77.438]\n[13.548, 95.652]\n[59.449, 39.595]\n[69.263, 94.240]\n</pre> In\u00a0[149]: Copied! <pre>## 1. Everytime we get a new pose we want to expand to grow the matrix by inserting something right behind the existing pose x_t\n## 2. Take out sub-matrix omega_prime and sub vector xi_prime of the new pose x_t+1 and the landmarks\n## 3. Calculate A, B from omega and C from xi\n## 4. Reduced omega is obtained by omega = omega_prime - A^T B^-1 A\n## 5. Reduced xi is obtained by xi = xi_prime A^T B^-1 C\n</pre> ## 1. Everytime we get a new pose we want to expand to grow the matrix by inserting something right behind the existing pose x_t ## 2. Take out sub-matrix omega_prime and sub vector xi_prime of the new pose x_t+1 and the landmarks ## 3. Calculate A, B from omega and C from xi ## 4. Reduced omega is obtained by omega = omega_prime - A^T B^-1 A ## 5. Reduced xi is obtained by xi = xi_prime A^T B^-1 C In\u00a0[150]: Copied! <pre># ------------\n#\n# creates a new matrix from the existing matrix elements.\n#\n# Example:\n#       l = matrix([[ 1,  2,  3,  4,  5], \n#                   [ 6,  7,  8,  9, 10], \n#                   [11, 12, 13, 14, 15]])\n#\n#       l.take([0, 2], [0, 2, 3])\n#\n# results in:\n#       \n#       [[1, 3, 4], \n#        [11, 13, 14]]\n#       \n# \n# take is used to remove rows and columns from existing matrices\n# list1/list2 define a sequence of rows/columns that shall be taken\n# is no list2 is provided, then list2 is set to list1 (good for symmetric matrices)\n#\n\ndef take(matrix, list1, list2 = []):\n    (m_rows, m_cols) = matrix.shape\n    if list2 == []:\n        list2 = list1\n    if len(list1) &gt; m_rows or len(list2) &gt; m_cols:\n        raise ValueError(\"list invalid in take()\")\n\n    res = np.asmatrix(np.zeros((len(list1), len(list2))))\n    for i in range(len(list1)):\n        for j in range(len(list2)):\n            res[i, j] = matrix[list1[i], list2[j]]\n    return res\n\n# ------------\n#\n# creates a new matrix from the existing matrix elements.\n#\n# Example:\n#       l = matrix([[1, 2, 3],\n#                  [4, 5, 6]])\n#\n#       l.expand(3, 5, [0, 2], [0, 2, 3])\n#\n# results in:\n#\n#       [[1, 0, 2, 3, 0], \n#        [0, 0, 0, 0, 0], \n#        [4, 0, 5, 6, 0]]\n# \n# expand is used to introduce new rows and columns into an existing matrix\n# list1/list2 are the new indexes of row/columns in which the matrix\n# elements are being mapped. Elements for rows and columns \n# that are not listed in list1/list2 \n# will be initialized by 0.0.\n#\n\ndef expand(matrix, rows, cols, list1, list2 = []):\n    (m_rows, m_cols) = matrix.shape\n    if list2 == []:\n        list2 = list1\n    if len(list1) &gt; m_rows or len(list2) &gt; m_cols:\n        raise ValueError(\"list invalid in expand()\")\n\n    res = np.asmatrix(np.zeros((rows, cols)))\n    for i in range(len(list1)):\n        for j in range(len(list2)):\n            res[list1[i], list2[j]] = matrix[i,j]\n    return res\n</pre> # ------------ # # creates a new matrix from the existing matrix elements. # # Example: #       l = matrix([[ 1,  2,  3,  4,  5],  #                   [ 6,  7,  8,  9, 10],  #                   [11, 12, 13, 14, 15]]) # #       l.take([0, 2], [0, 2, 3]) # # results in: #        #       [[1, 3, 4],  #        [11, 13, 14]] #        #  # take is used to remove rows and columns from existing matrices # list1/list2 define a sequence of rows/columns that shall be taken # is no list2 is provided, then list2 is set to list1 (good for symmetric matrices) #  def take(matrix, list1, list2 = []):     (m_rows, m_cols) = matrix.shape     if list2 == []:         list2 = list1     if len(list1) &gt; m_rows or len(list2) &gt; m_cols:         raise ValueError(\"list invalid in take()\")      res = np.asmatrix(np.zeros((len(list1), len(list2))))     for i in range(len(list1)):         for j in range(len(list2)):             res[i, j] = matrix[list1[i], list2[j]]     return res  # ------------ # # creates a new matrix from the existing matrix elements. # # Example: #       l = matrix([[1, 2, 3], #                  [4, 5, 6]]) # #       l.expand(3, 5, [0, 2], [0, 2, 3]) # # results in: # #       [[1, 0, 2, 3, 0],  #        [0, 0, 0, 0, 0],  #        [4, 0, 5, 6, 0]] #  # expand is used to introduce new rows and columns into an existing matrix # list1/list2 are the new indexes of row/columns in which the matrix # elements are being mapped. Elements for rows and columns  # that are not listed in list1/list2  # will be initialized by 0.0. #  def expand(matrix, rows, cols, list1, list2 = []):     (m_rows, m_cols) = matrix.shape     if list2 == []:         list2 = list1     if len(list1) &gt; m_rows or len(list2) &gt; m_cols:         raise ValueError(\"list invalid in expand()\")      res = np.asmatrix(np.zeros((rows, cols)))     for i in range(len(list1)):         for j in range(len(list2)):             res[list1[i], list2[j]] = matrix[i,j]     return res In\u00a0[151]: Copied! <pre>l = np.matrix([[ 1,  2,  3,  4,  5], \n                   [ 6,  7,  8,  9, 10], \n                   [11, 12, 13, 14, 15]])\n\ntake(l, [0, 2], [0, 2, 3])\n#\n# results in:\n#       \n#       [[1, 3, 4], \n#        [11, 13, 14]]\n</pre> l = np.matrix([[ 1,  2,  3,  4,  5],                     [ 6,  7,  8,  9, 10],                     [11, 12, 13, 14, 15]])  take(l, [0, 2], [0, 2, 3]) # # results in: #        #       [[1, 3, 4],  #        [11, 13, 14]] Out[151]: <pre>matrix([[  1.,   3.,   4.],\n        [ 11.,  13.,  14.]])</pre> In\u00a0[152]: Copied! <pre>l = np.matrix([[1, 2, 3],\n               [4, 5, 6]])\n\nexpand(l, 3, 5, [0, 2], [0, 2, 3])\n\n# results in:\n#\n#       [[1, 0, 2, 3, 0], \n#        [0, 0, 0, 0, 0], \n#        [4, 0, 5, 6, 0]]\n</pre> l = np.matrix([[1, 2, 3],                [4, 5, 6]])  expand(l, 3, 5, [0, 2], [0, 2, 3])  # results in: # #       [[1, 0, 2, 3, 0],  #        [0, 0, 0, 0, 0],  #        [4, 0, 5, 6, 0]] Out[152]: <pre>matrix([[ 1.,  0.,  2.,  3.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 4.,  0.,  5.,  6.,  0.]])</pre> In\u00a0[153]: Copied! <pre>def initialize_constraints_online(num_landmarks, world_size):\n    ''' This function takes in a number of time steps N, number of landmarks, and a world_size,\n        and returns initialized constraint matrices, omega and xi.'''\n    \n    ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable\n    \n    # initialize constraint matrices with 0's\n    # 1 pose (x, y) and num_landmarks (x, y) \n    size = 2 * (1 + num_landmarks)\n    \n    # add initial pose constraint    \n    ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values\n    ## for the initial x, y location of our robot\n    omega = np.asmatrix(np.zeros((size, size)))\n    omega[0,0] = 1\n    omega[1,1] = 1\n    \n    ## TODO: Define the constraint *vector*, xi\n    ## you can assume that the robot starts out in the middle of the world with 100% confidence\n    xi = np.asmatrix(np.zeros((size, 1)))\n    xi[0] = world_size/2\n    xi[1] = world_size/2\n    \n    return omega, xi\n</pre> def initialize_constraints_online(num_landmarks, world_size):     ''' This function takes in a number of time steps N, number of landmarks, and a world_size,         and returns initialized constraint matrices, omega and xi.'''          ## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable          # initialize constraint matrices with 0's     # 1 pose (x, y) and num_landmarks (x, y)      size = 2 * (1 + num_landmarks)          # add initial pose constraint         ## TODO: Define the constraint matrix, Omega, with two initial \"strength\" values     ## for the initial x, y location of our robot     omega = np.asmatrix(np.zeros((size, size)))     omega[0,0] = 1     omega[1,1] = 1          ## TODO: Define the constraint *vector*, xi     ## you can assume that the robot starts out in the middle of the world with 100% confidence     xi = np.asmatrix(np.zeros((size, 1)))     xi[0] = world_size/2     xi[1] = world_size/2          return omega, xi In\u00a0[154]: Copied! <pre>def construct_constraints_sense_online(omega, xi, measurement, measurement_noise):\n    ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific measurement including its noise,\n        and returns constraint matrices, omega and xi.'''\n    \n    landmark_index = measurement[0]\n    dx = measurement[1]\n    dy = measurement[2]\n    \n    \n    strength = 1.0 / measurement_noise\n    \n    # incorporate sense constraint for x coordinate\n    x_idx = 0\n    lx_idx = 2 * (1 + landmark_index)\n    omega[x_idx,x_idx] += strength\n    omega[x_idx,lx_idx] += -strength\n    omega[lx_idx,x_idx] += -strength\n    omega[lx_idx,lx_idx] += strength\n    xi[x_idx] += -dx / measurement_noise\n    xi[lx_idx] += dx / measurement_noise\n    \n    # incorporate sense constraint for y coordinate\n    y_idx = 1\n    ly_idx = 2 * (1 + landmark_index) + 1\n    omega[y_idx,y_idx] += strength\n    omega[y_idx,ly_idx] += -strength\n    omega[ly_idx,y_idx] += -strength\n    omega[ly_idx,ly_idx] += strength\n    xi[y_idx] += -dy / measurement_noise\n    xi[ly_idx] += dy / measurement_noise\n    \n    return omega, xi\n</pre> def construct_constraints_sense_online(omega, xi, measurement, measurement_noise):     ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific measurement including its noise,         and returns constraint matrices, omega and xi.'''          landmark_index = measurement[0]     dx = measurement[1]     dy = measurement[2]               strength = 1.0 / measurement_noise          # incorporate sense constraint for x coordinate     x_idx = 0     lx_idx = 2 * (1 + landmark_index)     omega[x_idx,x_idx] += strength     omega[x_idx,lx_idx] += -strength     omega[lx_idx,x_idx] += -strength     omega[lx_idx,lx_idx] += strength     xi[x_idx] += -dx / measurement_noise     xi[lx_idx] += dx / measurement_noise          # incorporate sense constraint for y coordinate     y_idx = 1     ly_idx = 2 * (1 + landmark_index) + 1     omega[y_idx,y_idx] += strength     omega[y_idx,ly_idx] += -strength     omega[ly_idx,y_idx] += -strength     omega[ly_idx,ly_idx] += strength     xi[y_idx] += -dy / measurement_noise     xi[ly_idx] += dy / measurement_noise          return omega, xi In\u00a0[155]: Copied! <pre>def construct_constraints_move_online(omega, xi, motion, motion_noise):\n    ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific motion including its noise,\n        and returns constraint matrices, omega and xi.'''\n    \n    dx = motion[0]\n    dy = motion[1]\n    \n    strength = 1.0 / motion_noise #1 #/motion_noise\n    \n    # incorporate motion constraint for x coordinate\n    x0 = 0\n    x1 = x0 + 2\n    omega[x0,x0] += strength\n    omega[x0,x1] += -strength\n    omega[x1,x0] += -strength\n    omega[x1,x1] += strength\n    xi[x0] += -dx / motion_noise\n    xi[x1] += dx / motion_noise\n    \n    # incorporate motion constraint for y coordinate\n    y0 = 1\n    y1 = y0 + 2\n    omega[y0,y0] += strength\n    omega[y0,y1] += -strength\n    omega[y1,y0] += -strength\n    omega[y1,y1] += strength\n    xi[y0] += -dy / motion_noise\n    xi[y1] += dy / motion_noise\n    \n    return omega, xi\n</pre> def construct_constraints_move_online(omega, xi, motion, motion_noise):     ''' This function takes in a number of time steps and landmarks for the new constraint matrices, and a specific motion including its noise,         and returns constraint matrices, omega and xi.'''          dx = motion[0]     dy = motion[1]          strength = 1.0 / motion_noise #1 #/motion_noise          # incorporate motion constraint for x coordinate     x0 = 0     x1 = x0 + 2     omega[x0,x0] += strength     omega[x0,x1] += -strength     omega[x1,x0] += -strength     omega[x1,x1] += strength     xi[x0] += -dx / motion_noise     xi[x1] += dx / motion_noise          # incorporate motion constraint for y coordinate     y0 = 1     y1 = y0 + 2     omega[y0,y0] += strength     omega[y0,y1] += -strength     omega[y1,y0] += -strength     omega[y1,y1] += strength     xi[y0] += -dy / motion_noise     xi[y1] += dy / motion_noise          return omega, xi In\u00a0[156]: Copied! <pre>def online_slam_step(data, time_step, omega, xi, motion_noise, measurement_noise):\n    \n    k = time_step\n    measurements = data[k][0]\n    motion = data[k][1]\n\n\n    ## TODO: update the constraint matrix/vector to account for all *measurements*\n    ## this should be a series of additions that take into account the measurement noise\n    for measurement in measurements:  \n        omega, xi = construct_constraints_sense_online(omega, xi, measurement, measurement_noise)\n\n\n    # expand the information matrix and vector by one new position\n    size = 2 * (1 + num_landmarks)\n    list1 = [0, 1] + list(range(4, size+2))\n    omega = expand(omega, size+2, size+2, list1, list1)\n    xi = expand(xi, size+2, 1, list1, [0])\n\n    ## TODO: update the constraint matrix/vector to account for all *motion* and motion noise\n    omega, xi = construct_constraints_move_online(omega, xi, motion, motion_noise)\n\n    # factor out the previous pose\n    newlist = list(range(2, omega.shape[0]))\n    a = take(omega, [0, 1], newlist)\n    b = take(omega, [0, 1])\n    c = take(xi, [0, 1], [0])\n    #print(a.shape)\n    #print(b.shape)\n    #print(c.shape)\n    #print(np.linalg.inv(b).shape)\n\n    omega = take(omega, newlist) - a.transpose() * np.linalg.inv(b) * a\n    xi = take(xi, newlist, [0]) - a.transpose() * np.linalg.inv(b) * c\n    \n    return omega, xi\n</pre> def online_slam_step(data, time_step, omega, xi, motion_noise, measurement_noise):          k = time_step     measurements = data[k][0]     motion = data[k][1]       ## TODO: update the constraint matrix/vector to account for all *measurements*     ## this should be a series of additions that take into account the measurement noise     for measurement in measurements:           omega, xi = construct_constraints_sense_online(omega, xi, measurement, measurement_noise)       # expand the information matrix and vector by one new position     size = 2 * (1 + num_landmarks)     list1 = [0, 1] + list(range(4, size+2))     omega = expand(omega, size+2, size+2, list1, list1)     xi = expand(xi, size+2, 1, list1, [0])      ## TODO: update the constraint matrix/vector to account for all *motion* and motion noise     omega, xi = construct_constraints_move_online(omega, xi, motion, motion_noise)      # factor out the previous pose     newlist = list(range(2, omega.shape[0]))     a = take(omega, [0, 1], newlist)     b = take(omega, [0, 1])     c = take(xi, [0, 1], [0])     #print(a.shape)     #print(b.shape)     #print(c.shape)     #print(np.linalg.inv(b).shape)      omega = take(omega, newlist) - a.transpose() * np.linalg.inv(b) * a     xi = take(xi, newlist, [0]) - a.transpose() * np.linalg.inv(b) * c          return omega, xi In\u00a0[157]: Copied! <pre>def compute_pose_and_landmarks(omega, xi):\n    ## TODO: After iterating through all the data\n    ## Compute the best estimate of poses and landmark positions\n    ## using the formula, omega_inverse * Xi\n    try:\n        omega_inv = np.linalg.inv(omega)\n    except:\n        print(\"Computing (Moore-Penrose) pseudo-inverse\")\n        omega_inv = np.linalg.pinv(omega)\n        \n    mu = omega_inv*xi\n    return mu\n</pre> def compute_pose_and_landmarks(omega, xi):     ## TODO: After iterating through all the data     ## Compute the best estimate of poses and landmark positions     ## using the formula, omega_inverse * Xi     try:         omega_inv = np.linalg.inv(omega)     except:         print(\"Computing (Moore-Penrose) pseudo-inverse\")         omega_inv = np.linalg.pinv(omega)              mu = omega_inv*xi     return mu In\u00a0[158]: Copied! <pre>## slam takes in 6 arguments and returns mu, \n## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations\ndef online_slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise):\n    \n    ## TODO: Use your initilization to create constraint matrices, omega and xi\n    # initialize the constraints\n    omega, xi = initialize_constraints_online(num_landmarks, world_size)\n    \n    ## TODO: Iterate through each time step in the data\n    ## get all the motion and measurement data as you iterate\n    for time_step in range(len(data)):\n        omega, xi = online_slam_step(data, time_step, omega, xi, motion_noise, measurement_noise)\n        \n    \n    ## TODO: After iterating through all the data\n    ## Compute the best estimate of poses and landmark positions\n    ## using the formula, omega_inverse * Xi\n    mu = compute_pose_and_landmarks(omega, xi)\n    \n    return mu # return `mu`\n</pre> ## slam takes in 6 arguments and returns mu,  ## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations def online_slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise):          ## TODO: Use your initilization to create constraint matrices, omega and xi     # initialize the constraints     omega, xi = initialize_constraints_online(num_landmarks, world_size)          ## TODO: Iterate through each time step in the data     ## get all the motion and measurement data as you iterate     for time_step in range(len(data)):         omega, xi = online_slam_step(data, time_step, omega, xi, motion_noise, measurement_noise)                   ## TODO: After iterating through all the data     ## Compute the best estimate of poses and landmark positions     ## using the formula, omega_inverse * Xi     mu = compute_pose_and_landmarks(omega, xi)          return mu # return `mu` In\u00a0[159]: Copied! <pre># call your implementation of slam, passing in the necessary parameters\nmu = online_slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise)\n</pre> # call your implementation of slam, passing in the necessary parameters mu = online_slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise) In\u00a0[160]: Copied! <pre># print out the resulting landmarks and poses\nif(mu is not None):\n    # get the lists of poses and landmarks\n    # and print them out\n    pose, landmarks = get_pose_landmarks_online(mu)\n    print_all_online(pose, landmarks)\n</pre> # print out the resulting landmarks and poses if(mu is not None):     # get the lists of poses and landmarks     # and print them out     pose, landmarks = get_pose_landmarks_online(mu)     print_all_online(pose, landmarks) <pre>\n\nEstimated Pose:\n[82.204, 5.921]\n\n\nEstimated Landmarks:\n[18.364, 89.130]\n[69.980, 86.949]\n[91.585, 5.242]\n[51.569, 26.748]\n[63.195, 51.718]\n</pre> In\u00a0[161]: Copied! <pre>##  Test Case 1\n##\n# Estimated Pose(s):\n#     [50.000, 50.000]\n#     [37.858, 33.921]\n#     [25.905, 18.268]\n#     [13.524, 2.224]\n#     [27.912, 16.886]\n#     [42.250, 30.994]\n#     [55.992, 44.886]\n#     [70.749, 59.867]\n#     [85.371, 75.230]\n#     [73.831, 92.354]\n#     [53.406, 96.465]\n#     [34.370, 100.134]\n#     [48.346, 83.952]\n#     [60.494, 68.338]\n#     [73.648, 53.082]\n#     [86.733, 38.197]\n#     [79.983, 20.324]\n#     [72.515, 2.837]\n#     [54.993, 13.221]\n#     [37.164, 22.283]\n\n\n# Estimated Landmarks:\n#     [82.679, 13.435]\n#     [70.417, 74.203]\n#     [36.688, 61.431]\n#     [18.705, 66.136]\n#     [20.437, 16.983]\n\n\n### Uncomment the following three lines for test case 1 and compare the output to the values above ###\n\nmu_1 = online_slam(test_data1, 20, 5, 100.0, 2.0, 2.0)\npose_1, landmarks_1 = get_pose_landmarks_online(mu_1)\nprint_all_online(pose_1, landmarks_1)\n</pre> ##  Test Case 1 ## # Estimated Pose(s): #     [50.000, 50.000] #     [37.858, 33.921] #     [25.905, 18.268] #     [13.524, 2.224] #     [27.912, 16.886] #     [42.250, 30.994] #     [55.992, 44.886] #     [70.749, 59.867] #     [85.371, 75.230] #     [73.831, 92.354] #     [53.406, 96.465] #     [34.370, 100.134] #     [48.346, 83.952] #     [60.494, 68.338] #     [73.648, 53.082] #     [86.733, 38.197] #     [79.983, 20.324] #     [72.515, 2.837] #     [54.993, 13.221] #     [37.164, 22.283]   # Estimated Landmarks: #     [82.679, 13.435] #     [70.417, 74.203] #     [36.688, 61.431] #     [18.705, 66.136] #     [20.437, 16.983]   ### Uncomment the following three lines for test case 1 and compare the output to the values above ###  mu_1 = online_slam(test_data1, 20, 5, 100.0, 2.0, 2.0) pose_1, landmarks_1 = get_pose_landmarks_online(mu_1) print_all_online(pose_1, landmarks_1) <pre>\n\nEstimated Pose:\n[37.416, 22.317]\n\n\nEstimated Landmarks:\n[82.956, 13.539]\n[70.495, 74.141]\n[36.740, 61.281]\n[18.698, 66.060]\n[20.635, 16.875]\n</pre> In\u00a0[162]: Copied! <pre>##  Test Case 2\n##\n# Estimated Pose(s):\n#     [50.000, 50.000]\n#     [69.035, 45.061]\n#     [87.655, 38.971]\n#     [76.084, 55.541]\n#     [64.283, 71.684]\n#     [52.396, 87.887]\n#     [44.674, 68.948]\n#     [37.532, 49.680]\n#     [31.392, 30.893]\n#     [24.796, 12.012]\n#     [33.641, 26.440]\n#     [43.858, 43.560]\n#     [54.735, 60.659]\n#     [65.884, 77.791]\n#     [77.413, 94.554]\n#     [96.740, 98.020]\n#     [76.149, 99.586]\n#     [70.211, 80.580]\n#     [64.130, 61.270]\n#     [58.183, 42.175]\n\n\n# Estimated Landmarks:\n#     [76.777, 42.415]\n#     [85.109, 76.850]\n#     [13.687, 95.386]\n#     [59.488, 39.149]\n#     [69.283, 93.654]\n\n# data, N, num_landmarks, world_size, motion_noise, measurement_noise\nmu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0)\npose_2, landmarks_2 = get_pose_landmarks_online(mu_2)\nprint_all_online(pose_2, landmarks_2)\n</pre> ##  Test Case 2 ## # Estimated Pose(s): #     [50.000, 50.000] #     [69.035, 45.061] #     [87.655, 38.971] #     [76.084, 55.541] #     [64.283, 71.684] #     [52.396, 87.887] #     [44.674, 68.948] #     [37.532, 49.680] #     [31.392, 30.893] #     [24.796, 12.012] #     [33.641, 26.440] #     [43.858, 43.560] #     [54.735, 60.659] #     [65.884, 77.791] #     [77.413, 94.554] #     [96.740, 98.020] #     [76.149, 99.586] #     [70.211, 80.580] #     [64.130, 61.270] #     [58.183, 42.175]   # Estimated Landmarks: #     [76.777, 42.415] #     [85.109, 76.850] #     [13.687, 95.386] #     [59.488, 39.149] #     [69.283, 93.654]  # data, N, num_landmarks, world_size, motion_noise, measurement_noise mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0) pose_2, landmarks_2 = get_pose_landmarks_online(mu_2) print_all_online(pose_2, landmarks_2) <pre>\n\nEstimated Pose:\n[58.107, 42.628]\n\n\nEstimated Landmarks:\n[76.779, 42.887]\n[85.065, 77.438]\n[13.548, 95.652]\n[59.449, 39.595]\n[69.263, 94.240]\n</pre> In\u00a0[163]: Copied! <pre>from matplotlib.legend_handler import HandlerBase\nfrom matplotlib.legend_handler import HandlerLine2D, HandlerTuple\nfrom helpers import TextHandlerA\nfrom matplotlib.text import Text\n\n# data, N, num_landmarks, world_size, motion_noise, measurement_noise\nclass OnlineSLAM:\n    def __init__(self, ax, data, robot=None, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0):\n        self.ax = ax\n        self.data = data\n        self.robot = robot\n        self.landmarks = None\n        if robot is not None: \n            self.robot.path = np.array(robot.path)\n            self.landmarks = self.robot.landmarks\n            print(self.landmarks)\n            \n        self.motion_noise = motion_noise\n        self.measurement_noise = measurement_noise\n        \n        \n        # using seaborn, set background grid to gray\n        sns.set_style(\"dark\")\n\n        # Plot grid of values\n        world_grid = np.zeros((world_size+1, world_size+1))\n\n        # Set minor axes in between the labels\n        cols = world_size+1\n        rows = world_size+1\n\n        self.ax.set_xticks([x for x in range(1,cols)],minor=True)\n        self.ax.set_yticks([y for y in range(1,rows)],minor=True)\n\n        # Plot grid on minor axes in gray (width = 1)\n        plt.grid(which='minor',ls='-',lw=1, color='white')\n\n        # Plot grid on major axes in larger width\n        plt.grid(which='major',ls='-',lw=2, color='white')\n        \n        plt.xlim([0, world_size])\n        plt.ylim([0, world_size])\n        \n        \n    def init(self):\n        \n        self.robot_gt_poses = []\n        self.robot_gt_path = []\n        if self.robot is not None:\n            # Robot and its path\n            self.robot_gt_path, = self.ax.plot(self.robot.path[:,0], self.robot.path[:,1], color='g', alpha=0.3)\n            for i, pose in enumerate(self.robot.path):\n                self.robot_gt_circle = plt.Circle((pose[0], pose[1]), radius=1, color='g', alpha=0.3)\n                self.robot_gt_poses.append(self.ax.add_patch(self.robot_gt_circle))\n                label = ax.annotate(f\"{i}\", xy=(pose[0], pose[1]), fontsize=10, ha=\"center\", va='center')\n        \n        \n        # Landmarks (ground truth)\n        self.landmarks_gt = []\n        if self.landmarks is not None:\n            size = 1\n            for location in landmarks:\n                #print(location)\n                landmark_gt_rect = plt.Rectangle([location[0]-size, location[1]-size], 2*size, 2*size, lw=0.5, color='b', alpha=0.1)\n                self.landmarks_gt.append(self.ax.add_patch(landmark_gt_rect))\n        \n        \n        artists = self.robot_gt_poses + [self.robot_gt_path] + self.landmarks_gt\n        return artists\n        \n\n    def __call__(self, frame):\n        if frame == 0:\n            ## TODO: Use your initilization to create constraint matrices, omega and xi\n            # initialize the constraints\n            self.omega, self.xi = initialize_constraints_online(num_landmarks, world_size)\n\n            ## TODO: After iterating through all the data\n            ## Compute the best estimate of poses and landmark positions\n            ## using the formula, omega_inverse * Xi\n            mu = compute_pose_and_landmarks(self.omega, self.xi)\n\n            position, landmarks = get_pose_landmarks_online(mu)\n\n            self.robot_est_text = ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)\n            \n            \n            self.landmarks_est_text = []\n            self.laser_scans = []\n            \n            for i, landmark in enumerate(landmarks):\n                self.landmarks_est_text.append(self.ax.text(landmark[0], landmark[1], 'x', ha='center', va='center', color='purple', fontsize=20))\n\n            \n                # Draw laser from robot to observed landmarks\n                #laser_scan, = self.ax.plot([position[0], landmark[0]], [position[1], landmark[1]], color='k', linestyle='dashed')\n                #self.laser_scans.append(laser_scan)\n                \n            for i, landmark in enumerate(self.robot.landmarks):\n                # Draw laser from robot to observed landmarks\n                dx = landmark[0] - position[0]\n                dy = landmark[1] - position[1]\n                if abs(dx) &lt;= self.robot.measurement_range and abs(dy) &lt;= self.robot.measurement_range:\n                    laser_scan, = self.ax.plot([position[0], landmark[0]], [position[1], landmark[1]], color='k', linestyle='dashed')\n                else:\n                    laser_scan, = self.ax.plot([], [], color='k', linestyle='dashed')\n                self.laser_scans.append(laser_scan)\n            \n        \n        \n        else:\n            time_step = frame - 1\n            ## Iterate through each time step in the data\n            ## get all the motion and measurement data as you iterate\n            self.omega, self.xi = online_slam_step(self.data, time_step, self.omega, self.xi, self.motion_noise, self.measurement_noise)\n            #print(self.omega)\n\n            ## TODO: After iterating through all the data\n            ## Compute the best estimate of poses and landmark positions\n            ## using the formula, omega_inverse * Xi\n            mu = compute_pose_and_landmarks(self.omega, self.xi)\n\n            position, landmarks = get_pose_landmarks_online(mu)\n\n\n            # Create an 'o' character that represents the robot\n            # ha = horizontal alignment, va = vertical\n            #ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)\n            self.robot_est_text.set_x(position[0])\n            self.robot_est_text.set_y(position[1])\n\n            # Draw landmarks if they exists\n            if(landmarks is not None):\n                # loop through all path indices and draw a dot (unless it's at the car's location)\n                for i, landmark in enumerate(landmarks):\n                    if(landmark != position):\n                        #ax.text(pos[0], pos[1], 'x', ha='center', va='center', color='purple', fontsize=20)\n                        self.landmarks_est_text[i].set_x(landmark[0])\n                        self.landmarks_est_text[i].set_y(landmark[1])\n                \n                \n                for i, landmark in enumerate(self.robot.landmarks):\n                    # Draw laser from robot to observed landmarks\n                    dx = landmark[0] - position[0]\n                    dy = landmark[1] - position[1]\n                    if abs(dx) &lt;= self.robot.measurement_range and abs(dy) &lt;= self.robot.measurement_range:\n                        self.laser_scans[i].set_data([position[0], landmark[0]], [position[1], landmark[1]])\n                    else:\n                        self.laser_scans[i].set_data([], [])\n            \n\n        \n        print(self.robot_est_text, len(landmarks))\n        \n        self.ax.legend([(self.robot_gt_circle, self.robot_gt_path), self.robot_est_text, self.landmarks_gt[0], self.landmarks_est_text[0], self.laser_scans[0]], \n               ['Robot ground truth path', 'Robot final pose', 'Landmark ground truth', 'Landmark estimates', 'Laser scans'], \n               numpoints=1, handler_map={tuple: HandlerTuple(ndivide=None), Text: TextHandlerA()})\n        \n        \n        artists = [self.robot_est_text] + self.landmarks_est_text + self.laser_scans\n        return artists\n</pre> from matplotlib.legend_handler import HandlerBase from matplotlib.legend_handler import HandlerLine2D, HandlerTuple from helpers import TextHandlerA from matplotlib.text import Text  # data, N, num_landmarks, world_size, motion_noise, measurement_noise class OnlineSLAM:     def __init__(self, ax, data, robot=None, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0):         self.ax = ax         self.data = data         self.robot = robot         self.landmarks = None         if robot is not None:              self.robot.path = np.array(robot.path)             self.landmarks = self.robot.landmarks             print(self.landmarks)                      self.motion_noise = motion_noise         self.measurement_noise = measurement_noise                           # using seaborn, set background grid to gray         sns.set_style(\"dark\")          # Plot grid of values         world_grid = np.zeros((world_size+1, world_size+1))          # Set minor axes in between the labels         cols = world_size+1         rows = world_size+1          self.ax.set_xticks([x for x in range(1,cols)],minor=True)         self.ax.set_yticks([y for y in range(1,rows)],minor=True)          # Plot grid on minor axes in gray (width = 1)         plt.grid(which='minor',ls='-',lw=1, color='white')          # Plot grid on major axes in larger width         plt.grid(which='major',ls='-',lw=2, color='white')                  plt.xlim([0, world_size])         plt.ylim([0, world_size])                       def init(self):                  self.robot_gt_poses = []         self.robot_gt_path = []         if self.robot is not None:             # Robot and its path             self.robot_gt_path, = self.ax.plot(self.robot.path[:,0], self.robot.path[:,1], color='g', alpha=0.3)             for i, pose in enumerate(self.robot.path):                 self.robot_gt_circle = plt.Circle((pose[0], pose[1]), radius=1, color='g', alpha=0.3)                 self.robot_gt_poses.append(self.ax.add_patch(self.robot_gt_circle))                 label = ax.annotate(f\"{i}\", xy=(pose[0], pose[1]), fontsize=10, ha=\"center\", va='center')                           # Landmarks (ground truth)         self.landmarks_gt = []         if self.landmarks is not None:             size = 1             for location in landmarks:                 #print(location)                 landmark_gt_rect = plt.Rectangle([location[0]-size, location[1]-size], 2*size, 2*size, lw=0.5, color='b', alpha=0.1)                 self.landmarks_gt.append(self.ax.add_patch(landmark_gt_rect))                           artists = self.robot_gt_poses + [self.robot_gt_path] + self.landmarks_gt         return artists               def __call__(self, frame):         if frame == 0:             ## TODO: Use your initilization to create constraint matrices, omega and xi             # initialize the constraints             self.omega, self.xi = initialize_constraints_online(num_landmarks, world_size)              ## TODO: After iterating through all the data             ## Compute the best estimate of poses and landmark positions             ## using the formula, omega_inverse * Xi             mu = compute_pose_and_landmarks(self.omega, self.xi)              position, landmarks = get_pose_landmarks_online(mu)              self.robot_est_text = ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)                                       self.landmarks_est_text = []             self.laser_scans = []                          for i, landmark in enumerate(landmarks):                 self.landmarks_est_text.append(self.ax.text(landmark[0], landmark[1], 'x', ha='center', va='center', color='purple', fontsize=20))                               # Draw laser from robot to observed landmarks                 #laser_scan, = self.ax.plot([position[0], landmark[0]], [position[1], landmark[1]], color='k', linestyle='dashed')                 #self.laser_scans.append(laser_scan)                              for i, landmark in enumerate(self.robot.landmarks):                 # Draw laser from robot to observed landmarks                 dx = landmark[0] - position[0]                 dy = landmark[1] - position[1]                 if abs(dx) &lt;= self.robot.measurement_range and abs(dy) &lt;= self.robot.measurement_range:                     laser_scan, = self.ax.plot([position[0], landmark[0]], [position[1], landmark[1]], color='k', linestyle='dashed')                 else:                     laser_scan, = self.ax.plot([], [], color='k', linestyle='dashed')                 self.laser_scans.append(laser_scan)                                        else:             time_step = frame - 1             ## Iterate through each time step in the data             ## get all the motion and measurement data as you iterate             self.omega, self.xi = online_slam_step(self.data, time_step, self.omega, self.xi, self.motion_noise, self.measurement_noise)             #print(self.omega)              ## TODO: After iterating through all the data             ## Compute the best estimate of poses and landmark positions             ## using the formula, omega_inverse * Xi             mu = compute_pose_and_landmarks(self.omega, self.xi)              position, landmarks = get_pose_landmarks_online(mu)               # Create an 'o' character that represents the robot             # ha = horizontal alignment, va = vertical             #ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)             self.robot_est_text.set_x(position[0])             self.robot_est_text.set_y(position[1])              # Draw landmarks if they exists             if(landmarks is not None):                 # loop through all path indices and draw a dot (unless it's at the car's location)                 for i, landmark in enumerate(landmarks):                     if(landmark != position):                         #ax.text(pos[0], pos[1], 'x', ha='center', va='center', color='purple', fontsize=20)                         self.landmarks_est_text[i].set_x(landmark[0])                         self.landmarks_est_text[i].set_y(landmark[1])                                                   for i, landmark in enumerate(self.robot.landmarks):                     # Draw laser from robot to observed landmarks                     dx = landmark[0] - position[0]                     dy = landmark[1] - position[1]                     if abs(dx) &lt;= self.robot.measurement_range and abs(dy) &lt;= self.robot.measurement_range:                         self.laser_scans[i].set_data([position[0], landmark[0]], [position[1], landmark[1]])                     else:                         self.laser_scans[i].set_data([], [])                                print(self.robot_est_text, len(landmarks))                  self.ax.legend([(self.robot_gt_circle, self.robot_gt_path), self.robot_est_text, self.landmarks_gt[0], self.landmarks_est_text[0], self.laser_scans[0]],                 ['Robot ground truth path', 'Robot final pose', 'Landmark ground truth', 'Landmark estimates', 'Laser scans'],                 numpoints=1, handler_map={tuple: HandlerTuple(ndivide=None), Text: TextHandlerA()})                           artists = [self.robot_est_text] + self.landmarks_est_text + self.laser_scans         return artists In\u00a0[164]: Copied! <pre># define figure size\nplt.rcParams[\"figure.figsize\"] = (10,10)    \n\nfig, ax = plt.subplots()\n# data, N, num_landmarks, world_size, motion_noise, measurement_noise\n# mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0)\nud = OnlineSLAM(ax, data, robot=robot, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0)\nplt.close()\n</pre> # define figure size plt.rcParams[\"figure.figsize\"] = (10,10)      fig, ax = plt.subplots() # data, N, num_landmarks, world_size, motion_noise, measurement_noise # mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0) ud = OnlineSLAM(ax, data, robot=robot, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0) plt.close() <pre>[[18, 89], [69, 87], [90, 4], [50, 26], [62, 51]]\n</pre> In\u00a0[165]: Copied! <pre>from matplotlib.animation import FuncAnimation\n</pre> from matplotlib.animation import FuncAnimation In\u00a0[166]: Copied! <pre>anim_html = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True)\n</pre> anim_html = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True) In\u00a0[167]: Copied! <pre>from IPython.display import HTML\nHTML(anim_html.to_html5_video())\n</pre> from IPython.display import HTML HTML(anim_html.to_html5_video()) <pre>Computing (Moore-Penrose) pseudo-inverse\nText(50,50,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(49.6242,69.9965,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(49.2099,90.0445,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(64.8832,78.6377,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(81.2858,65.7794,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(97.7488,52.8012,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(80.2032,42.0594,'o') 5\nText(63.2944,30.2655,'o') 5\nText(45.4809,18.0168,'o') 5\nText(27.9926,8.06578,'o') 5\nText(32.1712,28.9251,'o') 5\nText(36.7148,48.5267,'o') 5\nText(41.8487,66.9221,'o') 5\nText(49.0019,85.2264,'o') 5\nText(31.3999,94.0156,'o') 5\nText(40.5245,75.8164,'o') 5\nText(49.9822,59.7349,'o') 5\nText(61.2975,42.9729,'o') 5\nText(71.1077,24.2067,'o') 5\nText(82.2039,5.92124,'o') 5\n</pre> Out[167]:    Your browser does not support the video tag.  In\u00a0[168]: Copied! <pre>plt.rcParams[\"animation.html\"]\n</pre> plt.rcParams[\"animation.html\"] Out[168]: <pre>'jshtml'</pre> In\u00a0[169]: Copied! <pre># https://stackoverflow.com/a/47138474/2137370\n%matplotlib inline\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n# define figure size\nplt.rcParams[\"figure.figsize\"] = (10,10)    \n\nfig, ax = plt.subplots()\n# data, N, num_landmarks, world_size, motion_noise, measurement_noise\n# mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0)\nud = OnlineSLAM(ax, data, robot=robot, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0)\nplt.close()\n</pre> # https://stackoverflow.com/a/47138474/2137370 %matplotlib inline plt.rcParams[\"animation.html\"] = \"jshtml\" # define figure size plt.rcParams[\"figure.figsize\"] = (10,10)      fig, ax = plt.subplots() # data, N, num_landmarks, world_size, motion_noise, measurement_noise # mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0) ud = OnlineSLAM(ax, data, robot=robot, num_landmarks=5, world_size=100, motion_noise=2.0, measurement_noise=2.0) plt.close() <pre>[[18, 89], [69, 87], [90, 4], [50, 26], [62, 51]]\n</pre> In\u00a0[170]: Copied! <pre>anim = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True)\nanim\n</pre> anim = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True) anim <pre>Computing (Moore-Penrose) pseudo-inverse\nText(50,50,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(49.6242,69.9965,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(49.2099,90.0445,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(64.8832,78.6377,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(81.2858,65.7794,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(97.7488,52.8012,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(80.2032,42.0594,'o') 5\nText(63.2944,30.2655,'o') 5\nText(45.4809,18.0168,'o') 5\nText(27.9926,8.06578,'o') 5\nText(32.1712,28.9251,'o') 5\nText(36.7148,48.5267,'o') 5\nText(41.8487,66.9221,'o') 5\nText(49.0019,85.2264,'o') 5\nText(31.3999,94.0156,'o') 5\nText(40.5245,75.8164,'o') 5\nText(49.9822,59.7349,'o') 5\nText(61.2975,42.9729,'o') 5\nText(71.1077,24.2067,'o') 5\nText(82.2039,5.92124,'o') 5\n</pre> Out[170]:  Once       Loop       Reflect     In\u00a0[177]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[178]: Copied! <pre>from robot_class import robot\n\nnum_landmarks = 5\nr = robot()\nr.make_deterministic_landmarks(num_landmarks)\nr.landmarks\n</pre> from robot_class import robot  num_landmarks = 5 r = robot() r.make_deterministic_landmarks(num_landmarks) r.landmarks Out[178]: <pre>[[10.0, 10.0], [10.0, 50.0], [10.0, 90.0], [90.0, 10.0], [90.0, 50.0]]</pre> In\u00a0[179]: Copied! <pre>from robot_class import robot\nfrom math import *\nimport random\n\n# --------\n# this routine makes the robot data\n# the data is a list of measurements and movements: [measurements, [dx, dy]]\n# collected over a specified number of time steps, N\n#\ndef make_non_random_data(N, num_landmarks, world_size, measurement_range, motion_noise, \n              measurement_noise, distance):\n\n    # check that data has been made\n    try:\n        check_for_data(num_landmarks, world_size, measurement_range, motion_noise, measurement_noise)\n    except ValueError:\n        print('Error: You must implement the sense function in robot_class.py.')\n        return []\n    \n    complete = False\n    \n    r = robot(world_size, measurement_range, motion_noise, measurement_noise)\n    r.make_deterministic_landmarks(num_landmarks)\n\n    while not complete:\n\n        data = []\n\n        seen = [False for row in range(num_landmarks)]\n    \n        # guess an initial motion\n        #orientation = random.random() * 2.0 * pi\n        orientation = 0 * 2.0 * pi\n        dx = cos(orientation) * distance\n        dy = sin(orientation) * distance\n            \n        for k in range(N-1):\n    \n            # collect sensor measurements in a list, Z\n            Z = r.sense()\n\n            # check off all landmarks that were observed \n            for i in range(len(Z)):\n                seen[Z[i][0]] = True\n    \n            # move\n            #o = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n            #o = o + o\n            while not r.move(dx, dy):\n                # if we'd be leaving the robot world, pick instead a new direction\n                orientation = random.random() * 2.0 * pi\n                #orientation = o[k] * 2.0 * pi\n                dx = cos(orientation) * distance\n                dy = sin(orientation) * distance\n\n            # collect/memorize all sensor and motion data\n            data.append([Z, [dx, dy]])\n\n        # we are done when all landmarks were observed; otherwise re-run\n        complete = (sum(seen) == num_landmarks)\n\n    print(' ')\n    print('Landmarks: ', r.landmarks)\n    print(r)\n\n\n    return data, r\n\n\ndef check_for_data(num_landmarks, world_size, measurement_range, motion_noise, measurement_noise):\n    # make robot and landmarks\n    r = robot(world_size, measurement_range, motion_noise, measurement_noise)\n    r.make_deterministic_landmarks(num_landmarks)\n    \n    \n    # check that sense has been implemented/data has been made\n    test_Z = r.sense()\n    if(test_Z is None):\n        raise ValueError\n</pre> from robot_class import robot from math import * import random  # -------- # this routine makes the robot data # the data is a list of measurements and movements: [measurements, [dx, dy]] # collected over a specified number of time steps, N # def make_non_random_data(N, num_landmarks, world_size, measurement_range, motion_noise,                measurement_noise, distance):      # check that data has been made     try:         check_for_data(num_landmarks, world_size, measurement_range, motion_noise, measurement_noise)     except ValueError:         print('Error: You must implement the sense function in robot_class.py.')         return []          complete = False          r = robot(world_size, measurement_range, motion_noise, measurement_noise)     r.make_deterministic_landmarks(num_landmarks)      while not complete:          data = []          seen = [False for row in range(num_landmarks)]              # guess an initial motion         #orientation = random.random() * 2.0 * pi         orientation = 0 * 2.0 * pi         dx = cos(orientation) * distance         dy = sin(orientation) * distance                      for k in range(N-1):                  # collect sensor measurements in a list, Z             Z = r.sense()              # check off all landmarks that were observed              for i in range(len(Z)):                 seen[Z[i][0]] = True                  # move             #o = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]             #o = o + o             while not r.move(dx, dy):                 # if we'd be leaving the robot world, pick instead a new direction                 orientation = random.random() * 2.0 * pi                 #orientation = o[k] * 2.0 * pi                 dx = cos(orientation) * distance                 dy = sin(orientation) * distance              # collect/memorize all sensor and motion data             data.append([Z, [dx, dy]])          # we are done when all landmarks were observed; otherwise re-run         complete = (sum(seen) == num_landmarks)      print(' ')     print('Landmarks: ', r.landmarks)     print(r)       return data, r   def check_for_data(num_landmarks, world_size, measurement_range, motion_noise, measurement_noise):     # make robot and landmarks     r = robot(world_size, measurement_range, motion_noise, measurement_noise)     r.make_deterministic_landmarks(num_landmarks)               # check that sense has been implemented/data has been made     test_Z = r.sense()     if(test_Z is None):         raise ValueError In\u00a0[180]: Copied! <pre># your implementation of slam should work with the following inputs\n# feel free to change these input values and see how it responds!\n\n# world parameters\nnum_landmarks      = 5        # number of landmarks\nN                  = 20       # time steps\nworld_size         = 100.0    # size of world (square)\n\n# robot parameters\nmeasurement_range  = 40.0     # range at which we can sense landmarks\nmotion_noise       = 2.0      # noise in robot motion\nmeasurement_noise  = 2.0      # noise in the measurements\ndistance           = 20.0     # distance by which robot (intends to) move each iteratation \n\n\n# make_data instantiates a robot, AND generates random landmarks for a given world size and number of landmarks\ndata, robot = make_non_random_data(N, num_landmarks, world_size, measurement_range, motion_noise, measurement_noise, distance)\n</pre> # your implementation of slam should work with the following inputs # feel free to change these input values and see how it responds!  # world parameters num_landmarks      = 5        # number of landmarks N                  = 20       # time steps world_size         = 100.0    # size of world (square)  # robot parameters measurement_range  = 40.0     # range at which we can sense landmarks motion_noise       = 2.0      # noise in robot motion measurement_noise  = 2.0      # noise in the measurements distance           = 20.0     # distance by which robot (intends to) move each iteratation    # make_data instantiates a robot, AND generates random landmarks for a given world size and number of landmarks data, robot = make_non_random_data(N, num_landmarks, world_size, measurement_range, motion_noise, measurement_noise, distance) <pre> \nLandmarks:  [[10.0, 10.0], [10.0, 50.0], [10.0, 90.0], [90.0, 10.0], [90.0, 50.0]]\nRobot: [x=52.52432 y=71.44035]\n</pre> In\u00a0[181]: Copied! <pre># call your implementation of slam, passing in the necessary parameters\nmu = slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise)\n\n# print out the resulting landmarks and poses\nif(mu is not None):\n    # get the lists of poses and landmarks\n    # and print them out\n    poses, landmarks = get_poses_landmarks(mu, N)\n    print_all(poses, landmarks)\n</pre> # call your implementation of slam, passing in the necessary parameters mu = slam(data, N, num_landmarks, world_size, motion_noise, measurement_noise)  # print out the resulting landmarks and poses if(mu is not None):     # get the lists of poses and landmarks     # and print them out     poses, landmarks = get_poses_landmarks(mu, N)     print_all(poses, landmarks) <pre>\n\nEstimated Poses:\n[50.000, 50.000]\n[69.528, 49.516]\n[90.579, 49.646]\n[70.996, 55.000]\n[53.275, 58.891]\n[35.255, 62.487]\n[15.488, 66.598]\n[8.519, 47.615]\n[1.766, 28.629]\n[22.054, 31.112]\n[41.874, 33.695]\n[61.903, 37.520]\n[80.723, 42.557]\n[99.481, 45.778]\n[97.800, 25.893]\n[97.134, 5.985]\n[84.699, 23.041]\n[73.154, 39.108]\n[61.523, 55.916]\n[50.098, 72.332]\n\n\nEstimated Landmarks:\n[10.707, 10.007]\n[10.406, 51.815]\n[9.920, 90.045]\n[90.765, 11.672]\n[90.054, 51.219]\n</pre> In\u00a0[182]: Copied! <pre># Display the final world!\n\n# define figure size\nplt.rcParams[\"figure.figsize\"] = (10,10)\n\n# check if poses has been created\nif 'poses' in locals():\n    # print out the last pose\n    print('Last pose: ', poses[-1])\n    print('True last pose: ', robot.path[-1])\n    # display the last position of the robot *and* the landmark positions\n    display_world(int(world_size), poses[-1], landmarks)\n    display_world_extended(int(world_size), poses[-1], landmarks, robot)\n</pre> # Display the final world!  # define figure size plt.rcParams[\"figure.figsize\"] = (10,10)  # check if poses has been created if 'poses' in locals():     # print out the last pose     print('Last pose: ', poses[-1])     print('True last pose: ', robot.path[-1])     # display the last position of the robot *and* the landmark positions     display_world(int(world_size), poses[-1], landmarks)     display_world_extended(int(world_size), poses[-1], landmarks, robot) <pre>Last pose:  (50.09830494951987, 72.33173953448954)\nTrue last pose:  [52.524320896194645, 71.44034682637752]\n</pre> In\u00a0[183]: Copied! <pre># define figure size\nplt.rcParams[\"figure.figsize\"] = (10,10)    \n\nfig, ax = plt.subplots()\n# data, N, num_landmarks, world_size, motion_noise, measurement_noise\n# mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0)\nud = OnlineSLAM(ax, data, robot, num_landmarks, int(world_size), motion_noise, measurement_noise)\nplt.close()\n</pre> # define figure size plt.rcParams[\"figure.figsize\"] = (10,10)      fig, ax = plt.subplots() # data, N, num_landmarks, world_size, motion_noise, measurement_noise # mu_2 = online_slam(test_data2, 20, 5, 100.0, 2.0, 2.0) ud = OnlineSLAM(ax, data, robot, num_landmarks, int(world_size), motion_noise, measurement_noise) plt.close() <pre>[[10.0, 10.0], [10.0, 50.0], [10.0, 90.0], [90.0, 10.0], [90.0, 50.0]]\n</pre> In\u00a0[184]: Copied! <pre>anim_html = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True)\n</pre> anim_html = FuncAnimation(fig, ud, init_func=ud.init, frames=len(data)+1, interval=1000, blit=True) In\u00a0[185]: Copied! <pre>from IPython.display import HTML\nHTML(anim_html.to_html5_video())\n</pre> from IPython.display import HTML HTML(anim_html.to_html5_video()) <pre>Computing (Moore-Penrose) pseudo-inverse\nText(50,50,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(70,50,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(89.2718,49.4945,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(70.5281,53.2742,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(49.7275,59.1074,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(32.0338,63.1119,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(15.6308,66.4505,'o') 5\nComputing (Moore-Penrose) pseudo-inverse\nText(9.37764,47.6468,'o') 5\nText(2.98157,28.9266,'o') 5\nText(20.8408,33.4502,'o') 5\nText(41.0721,35.8401,'o') 5\nText(61.0277,38.0193,'o') 5\nText(81.5409,41.3692,'o') 5\nText(100.435,46.8982,'o') 5\nText(98.7807,25.582,'o') 5\nText(96.7986,5.8083,'o') 5\nText(86.2374,21.8918,'o') 5\nText(73.2503,39.4892,'o') 5\nText(61.8124,55.366,'o') 5\nText(50.0983,72.3317,'o') 5\n</pre> Out[185]:    Your browser does not support the video tag."},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#project-3-implement-slam","title":"Project 3:  Implement SLAM\u00b6","text":""},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#project-overview","title":"Project Overview\u00b6","text":"<p>In this project, you'll implement SLAM for robot that moves and senses in a 2 dimensional, grid world!</p> <p>SLAM gives us a way to both localize a robot and build up a map of its environment as a robot moves and senses in real-time. This is an active area of research in the fields of robotics and autonomous systems. Since this localization and map-building relies on the visual sensing of landmarks, this is a computer vision problem.</p> <p>Using what you've learned about robot motion, representations of uncertainty in motion and sensing, and localization techniques, you will be tasked with defining a function, <code>slam</code>, which takes in six parameters as input and returns the vector <code>mu</code>.</p> <p><code>mu</code> contains the (x,y) coordinate locations of the robot as it moves, and the positions of landmarks that it senses in the world</p> <p>You can implement helper functions as you see fit, but your function must return <code>mu</code>. The vector, <code>mu</code>, should have (x, y) coordinates interlaced, for example, if there were 2 poses and 2 landmarks, <code>mu</code> will look like the following, where <code>P</code> is the robot position and <code>L</code> the landmark position:</p> <pre><code>mu =  matrix([[Px0],\n              [Py0],\n              [Px1],\n              [Py1],\n              [Lx0],\n              [Ly0],\n              [Lx1],\n              [Ly1]])\n</code></pre> <p>You can see that <code>mu</code> holds the poses first <code>(x0, y0), (x1, y1), ...,</code> then the landmark locations at the end of the matrix; we consider a <code>nx1</code> matrix to be a vector.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#generating-an-environment","title":"Generating an environment\u00b6","text":"<p>In a real SLAM problem, you may be given a map that contains information about landmark locations, and in this example, we will make our own data using the <code>make_data</code> function, which generates a world grid with landmarks in it and then generates data by placing a robot in that world and moving and sensing over some numer of time steps. The <code>make_data</code> function relies on a correct implementation of robot move/sense functions, which, at this point, should be complete and in the <code>robot_class.py</code> file. The data is collected as an instantiated robot moves and senses in a world. Your SLAM function will take in this data as input. So, let's first create this data and explore how it represents the movement and sensor measurements that our robot takes.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#create-the-world","title":"Create the world\u00b6","text":"<p>Use the code below to generate a world of a specified size with randomly generated landmark locations. You can change these parameters and see how your implementation of SLAM responds!</p> <p><code>data</code> holds the sensors measurements and motion of your robot over time. It stores the measurements as <code>data[i][0]</code> and the motion as <code>data[i][1]</code>.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#helper-functions","title":"Helper functions\u00b6","text":"<p>You will be working with the <code>robot</code> class that may look familiar from the first notebook,</p> <p>In fact, in the <code>helpers.py</code> file, you can read the details of how data is made with the <code>make_data</code> function. It should look very similar to the robot move/sense cycle you've seen in the first notebook.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#a-note-on-make_data","title":"A note on <code>make_data</code>\u00b6","text":"<p>The function above, <code>make_data</code>, takes in so many world and robot motion/sensor parameters because it is responsible for:</p> <ol> <li>Instantiating a robot (using the robot class)</li> <li>Creating a grid world with landmarks in it</li> </ol> <p>This function also prints out the true location of landmarks and the final robot location, which you should refer back to when you test your implementation of SLAM.</p> <p>The <code>data</code> this returns is an array that holds information about robot sensor measurements and robot motion <code>(dx, dy)</code> that is collected over a number of time steps, <code>N</code>. You will have to use only these readings about motion and measurements to track a robot over time and find the determine the location of the landmarks using SLAM. We only print out the true landmark locations for comparison, later.</p> <p>In <code>data</code> the measurement and motion data can be accessed from the first and second index in the columns of the data array. See the following code for an example, where <code>i</code> is the time step:</p> <pre><code>measurement = data[i][0]\nmotion = data[i][1]\n</code></pre>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#initialize-constraints","title":"Initialize Constraints\u00b6","text":"<p>One of the most challenging tasks here will be to create and modify the constraint matrix and vector: omega and xi. In the second notebook, you saw an example of how omega and xi could hold all the values that define the relationships between robot poses <code>xi</code> and landmark positions <code>Li</code> in a 1D world, as seen below, where omega is the blue matrix and xi is the pink vector.</p> <p></p> <p>In this project, you are tasked with implementing constraints for a 2D world. We are referring to robot poses as <code>Px, Py</code> and landmark positions as <code>Lx, Ly</code>, and one way to approach this challenge is to add both x and y locations in the constraint matrices.</p> <p></p> <p>You may also choose to create two of each omega and xi (one for x and one for y positions).</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#todo-write-a-function-that-initializes-omega-and-xi","title":"TODO: Write a function that initializes omega and xi\u00b6","text":"<p>Complete the function <code>initialize_constraints</code> so that it returns <code>omega</code> and <code>xi</code> constraints for the starting position of the robot. Any values that we do not yet know should be initialized with the value <code>0</code>. You may assume that our robot starts out in exactly the middle of the world with 100% confidence (no motion or measurement noise at this point). The inputs <code>N</code> time steps, <code>num_landmarks</code>, and <code>world_size</code> should give you all the information you need to construct intial constraints of the correct size and starting values.</p> <p>Depending on your approach you may choose to return one omega and one xi that hold all (x,y) positions *or two of each (one for x values and one for y); choose whichever makes most sense to you!*</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#test-as-you-go","title":"Test as you go\u00b6","text":"<p>It's good practice to test out your code, as you go. Since <code>slam</code> relies on creating and updating constraint matrices, <code>omega</code> and <code>xi</code> to account for robot sensor measurements and motion, let's check that they initialize as expected for any given parameters.</p> <p>Below, you'll find some test code that allows you to visualize the results of your function <code>initialize_constraints</code>. We are using the seaborn library for visualization.</p> <p>Please change the test values of N, landmarks, and world_size and see the results. Be careful not to use these values as input into your final slam function.</p> <p>This code assumes that you have created one of each constraint: <code>omega</code> and <code>xi</code>, but you can change and add to this code, accordingly. The constraints should vary in size with the number of time steps and landmarks as these values affect the number of poses a robot will take <code>(Px0,Py0,...Pxn,Pyn)</code> and landmark locations <code>(Lx0,Ly0,...Lxn,Lyn)</code> whose relationships should be tracked in the constraint matrices. Recall that <code>omega</code> holds the weights of each variable and <code>xi</code> holds the value of the sum of these variables, as seen in Notebook 2. You'll need the <code>world_size</code> to determine the starting pose of the robot in the world and fill in the initial values for <code>xi</code>.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#slam-inputs","title":"SLAM inputs\u00b6","text":"<p>In addition to <code>data</code>, your slam function will also take in:</p> <ul> <li>N -   The number of time steps that a robot will be moving and sensing</li> <li>num_landmarks - The number of landmarks in the world</li> <li>world_size - The size (w/h) of your world</li> <li>motion_noise - The noise associated with motion; the update confidence for motion should be <code>1.0/motion_noise</code></li> <li>measurement_noise - The noise associated with measurement/sensing; the update weight for measurement should be <code>1.0/measurement_noise</code></li> </ul>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#a-note-on-noise","title":"A note on noise\u00b6","text":"<p>Recall that <code>omega</code> holds the relative \"strengths\" or weights for each position variable, and you can update these weights by accessing the correct index in omega <code>omega[row][col]</code> and adding/subtracting <code>1.0/noise</code> where <code>noise</code> is measurement or motion noise. <code>Xi</code> holds actual position values, and so to update <code>xi</code> you'll do a similar addition process only using the actual value of a motion or measurement. So for a vector index <code>xi[row][0]</code> you will end up adding/subtracting one measurement or motion divided by their respective <code>noise</code>.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#todo-implement-graph-slam","title":"TODO: Implement Graph SLAM\u00b6","text":"<p>Follow the TODO's below to help you complete this slam implementation (these TODO's are in the recommended order), then test out your implementation!</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#updating-with-motion-and-measurements","title":"Updating with motion and measurements\u00b6","text":"<p>With a 2D omega and xi structure as shown above (in earlier cells), you'll have to be mindful about how you update the values in these constraint matrices to account for motion and measurement constraints in the x and y directions. Recall that the solution to these matrices (which holds all values for robot poses <code>P</code> and landmark locations <code>L</code>) is the vector, <code>mu</code>, which can be computed at the end of the construction of omega and xi as the inverse of omega times xi: $\\mu = \\Omega^{-1}\\xi$</p> <p>You may also choose to return the values of <code>omega</code> and <code>xi</code> if you want to visualize their final state!</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#helper-functions","title":"Helper functions\u00b6","text":"<p>To check that your implementation of SLAM works for various inputs, we have provided two helper functions that will help display the estimated pose and landmark locations that your function has produced. First, given a result <code>mu</code> and number of time steps, <code>N</code>, we define a function that extracts the poses and landmarks locations and returns those as their own, separate lists.</p> <p>Then, we define a function that nicely print out these lists; both of these we will call, in the next step.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#run-slam","title":"Run SLAM\u00b6","text":"<p>Once you've completed your implementation of <code>slam</code>, see what <code>mu</code> it returns for different world sizes and different landmarks!</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#what-to-expect","title":"What to Expect\u00b6","text":"<p>The <code>data</code> that is generated is random, but you did specify the number, <code>N</code>, or time steps that the robot was expected to move and the <code>num_landmarks</code> in the world (which your implementation of <code>slam</code> should see and estimate a position for. Your robot should also start with an estimated pose in the very center of your square world, whose size is defined by <code>world_size</code>.</p> <p>With these values in mind, you should expect to see a result that displays two lists:</p> <ol> <li>Estimated poses, a list of (x, y) pairs that is exactly <code>N</code> in length since this is how many motions your robot has taken. The very first pose should be the center of your world, i.e. <code>[50.000, 50.000]</code> for a world that is 100.0 in square size.</li> <li>Estimated landmarks, a list of landmark positions (x, y) that is exactly <code>num_landmarks</code> in length.</li> </ol>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#landmark-locations","title":"Landmark Locations\u00b6","text":"<p>If you refer back to the printout of exact landmark locations when this data was created, you should see values that are very similar to those coordinates, but not quite (since <code>slam</code> must account for noise in motion and measurement).</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#visualize-the-constructed-world","title":"Visualize the constructed world\u00b6","text":"<p>Finally, using the <code>display_world</code> code from the <code>helpers.py</code> file (which was also used in the first notebook), we can actually visualize what you have coded with <code>slam</code>: the final position of the robot and the positon of landmarks, created from only motion and measurement data!</p> <p>Note that these should be very similar to the printed true landmark locations and final pose from our call to <code>make_data</code> early in this notebook.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#question-how-far-away-is-your-final-pose-as-estimated-by-slam-compared-to-the-true-final-pose-why-do-you-think-these-poses-are-different","title":"Question: How far away is your final pose (as estimated by <code>slam</code>) compared to the true final pose? Why do you think these poses are different?\u00b6","text":"<p>You can find the true value of the final pose in one of the first cells where <code>make_data</code> was called. You may also want to look at the true landmark locations and compare them to those that were estimated by <code>slam</code>. Ask yourself: what do you think would happen if we moved and sensed more (increased N)? Or if we had lower/higher noise parameters.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#testing","title":"Testing\u00b6","text":"<p>To confirm that your slam code works before submitting your project, it is suggested that you run it on some test data and cases. A few such cases have been provided for you, in the cells below. When you are ready, uncomment the test cases in the next cells (there are two test cases, total); your output should be close-to or exactly identical to the given results. If there are minor discrepancies it could be a matter of floating point accuracy or in the calculation of the inverse matrix.</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#submit-your-project","title":"Submit your project\u00b6","text":"<p>If you pass these tests, it is a good indication that your project will pass all the specifications in the project rubric. Follow the submission instructions to officially submit!</p>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#todos","title":"TODOs\u00b6","text":"<ul> <li>Create a new version of <code>slam</code> in which <code>omega</code> only keeps track of the latest robot pose (you do not need all of them to implement <code>slam</code> correctly).</li> </ul> <p>See Udacity Artificial Intelligence for Robotics on how to implement online Graph SLAM. See also specific video.</p> <ul> <li>Add visualization code that creates a more realistic-looking display world</li> <li>Create a non-random maze of landmarks and see how your implementation of slam performs</li> <li>Display your robot world at every time step and stack these image frames to create a short video clip and to see how the robot localizes itself and builds up a model of the world over time</li> <li>Take a look at an implementation of slam that uses reinforcement learning and probabilistic motion models, at this Github link</li> </ul>"},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#online-slam","title":"Online SLAM\u00b6","text":""},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#create-a-non-random-maze-of-landmarks-and-see-how-your-implementation-of-slam-performs","title":"Create a non-random maze of landmarks and see how your implementation of slam performs\u00b6","text":""},{"location":"theory/slam/3.%20Landmark%20Detection%20and%20Tracking/#online-slam","title":"Online SLAM\u00b6","text":""},{"location":"theory/slam/helpers/","title":"Helpers","text":"In\u00a0[\u00a0]: Copied! <pre>from robot_class import robot\nfrom math import *\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> from robot_class import robot from math import * import random import numpy as np import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre># --------\n# this helper function displays the world that a robot is in\n# it assumes the world is a square grid of some given size\n# and that landmarks is a list of landmark positions(an optional argument)\ndef display_world(world_size, position, landmarks=None):\n    \n    # using seaborn, set background grid to gray\n    sns.set_style(\"dark\")\n\n    # Plot grid of values\n    world_grid = np.zeros((world_size+1, world_size+1))\n\n    # Set minor axes in between the labels\n    ax=plt.gca()\n    cols = world_size+1\n    rows = world_size+1\n\n    ax.set_xticks([x for x in range(1,cols)],minor=True )\n    ax.set_yticks([y for y in range(1,rows)],minor=True)\n    \n    # Plot grid on minor axes in gray (width = 1)\n    plt.grid(which='minor',ls='-',lw=1, color='white')\n    \n    # Plot grid on major axes in larger width\n    plt.grid(which='major',ls='-',lw=2, color='white')\n    \n    # Create an 'o' character that represents the robot\n    # ha = horizontal alignment, va = vertical\n    ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)\n    \n    # Draw landmarks if they exists\n    if(landmarks is not None):\n        # loop through all path indices and draw a dot (unless it's at the car's location)\n        for pos in landmarks:\n            if(pos != position):\n                ax.text(pos[0], pos[1], 'x', ha='center', va='center', color='purple', fontsize=20)\n    \n    # Display final result\n    plt.show()\n</pre> # -------- # this helper function displays the world that a robot is in # it assumes the world is a square grid of some given size # and that landmarks is a list of landmark positions(an optional argument) def display_world(world_size, position, landmarks=None):          # using seaborn, set background grid to gray     sns.set_style(\"dark\")      # Plot grid of values     world_grid = np.zeros((world_size+1, world_size+1))      # Set minor axes in between the labels     ax=plt.gca()     cols = world_size+1     rows = world_size+1      ax.set_xticks([x for x in range(1,cols)],minor=True )     ax.set_yticks([y for y in range(1,rows)],minor=True)          # Plot grid on minor axes in gray (width = 1)     plt.grid(which='minor',ls='-',lw=1, color='white')          # Plot grid on major axes in larger width     plt.grid(which='major',ls='-',lw=2, color='white')          # Create an 'o' character that represents the robot     # ha = horizontal alignment, va = vertical     ax.text(position[0], position[1], 'o', ha='center', va='center', color='r', fontsize=30)          # Draw landmarks if they exists     if(landmarks is not None):         # loop through all path indices and draw a dot (unless it's at the car's location)         for pos in landmarks:             if(pos != position):                 ax.text(pos[0], pos[1], 'x', ha='center', va='center', color='purple', fontsize=20)          # Display final result     plt.show() In\u00a0[\u00a0]: Copied! <pre># --------\n# this routine makes the robot data\n# the data is a list of measurements and movements: [measurements, [dx, dy]]\n# collected over a specified number of time steps, N\n#\ndef make_data(N, num_landmarks, world_size, measurement_range, motion_noise, \n              measurement_noise, distance):\n\n\n    # check if data has been made\n    complete = False\n\n    while not complete:\n\n        data = []\n\n        # make robot and landmarks\n        r = robot(world_size, measurement_range, motion_noise, measurement_noise)\n        r.make_landmarks(num_landmarks)\n        seen = [False for row in range(num_landmarks)]\n    \n        # guess an initial motion\n        orientation = random.random() * 2.0 * pi\n        dx = cos(orientation) * distance\n        dy = sin(orientation) * distance\n    \n        for k in range(N-1):\n    \n            # collect sensor measurements in a list, Z\n            Z = r.sense()\n\n            # check off all landmarks that were observed \n            for i in range(len(Z)):\n                seen[Z[i][0]] = True\n    \n            # move\n            while not r.move(dx, dy):\n                # if we'd be leaving the robot world, pick instead a new direction\n                orientation = random.random() * 2.0 * pi\n                dx = cos(orientation) * distance\n                dy = sin(orientation) * distance\n\n            # collect/memorize all sensor and motion data\n            data.append([Z, [dx, dy]])\n\n        # we are done when all landmarks were observed; otherwise re-run\n        complete = (sum(seen) == num_landmarks)\n\n    print(' ')\n    print('Landmarks: ', r.landmarks)\n    print(r)\n\n\n    return data\n</pre> # -------- # this routine makes the robot data # the data is a list of measurements and movements: [measurements, [dx, dy]] # collected over a specified number of time steps, N # def make_data(N, num_landmarks, world_size, measurement_range, motion_noise,                measurement_noise, distance):       # check if data has been made     complete = False      while not complete:          data = []          # make robot and landmarks         r = robot(world_size, measurement_range, motion_noise, measurement_noise)         r.make_landmarks(num_landmarks)         seen = [False for row in range(num_landmarks)]              # guess an initial motion         orientation = random.random() * 2.0 * pi         dx = cos(orientation) * distance         dy = sin(orientation) * distance              for k in range(N-1):                  # collect sensor measurements in a list, Z             Z = r.sense()              # check off all landmarks that were observed              for i in range(len(Z)):                 seen[Z[i][0]] = True                  # move             while not r.move(dx, dy):                 # if we'd be leaving the robot world, pick instead a new direction                 orientation = random.random() * 2.0 * pi                 dx = cos(orientation) * distance                 dy = sin(orientation) * distance              # collect/memorize all sensor and motion data             data.append([Z, [dx, dy]])          # we are done when all landmarks were observed; otherwise re-run         complete = (sum(seen) == num_landmarks)      print(' ')     print('Landmarks: ', r.landmarks)     print(r)       return data"},{"location":"theory/slam/robot_class/","title":"Robot class","text":"In\u00a0[\u00a0]: Copied! <pre>from math import *\nimport random\nimport numpy as np\n</pre> from math import * import random import numpy as np In\u00a0[\u00a0]: Copied! <pre>### ------------------------------------- ###\n# Below, is the robot class\n#\n# This robot lives in 2D, x-y space, and its motion is\n# pointed in a random direction, initially.\n# It moves in a straight line until it comes close to a wall \n# at which point it stops.\n#\n# For measurements, it  senses the x- and y-distance\n# to landmarks. This is different from range and bearing as\n# commonly studied in the literature, but this makes it much\n# easier to implement the essentials of SLAM without\n# cluttered math.\n#\nclass robot:\n    \n    # --------\n    # init:\n    #   creates a robot with the specified parameters and initializes\n    #   the location (self.x, self.y) to the center of the world\n    #\n    def __init__(self, world_size = 100.0, measurement_range = 30.0,\n                 motion_noise = 1.0, measurement_noise = 1.0):\n        self.measurement_noise = 0.0\n        self.world_size = world_size\n        self.measurement_range = measurement_range\n        self.x = world_size / 2.0\n        self.y = world_size / 2.0\n        self.motion_noise = motion_noise\n        self.measurement_noise = measurement_noise\n        self.landmarks = []\n        self.num_landmarks = 0\n        # Initialize path with initial position\n        self.path = [[self.x, self.y]]\n    \n    \n    # returns a positive, random float\n    def rand(self):\n        return random.random() * 2.0 - 1.0\n    \n    \n    # --------\n    # move: attempts to move robot by dx, dy. If outside world\n    #       boundary, then the move does nothing and instead returns failure\n    #\n    def move(self, dx, dy):\n        \n        x = self.x + dx + self.rand() * self.motion_noise\n        y = self.y + dy + self.rand() * self.motion_noise\n        \n        if x &lt; 0.0 or x &gt; self.world_size or y &lt; 0.0 or y &gt; self.world_size:\n            return False\n        else:\n            self.x = x\n            self.y = y\n            self.path.append([x, y])\n            return True\n\n\n    # --------\n    # sense: returns x- and y- distances to landmarks within visibility range\n    #        because not all landmarks may be in this range, the list of measurements\n    #        is of variable length. Set measurement_range to -1 if you want all\n    #        landmarks to be visible at all times\n    #\n    \n    ## TODO: paste your complete the sense function, here\n    ## make sure the indentation of the code is correct\n    def sense(self):\n        ''' This function does not take in any parameters, instead it references internal variables\n            (such as self.landamrks) to measure the distance between the robot and any landmarks\n            that the robot can see (that are within its measurement range).\n            This function returns a list of landmark indices, and the measured distances (dx, dy)\n            between the robot's position and said landmarks.\n            This function should account for measurement_noise and measurement_range.\n            One item in the returned list should be in the form: [landmark_index, dx, dy].\n            '''\n           \n        measurements = []\n        \n        ## TODO: iterate through all of the landmarks in a world\n        \n        ## TODO: For each landmark\n        ## 1. compute dx and dy, the distances between the robot and the landmark\n        ## 2. account for measurement noise by *adding* a noise component to dx and dy\n        ##    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise\n        ##    - Feel free to use the function self.rand() to help calculate this noise component\n        ## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range\n        ##    then we cannot record them; if they do fall in the range, then add them to the measurements list\n        ##    as list.append([index, dx, dy]), this format is important for data creation done later\n        \n        ## TODO: return the final, complete list of measurements\n        for index, landmark in enumerate(self.landmarks):\n            rx = self.x\n            ry = self.y\n            \n            lx = landmark[0]\n            ly = landmark[1]\n            \n            # 1. compute dx and dy, the distances between the robot and the landmark\n            # The measurements are with respect to robot's position, \n            # hence it's position has to be subtracted from the landmarks' position and not the other way around.\n            dx = lx - rx\n            dy = ly - ry\n            \n            # 2. account for measurement noise by *adding* a noise component to dx and dy\n            #    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise\n            #    - Feel free to use the function self.rand() to help calculate this noise component\n            #    - It may help to reference the `move` function for noise calculation\n            dx = dx + self.rand() * self.measurement_noise\n            dy = dy + self.rand() * self.measurement_noise\n            \n            # 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range\n            #    then we cannot record them; if they do fall in the range, then add them to the measurements list\n            #    as list.append([index, dx, dy]), this format is important for data creation done later\n            if abs(dx) &gt; self.measurement_range or abs(dy) &gt; self.measurement_range:\n                continue\n            \n            measurements.append([index, dx, dy])\n            \n        return measurements\n\n\n    # --------\n    # make_landmarks:\n    # make random landmarks located in the world\n    #\n    def make_landmarks(self, num_landmarks):\n        self.landmarks = []\n        for i in range(num_landmarks):\n            self.landmarks.append([round(random.random() * self.world_size),\n                                   round(random.random() * self.world_size)])\n        self.num_landmarks = num_landmarks\n        \n        \n    def make_deterministic_landmarks(self, num_landmarks):\n        self.num_landmarks = num_landmarks\n        \n        rows = round(sqrt(self.num_landmarks))\n        cols = self.num_landmarks - rows\n        \n        self.landmarks = []\n        \n        for row in np.linspace(int(self.world_size * 0.1), int(self.world_size * 0.9), rows):\n            for col in np.linspace(int(self.world_size * 0.1), int(self.world_size * 0.9), cols):\n                self.landmarks.append([row, col])\n                \n        self.landmarks = self.landmarks[:num_landmarks]\n\n\n    # called when print(robot) is called; prints the robot's location\n    def __repr__(self):\n        return 'Robot: [x=%.5f y=%.5f]'  % (self.x, self.y)\n</pre> ### ------------------------------------- ### # Below, is the robot class # # This robot lives in 2D, x-y space, and its motion is # pointed in a random direction, initially. # It moves in a straight line until it comes close to a wall  # at which point it stops. # # For measurements, it  senses the x- and y-distance # to landmarks. This is different from range and bearing as # commonly studied in the literature, but this makes it much # easier to implement the essentials of SLAM without # cluttered math. # class robot:          # --------     # init:     #   creates a robot with the specified parameters and initializes     #   the location (self.x, self.y) to the center of the world     #     def __init__(self, world_size = 100.0, measurement_range = 30.0,                  motion_noise = 1.0, measurement_noise = 1.0):         self.measurement_noise = 0.0         self.world_size = world_size         self.measurement_range = measurement_range         self.x = world_size / 2.0         self.y = world_size / 2.0         self.motion_noise = motion_noise         self.measurement_noise = measurement_noise         self.landmarks = []         self.num_landmarks = 0         # Initialize path with initial position         self.path = [[self.x, self.y]]               # returns a positive, random float     def rand(self):         return random.random() * 2.0 - 1.0               # --------     # move: attempts to move robot by dx, dy. If outside world     #       boundary, then the move does nothing and instead returns failure     #     def move(self, dx, dy):                  x = self.x + dx + self.rand() * self.motion_noise         y = self.y + dy + self.rand() * self.motion_noise                  if x &lt; 0.0 or x &gt; self.world_size or y &lt; 0.0 or y &gt; self.world_size:             return False         else:             self.x = x             self.y = y             self.path.append([x, y])             return True       # --------     # sense: returns x- and y- distances to landmarks within visibility range     #        because not all landmarks may be in this range, the list of measurements     #        is of variable length. Set measurement_range to -1 if you want all     #        landmarks to be visible at all times     #          ## TODO: paste your complete the sense function, here     ## make sure the indentation of the code is correct     def sense(self):         ''' This function does not take in any parameters, instead it references internal variables             (such as self.landamrks) to measure the distance between the robot and any landmarks             that the robot can see (that are within its measurement range).             This function returns a list of landmark indices, and the measured distances (dx, dy)             between the robot's position and said landmarks.             This function should account for measurement_noise and measurement_range.             One item in the returned list should be in the form: [landmark_index, dx, dy].             '''                     measurements = []                  ## TODO: iterate through all of the landmarks in a world                  ## TODO: For each landmark         ## 1. compute dx and dy, the distances between the robot and the landmark         ## 2. account for measurement noise by *adding* a noise component to dx and dy         ##    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise         ##    - Feel free to use the function self.rand() to help calculate this noise component         ## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range         ##    then we cannot record them; if they do fall in the range, then add them to the measurements list         ##    as list.append([index, dx, dy]), this format is important for data creation done later                  ## TODO: return the final, complete list of measurements         for index, landmark in enumerate(self.landmarks):             rx = self.x             ry = self.y                          lx = landmark[0]             ly = landmark[1]                          # 1. compute dx and dy, the distances between the robot and the landmark             # The measurements are with respect to robot's position,              # hence it's position has to be subtracted from the landmarks' position and not the other way around.             dx = lx - rx             dy = ly - ry                          # 2. account for measurement noise by *adding* a noise component to dx and dy             #    - The noise component should be a random value between [-1.0, 1.0)*measurement_noise             #    - Feel free to use the function self.rand() to help calculate this noise component             #    - It may help to reference the `move` function for noise calculation             dx = dx + self.rand() * self.measurement_noise             dy = dy + self.rand() * self.measurement_noise                          # 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range             #    then we cannot record them; if they do fall in the range, then add them to the measurements list             #    as list.append([index, dx, dy]), this format is important for data creation done later             if abs(dx) &gt; self.measurement_range or abs(dy) &gt; self.measurement_range:                 continue                          measurements.append([index, dx, dy])                      return measurements       # --------     # make_landmarks:     # make random landmarks located in the world     #     def make_landmarks(self, num_landmarks):         self.landmarks = []         for i in range(num_landmarks):             self.landmarks.append([round(random.random() * self.world_size),                                    round(random.random() * self.world_size)])         self.num_landmarks = num_landmarks                       def make_deterministic_landmarks(self, num_landmarks):         self.num_landmarks = num_landmarks                  rows = round(sqrt(self.num_landmarks))         cols = self.num_landmarks - rows                  self.landmarks = []                  for row in np.linspace(int(self.world_size * 0.1), int(self.world_size * 0.9), rows):             for col in np.linspace(int(self.world_size * 0.1), int(self.world_size * 0.9), cols):                 self.landmarks.append([row, col])                          self.landmarks = self.landmarks[:num_landmarks]       # called when print(robot) is called; prints the robot's location     def __repr__(self):         return 'Robot: [x=%.5f y=%.5f]'  % (self.x, self.y) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"theory/slam/robot_class/#end-robot-class","title":"END robot class\u00b6","text":""},{"location":"theory/slam/slam-overview/","title":"SLAM Overview","text":""},{"location":"theory/slam/slam-overview/#simultaneous-localization-and-mapping-overview","title":"Simultaneous Localization and Mapping Overview","text":"<ul> <li>EKF SLAM</li> <li>GraphSLAM</li> </ul> Getting familiar with SLAM (https://github.com/changh95/visual-slam-roadmap)"},{"location":"theory/slam/slam-overview/#resources","title":"Resources","text":""},{"location":"theory/slam/slam-overview/#online","title":"Online","text":"<ul> <li>OpenSLAM.org</li> <li>tzutalin/awesome-visual-slam</li> <li>changh95/visual-slam-roadmap</li> </ul>"},{"location":"theory/slam/slam-overview/#books","title":"Books","text":"<ul> <li>Probabilistic Robotics, Sebastian Thrun, Wolfram Burgard, Dieter Fox, MIT Press</li> </ul>"},{"location":"theory/state-estimation/nonparametric-filters/","title":"Nonparametric Filters (Particle Filter)","text":""},{"location":"theory/state-estimation/nonparametric-filters/#non-parametric-filters-particle-filters","title":"Non-parametric Filters Particle Filters","text":"<p>The goal of localization is to determin the pose(s) of a robot given a map of the environment as well as the noisy sensor measurements,  e.g., wheel odometry and laser distance measurements. This form of state estimation can be achieved with non-parametric filters, such as particle filters.</p>"},{"location":"theory/state-estimation/recursive-state-estimation/","title":"Recursive State Estimation","text":""},{"location":"theory/state-estimation/recursive-state-estimation/#recursive-state-estimation","title":"Recursive State Estimation","text":""},{"location":"theory/state-estimation/recursive-state-estimation/#robot-environment-interaction","title":"Robot Environment Interaction","text":""},{"location":"theory/state-estimation/recursive-state-estimation/#bayes-filters","title":"Bayes Filters","text":""}]}